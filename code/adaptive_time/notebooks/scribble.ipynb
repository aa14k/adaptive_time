{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from adaptive_time.features import Fourier_Features\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "tau = 0.00002\n",
    "env.stepTime(tau)\n",
    "def generate_trajectory(env):\n",
    "    observation, _ = env.reset()\n",
    "    trajectory = []\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = 0\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        trajectory.append([observation, action, reward, observation_])\n",
    "        observation = observation_\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trajectory = generate_trajectory(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12171"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = Fourier_Features()\n",
    "phi.init_fourier_features(4,4)\n",
    "phi.init_state_normalizers(np.array([4.8,2.0,0.418,1]), np.array([-4.8,-2.0,-0.418,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  empirical returns: 12.922285286285284  predicted returns: 12.922285286285303\n",
      "episode: 1  empirical returns: 12.922285286285284  predicted returns: 12.922285286285291\n",
      "episode: 2  empirical returns: 12.922285286285282  predicted returns: 12.922285286285254\n",
      "episode: 3  empirical returns: 12.922285286285284  predicted returns: 12.922285286285842\n",
      "episode: 4  empirical returns: 12.922285286285284  predicted returns: 12.922285286285295\n",
      "episode: 5  empirical returns: 12.922285286285286  predicted returns: 12.922285286284762\n",
      "episode: 6  empirical returns: 12.922285286285286  predicted returns: 12.92228528628531\n",
      "episode: 7  empirical returns: 12.922285286285284  predicted returns: 12.922285286285272\n",
      "episode: 8  empirical returns: 12.922285286285284  predicted returns: 12.92228528628513\n",
      "episode: 9  empirical returns: 12.922285286285284  predicted returns: 12.922285286285282\n",
      "episode: 10  empirical returns: 12.922285286285284  predicted returns: 12.922285286285131\n",
      "episode: 11  empirical returns: 12.922285286285282  predicted returns: 12.922285286285598\n",
      "episode: 12  empirical returns: 12.922285286285282  predicted returns: 12.92228528628532\n",
      "episode: 13  empirical returns: 12.922285286285282  predicted returns: 12.922285286285403\n",
      "episode: 14  empirical returns: 12.92228528628528  predicted returns: 12.92228528628544\n",
      "episode: 15  empirical returns: 12.922285286285284  predicted returns: 12.922285286285781\n",
      "episode: 16  empirical returns: 12.922285286285284  predicted returns: 12.922285286285277\n",
      "episode: 17  empirical returns: 12.922285286285282  predicted returns: 12.922285286283174\n",
      "episode: 18  empirical returns: 12.922285286285282  predicted returns: 12.922285286285092\n",
      "episode: 19  empirical returns: 12.922285286285282  predicted returns: 12.922285286285021\n",
      "episode: 20  empirical returns: 12.922285286285282  predicted returns: 12.922285286284811\n",
      "episode: 21  empirical returns: 12.92228528628528  predicted returns: 12.922285286285158\n",
      "episode: 22  empirical returns: 12.92228528628528  predicted returns: 12.922285286285103\n",
      "episode: 23  empirical returns: 12.922285286285282  predicted returns: 12.922285286285154\n",
      "episode: 24  empirical returns: 12.922285286285282  predicted returns: 12.922285286285387\n",
      "episode: 25  empirical returns: 12.922285286285282  predicted returns: 12.922285286285815\n",
      "episode: 26  empirical returns: 12.922285286285282  predicted returns: 12.922285286285485\n",
      "episode: 27  empirical returns: 12.922285286285282  predicted returns: 12.92228528628532\n",
      "episode: 28  empirical returns: 12.92228528628528  predicted returns: 12.922285286285042\n",
      "episode: 29  empirical returns: 12.92228528628528  predicted returns: 12.922285286286145\n",
      "episode: 30  empirical returns: 12.92228528628528  predicted returns: 12.922285286285605\n",
      "episode: 31  empirical returns: 12.922285286285284  predicted returns: 12.922285286285542\n",
      "episode: 32  empirical returns: 12.922285286285284  predicted returns: 12.922285286285373\n",
      "episode: 33  empirical returns: 12.922285286285284  predicted returns: 12.92228528628524\n",
      "episode: 34  empirical returns: 12.922285286285282  predicted returns: 12.922285286285021\n",
      "episode: 35  empirical returns: 12.922285286285282  predicted returns: 12.92228528628523\n",
      "episode: 36  empirical returns: 12.922285286285282  predicted returns: 12.922285286284978\n",
      "episode: 37  empirical returns: 12.922285286285282  predicted returns: 12.92228528628452\n",
      "episode: 38  empirical returns: 12.922285286285282  predicted returns: 12.922285286284986\n",
      "episode: 39  empirical returns: 12.922285286285284  predicted returns: 12.922285286284342\n",
      "episode: 40  empirical returns: 12.922285286285284  predicted returns: 12.922285286285508\n",
      "episode: 41  empirical returns: 12.922285286285284  predicted returns: 12.922285286285458\n",
      "episode: 42  empirical returns: 12.922285286285284  predicted returns: 12.922285286285629\n",
      "episode: 43  empirical returns: 12.922285286285284  predicted returns: 12.922285286285119\n",
      "episode: 44  empirical returns: 12.922285286285284  predicted returns: 12.922285286285703\n",
      "episode: 45  empirical returns: 12.922285286285284  predicted returns: 12.922285286285216\n",
      "episode: 46  empirical returns: 12.922285286285282  predicted returns: 12.922285286284712\n",
      "episode: 47  empirical returns: 12.922285286285286  predicted returns: 12.922285286285373\n",
      "episode: 48  empirical returns: 12.922285286285286  predicted returns: 12.9222852862858\n",
      "episode: 49  empirical returns: 12.922285286285284  predicted returns: 12.922285286285408\n",
      "episode: 50  empirical returns: 12.922285286285284  predicted returns: 12.922285286283275\n",
      "episode: 51  empirical returns: 12.922285286285284  predicted returns: 12.922285286284506\n",
      "episode: 52  empirical returns: 12.922285286285284  predicted returns: 12.922285286285607\n",
      "episode: 53  empirical returns: 12.922285286285284  predicted returns: 12.922285286285438\n",
      "episode: 54  empirical returns: 12.922285286285284  predicted returns: 12.922285286284742\n",
      "episode: 55  empirical returns: 12.922285286285286  predicted returns: 12.922285286285426\n",
      "episode: 56  empirical returns: 12.922285286285286  predicted returns: 12.922285286285945\n",
      "episode: 57  empirical returns: 12.922285286285286  predicted returns: 12.92228528628638\n",
      "episode: 58  empirical returns: 12.922285286285286  predicted returns: 12.922285286286105\n",
      "episode: 59  empirical returns: 12.922285286285284  predicted returns: 12.922285286293231\n",
      "episode: 60  empirical returns: 12.922285286285284  predicted returns: 12.922285286285145\n",
      "episode: 61  empirical returns: 12.922285286285284  predicted returns: 12.922285286274644\n",
      "episode: 62  empirical returns: 12.922285286285284  predicted returns: 12.922285286285089\n",
      "episode: 63  empirical returns: 12.922285286285286  predicted returns: 12.922285286285447\n",
      "episode: 64  empirical returns: 12.922285286285286  predicted returns: 12.922285286285117\n",
      "episode: 65  empirical returns: 12.922285286285286  predicted returns: 12.92228528628491\n",
      "episode: 66  empirical returns: 12.922285286285286  predicted returns: 12.922285286285195\n",
      "episode: 67  empirical returns: 12.922285286285286  predicted returns: 12.922285286282374\n",
      "episode: 68  empirical returns: 12.922285286285284  predicted returns: 12.922285286285234\n",
      "episode: 69  empirical returns: 12.922285286285284  predicted returns: 12.922285286281863\n",
      "episode: 70  empirical returns: 12.922285286285284  predicted returns: 12.922285286286739\n",
      "episode: 71  empirical returns: 12.922285286285286  predicted returns: 12.92228528628588\n",
      "episode: 72  empirical returns: 12.922285286285286  predicted returns: 12.922285286285456\n",
      "episode: 73  empirical returns: 12.922285286285286  predicted returns: 12.922285286284705\n",
      "episode: 74  empirical returns: 12.922285286285286  predicted returns: 12.922285286285863\n",
      "episode: 75  empirical returns: 12.922285286285286  predicted returns: 12.922285286287234\n",
      "episode: 76  empirical returns: 12.922285286285286  predicted returns: 12.922285286285504\n",
      "episode: 77  empirical returns: 12.922285286285286  predicted returns: 12.9222852862853\n",
      "episode: 78  empirical returns: 12.922285286285284  predicted returns: 12.922285286285344\n",
      "episode: 79  empirical returns: 12.922285286285284  predicted returns: 12.922285286285462\n",
      "episode: 80  empirical returns: 12.922285286285286  predicted returns: 12.922285286285444\n",
      "episode: 81  empirical returns: 12.922285286285287  predicted returns: 12.922285286281856\n",
      "episode: 82  empirical returns: 12.922285286285287  predicted returns: 12.922285286285042\n",
      "episode: 83  empirical returns: 12.92228528628529  predicted returns: 12.922285286285167\n",
      "episode: 84  empirical returns: 12.922285286285291  predicted returns: 12.922285286285177\n",
      "episode: 85  empirical returns: 12.922285286285291  predicted returns: 12.92228528628717\n",
      "episode: 86  empirical returns: 12.922285286285293  predicted returns: 12.922285286283895\n",
      "episode: 87  empirical returns: 12.922285286285284  predicted returns: 12.922285286285817\n",
      "episode: 88  empirical returns: 12.922285286285284  predicted returns: 12.922285286283515\n",
      "episode: 89  empirical returns: 12.922285286285286  predicted returns: 12.922285286286439\n",
      "episode: 90  empirical returns: 12.922285286285287  predicted returns: 12.922285286286186\n",
      "episode: 91  empirical returns: 12.922285286285287  predicted returns: 12.922285286296301\n",
      "episode: 92  empirical returns: 12.92228528628529  predicted returns: 12.922285286285666\n",
      "episode: 93  empirical returns: 12.922285286285291  predicted returns: 12.922285286280385\n",
      "episode: 94  empirical returns: 12.922285286285291  predicted returns: 12.922285286284966\n",
      "episode: 95  empirical returns: 12.922285286285282  predicted returns: 12.922285286286373\n",
      "episode: 96  empirical returns: 12.922285286285284  predicted returns: 12.922285286286176\n",
      "episode: 97  empirical returns: 12.922285286285286  predicted returns: 12.922285286284694\n",
      "episode: 98  empirical returns: 12.922285286285286  predicted returns: 12.922285286285014\n",
      "episode: 99  empirical returns: 12.922285286285287  predicted returns: 12.922285286281351\n",
      "episode: 100  empirical returns: 12.922285286285287  predicted returns: 12.922285286284811\n",
      "episode: 101  empirical returns: 12.92228528628529  predicted returns: 12.922285286285575\n",
      "episode: 102  empirical returns: 12.92228528628529  predicted returns: 12.922285286283774\n",
      "episode: 103  empirical returns: 12.922285286285282  predicted returns: 12.922285286285433\n",
      "episode: 104  empirical returns: 12.922285286285282  predicted returns: 12.922285286285813\n",
      "episode: 105  empirical returns: 12.922285286285284  predicted returns: 12.92228528628547\n",
      "episode: 106  empirical returns: 12.922285286285286  predicted returns: 12.922285286283605\n",
      "episode: 107  empirical returns: 12.922285286285286  predicted returns: 12.922285286283739\n",
      "episode: 108  empirical returns: 12.922285286285287  predicted returns: 12.92228528631702\n",
      "episode: 109  empirical returns: 12.922285286285287  predicted returns: 12.92228528628394\n",
      "episode: 110  empirical returns: 12.92228528628529  predicted returns: 12.922285286285305\n",
      "episode: 111  empirical returns: 12.922285286285282  predicted returns: 12.92228528628495\n",
      "episode: 112  empirical returns: 12.922285286285282  predicted returns: 12.922285286285515\n",
      "episode: 113  empirical returns: 12.922285286285284  predicted returns: 12.92228528628519\n",
      "episode: 114  empirical returns: 12.922285286285284  predicted returns: 12.922285286307847\n",
      "episode: 115  empirical returns: 12.922285286285286  predicted returns: 12.922285286285932\n",
      "episode: 116  empirical returns: 12.922285286285286  predicted returns: 12.922285286285074\n",
      "episode: 117  empirical returns: 12.922285286285287  predicted returns: 12.922285286284993\n",
      "episode: 118  empirical returns: 12.922285286285287  predicted returns: 12.922285286285046\n",
      "episode: 119  empirical returns: 12.92228528628528  predicted returns: 12.922285286285293\n",
      "episode: 120  empirical returns: 12.922285286285282  predicted returns: 12.922285286287178\n",
      "episode: 121  empirical returns: 12.922285286285282  predicted returns: 12.922285286285033\n",
      "episode: 122  empirical returns: 12.922285286285284  predicted returns: 12.922285286287092\n",
      "episode: 123  empirical returns: 12.922285286285284  predicted returns: 12.922285286275724\n",
      "episode: 124  empirical returns: 12.922285286285286  predicted returns: 12.92228528628927\n",
      "episode: 125  empirical returns: 12.922285286285286  predicted returns: 12.922285286284122\n",
      "episode: 126  empirical returns: 12.922285286285287  predicted returns: 12.922285286281209\n",
      "episode: 127  empirical returns: 12.92228528628528  predicted returns: 12.922285286285423\n",
      "episode: 128  empirical returns: 12.922285286285284  predicted returns: 12.922285286284069\n",
      "episode: 129  empirical returns: 12.922285286285286  predicted returns: 12.922285286285817\n",
      "episode: 130  empirical returns: 12.922285286285286  predicted returns: 12.922285286291412\n",
      "episode: 131  empirical returns: 12.922285286285286  predicted returns: 12.92228528627777\n",
      "episode: 132  empirical returns: 12.922285286285284  predicted returns: 12.92228528628521\n",
      "episode: 133  empirical returns: 12.922285286285286  predicted returns: 12.922285286275184\n",
      "episode: 134  empirical returns: 12.922285286285286  predicted returns: 12.922285286282893\n",
      "episode: 135  empirical returns: 12.922285286285287  predicted returns: 12.922285286285533\n",
      "episode: 136  empirical returns: 12.922285286285286  predicted returns: 12.922285286284808\n",
      "episode: 137  empirical returns: 12.922285286285284  predicted returns: 12.922285286284348\n",
      "episode: 138  empirical returns: 12.922285286285286  predicted returns: 12.922285286282605\n",
      "episode: 139  empirical returns: 12.922285286285286  predicted returns: 12.922285286286204\n",
      "episode: 140  empirical returns: 12.922285286285286  predicted returns: 12.922285286285387\n",
      "episode: 141  empirical returns: 12.922285286285284  predicted returns: 12.922285287129398\n",
      "episode: 142  empirical returns: 12.922285286285286  predicted returns: 12.922285286275667\n",
      "episode: 143  empirical returns: 12.922285286285286  predicted returns: 12.922285286284971\n",
      "episode: 144  empirical returns: 12.922285286285287  predicted returns: 12.922285286280115\n",
      "episode: 145  empirical returns: 12.922285286285286  predicted returns: 12.922285286286453\n",
      "episode: 146  empirical returns: 12.922285286285286  predicted returns: 12.922285286283987\n",
      "episode: 147  empirical returns: 12.922285286285286  predicted returns: 12.922285286288286\n",
      "episode: 148  empirical returns: 12.922285286285286  predicted returns: 12.92228528628587\n",
      "episode: 149  empirical returns: 12.922285286285286  predicted returns: 12.92228528628409\n",
      "episode: 150  empirical returns: 12.922285286285284  predicted returns: 12.922285286284719\n",
      "episode: 151  empirical returns: 12.922285286285286  predicted returns: 12.922285286286755\n",
      "episode: 152  empirical returns: 12.922285286285286  predicted returns: 12.922285286286726\n",
      "episode: 153  empirical returns: 12.922285286285286  predicted returns: 12.922285286284584\n",
      "episode: 154  empirical returns: 12.922285286285287  predicted returns: 12.922285286283461\n",
      "episode: 155  empirical returns: 12.922285286285287  predicted returns: 12.922285286286517\n",
      "episode: 156  empirical returns: 12.92228528628529  predicted returns: 12.922285286288414\n",
      "episode: 157  empirical returns: 12.92228528628529  predicted returns: 12.922285286286568\n",
      "episode: 158  empirical returns: 12.92228528628529  predicted returns: 12.922285286288453\n",
      "episode: 159  empirical returns: 12.922285286285284  predicted returns: 12.922285286285042\n",
      "episode: 160  empirical returns: 12.922285286285286  predicted returns: 12.922285286287341\n",
      "episode: 161  empirical returns: 12.922285286285286  predicted returns: 12.922285286287188\n",
      "episode: 162  empirical returns: 12.922285286285286  predicted returns: 12.922285286286822\n",
      "episode: 163  empirical returns: 12.922285286285287  predicted returns: 12.922285286286638\n",
      "episode: 164  empirical returns: 12.922285286285287  predicted returns: 12.922285286284733\n",
      "episode: 165  empirical returns: 12.922285286285287  predicted returns: 12.92228528628478\n",
      "episode: 166  empirical returns: 12.92228528628529  predicted returns: 12.92228528628578\n",
      "episode: 167  empirical returns: 12.922285286285284  predicted returns: 12.922285286287016\n",
      "episode: 168  empirical returns: 12.922285286285284  predicted returns: 12.922285286282104\n",
      "episode: 169  empirical returns: 12.922285286285286  predicted returns: 12.92228528628919\n",
      "episode: 170  empirical returns: 12.922285286285286  predicted returns: 12.922285286287824\n",
      "episode: 171  empirical returns: 12.922285286285286  predicted returns: 12.922285286286252\n",
      "episode: 172  empirical returns: 12.922285286285287  predicted returns: 12.922285286292862\n",
      "episode: 173  empirical returns: 12.922285286285287  predicted returns: 12.92228528628842\n",
      "episode: 174  empirical returns: 12.922285286285287  predicted returns: 12.92228528628085\n",
      "episode: 175  empirical returns: 12.922285286285284  predicted returns: 12.922285286284861\n",
      "episode: 176  empirical returns: 12.922285286285284  predicted returns: 12.922285286285671\n",
      "episode: 177  empirical returns: 12.922285286285284  predicted returns: 12.922285286310512\n",
      "episode: 178  empirical returns: 12.922285286285286  predicted returns: 12.922285286282928\n",
      "episode: 179  empirical returns: 12.922285286285286  predicted returns: 12.922285286290524\n",
      "episode: 180  empirical returns: 12.922285286285286  predicted returns: 12.922285286284433\n",
      "episode: 181  empirical returns: 12.922285286285287  predicted returns: 12.922285286287373\n",
      "episode: 182  empirical returns: 12.922285286285287  predicted returns: 12.922285286282294\n",
      "episode: 183  empirical returns: 12.922285286285284  predicted returns: 12.922285286286126\n",
      "episode: 184  empirical returns: 12.922285286285284  predicted returns: 12.922285286282147\n",
      "episode: 185  empirical returns: 12.922285286285284  predicted returns: 12.922285286283426\n",
      "episode: 186  empirical returns: 12.922285286285286  predicted returns: 12.92228528628219\n",
      "episode: 187  empirical returns: 12.922285286285286  predicted returns: 12.922285286285156\n",
      "episode: 188  empirical returns: 12.922285286285286  predicted returns: 12.922285286330322\n",
      "episode: 189  empirical returns: 12.922285286285286  predicted returns: 12.922285286286494\n",
      "episode: 190  empirical returns: 12.922285286285287  predicted returns: 12.922285286283424\n",
      "episode: 191  empirical returns: 12.922285286285282  predicted returns: 12.922285286284485\n",
      "episode: 192  empirical returns: 12.922285286285284  predicted returns: 12.922285286284204\n",
      "episode: 193  empirical returns: 12.922285286285284  predicted returns: 12.922285286284879\n",
      "episode: 194  empirical returns: 12.922285286285284  predicted returns: 12.922285286281834\n",
      "episode: 195  empirical returns: 12.922285286285286  predicted returns: 12.922285286283206\n",
      "episode: 196  empirical returns: 12.922285286285286  predicted returns: 12.922285286285163\n",
      "episode: 197  empirical returns: 12.922285286285286  predicted returns: 12.922285286285023\n",
      "episode: 198  empirical returns: 12.922285286285286  predicted returns: 12.922285286285804\n",
      "episode: 199  empirical returns: 12.922285286285282  predicted returns: 12.922285286284414\n",
      "episode: 200  empirical returns: 12.922285286285282  predicted returns: 12.92228528628463\n",
      "episode: 201  empirical returns: 12.922285286285284  predicted returns: 12.922285286286874\n",
      "episode: 202  empirical returns: 12.922285286285284  predicted returns: 12.92228528628362\n",
      "episode: 203  empirical returns: 12.922285286285284  predicted returns: 12.92228528628399\n",
      "episode: 204  empirical returns: 12.922285286285286  predicted returns: 12.922285286284053\n",
      "episode: 205  empirical returns: 12.922285286285286  predicted returns: 12.922285286282772\n",
      "episode: 206  empirical returns: 12.922285286285286  predicted returns: 12.922285286266145\n",
      "episode: 207  empirical returns: 12.922285286285282  predicted returns: 12.922285286292919\n",
      "episode: 208  empirical returns: 12.922285286285282  predicted returns: 12.922285286283984\n",
      "episode: 209  empirical returns: 12.922285286285282  predicted returns: 12.92228528624912\n",
      "episode: 210  empirical returns: 12.922285286285284  predicted returns: 12.922285286283191\n",
      "episode: 211  empirical returns: 12.922285286285284  predicted returns: 12.922285286293224\n",
      "episode: 212  empirical returns: 12.922285286285284  predicted returns: 12.922285286284414\n",
      "episode: 213  empirical returns: 12.922285286285286  predicted returns: 12.922285286285657\n",
      "episode: 214  empirical returns: 12.922285286285286  predicted returns: 12.922285286284723\n",
      "episode: 215  empirical returns: 12.922285286285282  predicted returns: 12.922285286284751\n",
      "episode: 216  empirical returns: 12.922285286285282  predicted returns: 12.922285286287757\n",
      "episode: 217  empirical returns: 12.922285286285282  predicted returns: 12.922285286283667\n",
      "episode: 218  empirical returns: 12.922285286285284  predicted returns: 12.922285286286787\n",
      "episode: 219  empirical returns: 12.922285286285284  predicted returns: 12.922285286289362\n",
      "episode: 220  empirical returns: 12.922285286285284  predicted returns: 12.922285286284602\n",
      "episode: 221  empirical returns: 12.922285286285284  predicted returns: 12.922285286283199\n",
      "episode: 222  empirical returns: 12.922285286285286  predicted returns: 12.922285286285685\n",
      "episode: 223  empirical returns: 12.922285286285282  predicted returns: 12.922285286285065\n",
      "episode: 224  empirical returns: 12.922285286285282  predicted returns: 12.922285286284998\n",
      "episode: 225  empirical returns: 12.922285286285282  predicted returns: 12.922285286283849\n",
      "episode: 226  empirical returns: 12.922285286285282  predicted returns: 12.922285286285671\n",
      "episode: 227  empirical returns: 12.922285286285284  predicted returns: 12.922285286284525\n",
      "episode: 228  empirical returns: 12.922285286285284  predicted returns: 12.922285286285206\n",
      "episode: 229  empirical returns: 12.922285286285284  predicted returns: 12.922285286285216\n",
      "episode: 230  empirical returns: 12.922285286285284  predicted returns: 12.92228528628338\n",
      "episode: 231  empirical returns: 12.92228528628528  predicted returns: 12.922285286284865\n",
      "episode: 232  empirical returns: 12.922285286285282  predicted returns: 12.922285286285199\n",
      "episode: 233  empirical returns: 12.922285286285282  predicted returns: 12.922285286284495\n",
      "episode: 234  empirical returns: 12.922285286285282  predicted returns: 12.922285286287126\n",
      "episode: 235  empirical returns: 12.922285286285282  predicted returns: 12.922285286284776\n",
      "episode: 236  empirical returns: 12.922285286285284  predicted returns: 12.92228528628505\n",
      "episode: 237  empirical returns: 12.922285286285284  predicted returns: 12.92228528634945\n",
      "episode: 238  empirical returns: 12.922285286285284  predicted returns: 12.922285286282069\n",
      "episode: 239  empirical returns: 12.92228528628528  predicted returns: 12.922285286271844\n",
      "episode: 240  empirical returns: 12.922285286285282  predicted returns: 12.922285286286252\n",
      "episode: 241  empirical returns: 12.922285286285282  predicted returns: 12.922285286289199\n",
      "episode: 242  empirical returns: 12.922285286285282  predicted returns: 12.922285286285142\n",
      "episode: 243  empirical returns: 12.922285286285282  predicted returns: 12.92228528628498\n",
      "episode: 244  empirical returns: 12.922285286285282  predicted returns: 12.92228528627929\n",
      "episode: 245  empirical returns: 12.922285286285284  predicted returns: 12.922285286286932\n",
      "episode: 246  empirical returns: 12.922285286285284  predicted returns: 12.922285286279168\n",
      "episode: 247  empirical returns: 12.92228528628528  predicted returns: 12.922285286284284\n",
      "episode: 248  empirical returns: 12.922285286285282  predicted returns: 12.922285286286838\n",
      "episode: 249  empirical returns: 12.922285286285284  predicted returns: 12.922285286283408\n",
      "episode: 250  empirical returns: 12.922285286285284  predicted returns: 12.922285286288329\n",
      "episode: 251  empirical returns: 12.922285286285282  predicted returns: 12.92228528628479\n",
      "episode: 252  empirical returns: 12.922285286285282  predicted returns: 12.922285286288098\n",
      "episode: 253  empirical returns: 12.922285286285284  predicted returns: 12.922285286283904\n",
      "episode: 254  empirical returns: 12.922285286285284  predicted returns: 12.922285286289615\n",
      "episode: 255  empirical returns: 12.92228528628528  predicted returns: 12.922285286284449\n",
      "episode: 256  empirical returns: 12.922285286285282  predicted returns: 12.922285286282401\n",
      "episode: 257  empirical returns: 12.922285286285282  predicted returns: 12.922285286288016\n",
      "episode: 258  empirical returns: 12.922285286285284  predicted returns: 12.922285286284913\n",
      "episode: 259  empirical returns: 12.922285286285282  predicted returns: 12.922285286285636\n",
      "episode: 260  empirical returns: 12.922285286285282  predicted returns: 12.92228528628613\n",
      "episode: 261  empirical returns: 12.922285286285282  predicted returns: 12.922285286182387\n",
      "episode: 262  empirical returns: 12.922285286285284  predicted returns: 12.92228528628623\n",
      "episode: 263  empirical returns: 12.922285286285284  predicted returns: 12.9222852862857\n",
      "episode: 264  empirical returns: 12.922285286285282  predicted returns: 12.922285286282554\n",
      "episode: 265  empirical returns: 12.922285286285282  predicted returns: 12.922285286285984\n",
      "episode: 266  empirical returns: 12.922285286285284  predicted returns: 12.92228528628559\n",
      "episode: 267  empirical returns: 12.922285286285284  predicted returns: 12.922285286286955\n",
      "episode: 268  empirical returns: 12.922285286285284  predicted returns: 12.922285286287625\n",
      "episode: 269  empirical returns: 12.922285286285282  predicted returns: 12.92228528628645\n",
      "episode: 270  empirical returns: 12.922285286285282  predicted returns: 12.922285286286993\n",
      "episode: 271  empirical returns: 12.922285286285287  predicted returns: 12.922285286259779\n",
      "episode: 272  empirical returns: 12.922285286285286  predicted returns: 12.922285286286495\n",
      "episode: 273  empirical returns: 12.922285286285286  predicted returns: 12.922285286285227\n",
      "episode: 274  empirical returns: 12.922285286285286  predicted returns: 12.922285286285982\n",
      "episode: 275  empirical returns: 12.922285286285286  predicted returns: 12.922285286285836\n",
      "episode: 276  empirical returns: 12.922285286285287  predicted returns: 12.922285286285476\n",
      "episode: 277  empirical returns: 12.922285286285286  predicted returns: 12.922285286284058\n",
      "episode: 278  empirical returns: 12.922285286285286  predicted returns: 12.922285286284382\n",
      "episode: 279  empirical returns: 12.922285286285286  predicted returns: 12.922285286284856\n",
      "episode: 280  empirical returns: 12.922285286285287  predicted returns: 12.922285286284382\n",
      "episode: 281  empirical returns: 12.922285286285287  predicted returns: 12.922285286283248\n",
      "episode: 282  empirical returns: 12.922285286285286  predicted returns: 12.922285286290595\n",
      "episode: 283  empirical returns: 12.922285286285286  predicted returns: 12.922285286285465\n",
      "episode: 284  empirical returns: 12.922285286285286  predicted returns: 12.922285286284048\n",
      "episode: 285  empirical returns: 12.922285286285286  predicted returns: 12.922285286284822\n",
      "episode: 286  empirical returns: 12.922285286285286  predicted returns: 12.922285286283298\n",
      "episode: 287  empirical returns: 12.922285286285286  predicted returns: 12.92228528628393\n",
      "episode: 288  empirical returns: 12.922285286285286  predicted returns: 12.922285286285454\n",
      "episode: 289  empirical returns: 12.922285286285287  predicted returns: 12.92228528628478\n",
      "episode: 290  empirical returns: 12.922285286285286  predicted returns: 12.922285286288357\n",
      "episode: 291  empirical returns: 12.922285286285286  predicted returns: 12.92228528628526\n",
      "episode: 292  empirical returns: 12.922285286285286  predicted returns: 12.922285286285138\n",
      "episode: 293  empirical returns: 12.922285286285286  predicted returns: 12.922285286285648\n",
      "episode: 294  empirical returns: 12.922285286285286  predicted returns: 12.922285286285145\n",
      "episode: 295  empirical returns: 12.922285286285286  predicted returns: 12.922285286291498\n",
      "episode: 296  empirical returns: 12.922285286285286  predicted returns: 12.92228528627372\n",
      "episode: 297  empirical returns: 12.922285286285286  predicted returns: 12.922285286286272\n",
      "episode: 298  empirical returns: 12.922285286285286  predicted returns: 12.922285286285476\n",
      "episode: 299  empirical returns: 12.922285286285287  predicted returns: 12.922285286285838\n",
      "episode: 300  empirical returns: 12.922285286285287  predicted returns: 12.92228528628566\n",
      "episode: 301  empirical returns: 12.922285286285287  predicted returns: 12.922285286285932\n",
      "episode: 302  empirical returns: 12.922285286285287  predicted returns: 12.922285286288025\n",
      "episode: 303  empirical returns: 12.922285286285286  predicted returns: 12.922285286285996\n",
      "episode: 304  empirical returns: 12.922285286285286  predicted returns: 12.922285286285755\n",
      "episode: 305  empirical returns: 12.922285286285286  predicted returns: 12.922285286286062\n",
      "episode: 306  empirical returns: 12.922285286285286  predicted returns: 12.922285286286161\n",
      "episode: 307  empirical returns: 12.922285286285286  predicted returns: 12.922285286286504\n",
      "episode: 308  empirical returns: 12.922285286285287  predicted returns: 12.92228528628576\n",
      "episode: 309  empirical returns: 12.922285286285287  predicted returns: 12.922285286292137\n",
      "episode: 310  empirical returns: 12.922285286285287  predicted returns: 12.92228528628571\n",
      "episode: 311  empirical returns: 12.922285286285286  predicted returns: 12.922285286283884\n",
      "episode: 312  empirical returns: 12.922285286285286  predicted returns: 12.922285286285803\n",
      "episode: 313  empirical returns: 12.922285286285286  predicted returns: 12.922285286286153\n",
      "episode: 314  empirical returns: 12.922285286285286  predicted returns: 12.92228528628494\n",
      "episode: 315  empirical returns: 12.922285286285286  predicted returns: 12.922285286286813\n",
      "episode: 316  empirical returns: 12.922285286285286  predicted returns: 12.922285286285847\n",
      "episode: 317  empirical returns: 12.922285286285287  predicted returns: 12.922285286284824\n",
      "episode: 318  empirical returns: 12.92228528628529  predicted returns: 12.922285286285009\n",
      "episode: 319  empirical returns: 12.922285286285284  predicted returns: 12.922285286285028\n",
      "episode: 320  empirical returns: 12.922285286285286  predicted returns: 12.922285286286552\n",
      "episode: 321  empirical returns: 12.922285286285286  predicted returns: 12.92228528628411\n",
      "episode: 322  empirical returns: 12.922285286285284  predicted returns: 12.92228528628626\n",
      "episode: 323  empirical returns: 12.922285286285286  predicted returns: 12.922285286297178\n",
      "episode: 324  empirical returns: 12.922285286285287  predicted returns: 12.922285286284882\n",
      "episode: 325  empirical returns: 12.922285286285286  predicted returns: 12.922285286285865\n",
      "episode: 326  empirical returns: 12.922285286285286  predicted returns: 12.922285286277846\n",
      "episode: 327  empirical returns: 12.922285286285284  predicted returns: 12.92228528628576\n",
      "episode: 328  empirical returns: 12.922285286285284  predicted returns: 12.922285286287323\n",
      "episode: 329  empirical returns: 12.922285286285286  predicted returns: 12.922285286281904\n",
      "episode: 330  empirical returns: 12.922285286285287  predicted returns: 12.922285286280413\n",
      "episode: 331  empirical returns: 12.922285286285286  predicted returns: 12.922285286284401\n",
      "episode: 332  empirical returns: 12.922285286285284  predicted returns: 12.922285286292517\n",
      "episode: 333  empirical returns: 12.922285286285286  predicted returns: 12.922285286287432\n",
      "episode: 334  empirical returns: 12.922285286285287  predicted returns: 12.922285286285206\n",
      "episode: 335  empirical returns: 12.922285286285284  predicted returns: 12.922285286284797\n",
      "episode: 336  empirical returns: 12.922285286285286  predicted returns: 12.92228528630445\n",
      "episode: 337  empirical returns: 12.922285286285284  predicted returns: 12.922285286296926\n",
      "episode: 338  empirical returns: 12.922285286285284  predicted returns: 12.92228528628037\n",
      "episode: 339  empirical returns: 12.922285286285286  predicted returns: 12.922285286279351\n",
      "episode: 340  empirical returns: 12.922285286285287  predicted returns: 12.922285286301033\n",
      "episode: 341  empirical returns: 12.922285286285286  predicted returns: 12.922285286182102\n",
      "episode: 342  empirical returns: 12.922285286285286  predicted returns: 12.922285286284692\n",
      "episode: 343  empirical returns: 12.922285286285284  predicted returns: 12.922285286286487\n",
      "episode: 344  empirical returns: 12.922285286285282  predicted returns: 12.92228528628152\n",
      "episode: 345  empirical returns: 12.922285286285284  predicted returns: 12.922285286279218\n",
      "episode: 346  empirical returns: 12.922285286285286  predicted returns: 12.922285286283172\n",
      "episode: 347  empirical returns: 12.922285286285286  predicted returns: 12.922285286283678\n",
      "episode: 348  empirical returns: 12.922285286285284  predicted returns: 12.922285286298674\n",
      "episode: 349  empirical returns: 12.922285286285286  predicted returns: 12.922285286287085\n",
      "episode: 350  empirical returns: 12.922285286285287  predicted returns: 12.92228528628385\n",
      "episode: 351  empirical returns: 12.922285286285284  predicted returns: 12.922285286284252\n",
      "episode: 352  empirical returns: 12.922285286285286  predicted returns: 12.922285286282953\n",
      "episode: 353  empirical returns: 12.922285286285284  predicted returns: 12.92228528628157\n",
      "episode: 354  empirical returns: 12.922285286285284  predicted returns: 12.922285286284138\n",
      "episode: 355  empirical returns: 12.922285286285284  predicted returns: 12.922285286278122\n",
      "episode: 356  empirical returns: 12.922285286285286  predicted returns: 12.922285286282184\n",
      "episode: 357  empirical returns: 12.922285286285286  predicted returns: 12.922285286285724\n",
      "episode: 358  empirical returns: 12.922285286285284  predicted returns: 12.922285286279374\n",
      "episode: 359  empirical returns: 12.922285286285284  predicted returns: 12.922285286279855\n",
      "episode: 360  empirical returns: 12.922285286285282  predicted returns: 12.922285286282547\n",
      "episode: 361  empirical returns: 12.922285286285284  predicted returns: 12.922285286283994\n",
      "episode: 362  empirical returns: 12.922285286285286  predicted returns: 12.9222852862841\n",
      "episode: 363  empirical returns: 12.922285286285284  predicted returns: 12.922285286284271\n",
      "episode: 364  empirical returns: 12.922285286285284  predicted returns: 12.922285286286346\n",
      "episode: 365  empirical returns: 12.922285286285286  predicted returns: 12.922285286284676\n",
      "episode: 366  empirical returns: 12.922285286285286  predicted returns: 12.922285286284323\n",
      "episode: 367  empirical returns: 12.922285286285284  predicted returns: 12.922285286283454\n",
      "episode: 368  empirical returns: 12.922285286285284  predicted returns: 12.922285286317376\n",
      "episode: 369  empirical returns: 12.922285286285284  predicted returns: 12.922285286283348\n",
      "episode: 370  empirical returns: 12.922285286285282  predicted returns: 12.922285286271864\n",
      "episode: 371  empirical returns: 12.922285286285284  predicted returns: 12.922285286283952\n",
      "episode: 372  empirical returns: 12.922285286285286  predicted returns: 12.922285286276876\n",
      "episode: 373  empirical returns: 12.922285286285286  predicted returns: 12.922285286285202\n",
      "episode: 374  empirical returns: 12.922285286285284  predicted returns: 12.92228528634837\n",
      "episode: 375  empirical returns: 12.922285286285282  predicted returns: 12.922285286282797\n",
      "episode: 376  empirical returns: 12.922285286285282  predicted returns: 12.922285286286096\n",
      "episode: 377  empirical returns: 12.922285286285284  predicted returns: 12.922285286277038\n",
      "episode: 378  empirical returns: 12.922285286285286  predicted returns: 12.922285286282357\n",
      "episode: 379  empirical returns: 12.922285286285284  predicted returns: 12.922285286284465\n",
      "episode: 380  empirical returns: 12.922285286285284  predicted returns: 12.922285286283195\n",
      "episode: 381  empirical returns: 12.922285286285284  predicted returns: 12.922285286284227\n",
      "episode: 382  empirical returns: 12.922285286285286  predicted returns: 12.922285286282113\n",
      "episode: 383  empirical returns: 12.922285286285282  predicted returns: 12.922285286283122\n",
      "episode: 384  empirical returns: 12.922285286285284  predicted returns: 12.922285286274544\n",
      "episode: 385  empirical returns: 12.922285286285284  predicted returns: 12.922285286284101\n",
      "episode: 386  empirical returns: 12.922285286285282  predicted returns: 12.92228528628321\n",
      "episode: 387  empirical returns: 12.922285286285284  predicted returns: 12.922285286283405\n",
      "episode: 388  empirical returns: 12.922285286285286  predicted returns: 12.922285286284325\n",
      "episode: 389  empirical returns: 12.922285286285284  predicted returns: 12.922285286283643\n",
      "episode: 390  empirical returns: 12.922285286285284  predicted returns: 12.922285286283163\n",
      "episode: 391  empirical returns: 12.922285286285282  predicted returns: 12.922285286283593\n",
      "episode: 392  empirical returns: 12.922285286285282  predicted returns: 12.922285286281044\n",
      "episode: 393  empirical returns: 12.922285286285284  predicted returns: 12.92228528628149\n",
      "episode: 394  empirical returns: 12.922285286285284  predicted returns: 12.922285286280339\n",
      "episode: 395  empirical returns: 12.922285286285284  predicted returns: 12.922285286282026\n",
      "episode: 396  empirical returns: 12.922285286285282  predicted returns: 12.922285286283081\n",
      "episode: 397  empirical returns: 12.922285286285284  predicted returns: 12.922285286286222\n",
      "episode: 398  empirical returns: 12.922285286285286  predicted returns: 12.922285286286034\n",
      "episode: 399  empirical returns: 12.922285286285282  predicted returns: 12.922285286287961\n",
      "episode: 400  empirical returns: 12.922285286285284  predicted returns: 12.922285286285016\n",
      "episode: 401  empirical returns: 12.922285286285282  predicted returns: 12.922285286284227\n",
      "episode: 402  empirical returns: 12.922285286285282  predicted returns: 12.922285286286462\n",
      "episode: 403  empirical returns: 12.922285286285284  predicted returns: 12.922285286284463\n",
      "episode: 404  empirical returns: 12.922285286285284  predicted returns: 12.922285286284367\n",
      "episode: 405  empirical returns: 12.922285286285284  predicted returns: 12.92228528628508\n",
      "episode: 406  empirical returns: 12.922285286285284  predicted returns: 12.92228528628417\n",
      "episode: 407  empirical returns: 12.922285286285282  predicted returns: 12.922285286285865\n",
      "episode: 408  empirical returns: 12.922285286285282  predicted returns: 12.922285286284973\n",
      "episode: 409  empirical returns: 12.922285286285282  predicted returns: 12.922285286284211\n",
      "episode: 410  empirical returns: 12.922285286285284  predicted returns: 12.92228528628499\n",
      "episode: 411  empirical returns: 12.922285286285284  predicted returns: 12.92228528628372\n",
      "episode: 412  empirical returns: 12.922285286285282  predicted returns: 12.922285286278921\n",
      "episode: 413  empirical returns: 12.922285286285284  predicted returns: 12.922285286285588\n",
      "episode: 414  empirical returns: 12.922285286285286  predicted returns: 12.922285286284332\n",
      "episode: 415  empirical returns: 12.922285286285282  predicted returns: 12.92228528628565\n",
      "episode: 416  empirical returns: 12.922285286285284  predicted returns: 12.922285286285165\n",
      "episode: 417  empirical returns: 12.922285286285282  predicted returns: 12.922285286282776\n",
      "episode: 418  empirical returns: 12.922285286285282  predicted returns: 12.922285286282813\n",
      "episode: 419  empirical returns: 12.922285286285282  predicted returns: 12.922285286280498\n",
      "episode: 420  empirical returns: 12.922285286285284  predicted returns: 12.922285286284811\n",
      "episode: 421  empirical returns: 12.922285286285284  predicted returns: 12.922285286287586\n",
      "episode: 422  empirical returns: 12.922285286285282  predicted returns: 12.922285286285806\n",
      "episode: 423  empirical returns: 12.922285286285282  predicted returns: 12.922285286284374\n",
      "episode: 424  empirical returns: 12.92228528628528  predicted returns: 12.922285286285263\n",
      "episode: 425  empirical returns: 12.922285286285282  predicted returns: 12.922285286283556\n",
      "episode: 426  empirical returns: 12.922285286285284  predicted returns: 12.92228528628528\n",
      "episode: 427  empirical returns: 12.922285286285282  predicted returns: 12.922285286288595\n",
      "episode: 428  empirical returns: 12.922285286285282  predicted returns: 12.922285286284705\n",
      "episode: 429  empirical returns: 12.922285286285284  predicted returns: 12.922285286358601\n",
      "episode: 430  empirical returns: 12.922285286285284  predicted returns: 12.922285286285776\n",
      "episode: 431  empirical returns: 12.922285286285282  predicted returns: 12.922285286288261\n",
      "episode: 432  empirical returns: 12.922285286285282  predicted returns: 12.922285286287444\n",
      "episode: 433  empirical returns: 12.922285286285282  predicted returns: 12.922285286286266\n",
      "episode: 434  empirical returns: 12.922285286285282  predicted returns: 12.922285286286531\n",
      "episode: 435  empirical returns: 12.922285286285282  predicted returns: 12.922285286286353\n",
      "episode: 436  empirical returns: 12.922285286285284  predicted returns: 12.922285286285756\n",
      "episode: 437  empirical returns: 12.922285286285284  predicted returns: 12.922285286285952\n",
      "episode: 438  empirical returns: 12.922285286285282  predicted returns: 12.92228528628749\n",
      "episode: 439  empirical returns: 12.922285286285282  predicted returns: 12.922285286286392\n",
      "episode: 440  empirical returns: 12.92228528628528  predicted returns: 12.922285286286321\n",
      "episode: 441  empirical returns: 12.922285286285282  predicted returns: 12.92228528628653\n",
      "episode: 442  empirical returns: 12.922285286285284  predicted returns: 12.922285286287526\n",
      "episode: 443  empirical returns: 12.922285286285282  predicted returns: 12.922285286335672\n",
      "episode: 444  empirical returns: 12.922285286285282  predicted returns: 12.922285286277674\n",
      "episode: 445  empirical returns: 12.922285286285282  predicted returns: 12.922285286275702\n",
      "episode: 446  empirical returns: 12.922285286285284  predicted returns: 12.922285286294825\n",
      "episode: 447  empirical returns: 12.922285286285282  predicted returns: 12.922285286288455\n",
      "episode: 448  empirical returns: 12.922285286285282  predicted returns: 12.922285286294063\n",
      "episode: 449  empirical returns: 12.922285286285282  predicted returns: 12.922285286111666\n",
      "episode: 450  empirical returns: 12.92228528628528  predicted returns: 12.922285286273917\n",
      "episode: 451  empirical returns: 12.922285286285282  predicted returns: 12.922285286540273\n",
      "episode: 452  empirical returns: 12.922285286285284  predicted returns: 12.922285286276797\n",
      "episode: 453  empirical returns: 12.922285286285282  predicted returns: 12.922285286283625\n",
      "episode: 454  empirical returns: 12.922285286285282  predicted returns: 12.92228528627385\n",
      "episode: 455  empirical returns: 12.92228528628528  predicted returns: 12.922285286284762\n",
      "episode: 456  empirical returns: 12.92228528628528  predicted returns: 12.922285286283993\n",
      "episode: 457  empirical returns: 12.922285286285282  predicted returns: 12.92228528588592\n",
      "episode: 458  empirical returns: 12.922285286285282  predicted returns: 12.922285286197907\n",
      "episode: 459  empirical returns: 12.922285286285282  predicted returns: 12.922285286284087\n",
      "episode: 460  empirical returns: 12.922285286285282  predicted returns: 12.92228528628335\n",
      "episode: 461  empirical returns: 12.922285286285282  predicted returns: 12.922285286284723\n",
      "episode: 462  empirical returns: 12.922285286285284  predicted returns: 12.92228528628138\n",
      "episode: 463  empirical returns: 12.92228528628528  predicted returns: 12.922285286283508\n",
      "episode: 464  empirical returns: 12.922285286285282  predicted returns: 12.922285286283227\n",
      "episode: 465  empirical returns: 12.922285286285282  predicted returns: 12.922285286282868\n",
      "episode: 466  empirical returns: 12.92228528628528  predicted returns: 12.922285286281285\n",
      "episode: 467  empirical returns: 12.922285286285282  predicted returns: 12.922285286282836\n",
      "episode: 468  empirical returns: 12.922285286285284  predicted returns: 12.922285286284993\n",
      "episode: 469  empirical returns: 12.922285286285282  predicted returns: 12.922285286285444\n",
      "episode: 470  empirical returns: 12.922285286285282  predicted returns: 12.922285286288382\n",
      "episode: 471  empirical returns: 12.92228528628528  predicted returns: 12.922285286288197\n",
      "episode: 472  empirical returns: 12.92228528628528  predicted returns: 12.922285286285529\n",
      "episode: 473  empirical returns: 12.922285286285282  predicted returns: 12.92228528628614\n",
      "episode: 474  empirical returns: 12.922285286285282  predicted returns: 12.922285286291128\n",
      "episode: 475  empirical returns: 12.922285286285282  predicted returns: 12.922285364958498\n",
      "episode: 476  empirical returns: 12.92228528628528  predicted returns: 12.9222852862788\n",
      "episode: 477  empirical returns: 12.922285286285282  predicted returns: 12.922285286289036\n",
      "episode: 478  empirical returns: 12.922285286285284  predicted returns: 12.922285285784028\n",
      "episode: 479  empirical returns: 12.92228528628528  predicted returns: 12.922285286279187\n",
      "episode: 480  empirical returns: 12.922285286285282  predicted returns: 12.922285286281056\n",
      "episode: 481  empirical returns: 12.922285286285282  predicted returns: 12.9222852862836\n",
      "episode: 482  empirical returns: 12.92228528628528  predicted returns: 12.922285286292098\n",
      "episode: 483  empirical returns: 12.922285286285282  predicted returns: 12.92228528629579\n",
      "episode: 484  empirical returns: 12.922285286285282  predicted returns: 12.92228528630394\n",
      "episode: 485  empirical returns: 12.922285286285282  predicted returns: 12.922285286275525\n",
      "episode: 486  empirical returns: 12.922285286285282  predicted returns: 12.922285286287181\n",
      "episode: 487  empirical returns: 12.92228528628528  predicted returns: 12.922285286288012\n",
      "episode: 488  empirical returns: 12.922285286285282  predicted returns: 12.922285286286204\n",
      "episode: 489  empirical returns: 12.92228528628528  predicted returns: 12.922285286284671\n",
      "episode: 490  empirical returns: 12.922285286285282  predicted returns: 12.922285286284797\n",
      "episode: 491  empirical returns: 12.922285286285282  predicted returns: 12.922285286281749\n",
      "episode: 492  empirical returns: 12.92228528628528  predicted returns: 12.922285286283032\n",
      "episode: 493  empirical returns: 12.922285286285282  predicted returns: 12.92228528628747\n",
      "episode: 494  empirical returns: 12.922285286285284  predicted returns: 12.922285286283302\n",
      "episode: 495  empirical returns: 12.92228528628528  predicted returns: 12.922285286293477\n",
      "episode: 496  empirical returns: 12.922285286285282  predicted returns: 12.92228528628404\n",
      "episode: 497  empirical returns: 12.922285286285282  predicted returns: 12.922285286302412\n",
      "episode: 498  empirical returns: 12.922285286285282  predicted returns: 12.922285286283504\n",
      "episode: 499  empirical returns: 12.922285286285282  predicted returns: 12.922285286290958\n",
      "episode: 500  empirical returns: 12.922285286285282  predicted returns: 12.922285286274729\n",
      "episode: 501  empirical returns: 12.922285286285282  predicted returns: 12.922285286283481\n",
      "episode: 502  empirical returns: 12.92228528628528  predicted returns: 12.922285286285971\n",
      "episode: 503  empirical returns: 12.92228528628528  predicted returns: 12.922285286287881\n",
      "episode: 504  empirical returns: 12.922285286285282  predicted returns: 12.922285286283891\n",
      "episode: 505  empirical returns: 12.92228528628528  predicted returns: 12.922285286285234\n",
      "episode: 506  empirical returns: 12.922285286285282  predicted returns: 12.922285286284374\n",
      "episode: 507  empirical returns: 12.922285286285282  predicted returns: 12.922285286282982\n",
      "episode: 508  empirical returns: 12.92228528628528  predicted returns: 12.922285286284641\n",
      "episode: 509  empirical returns: 12.922285286285282  predicted returns: 12.922285286285478\n",
      "episode: 510  empirical returns: 12.922285286285282  predicted returns: 12.922285286288748\n",
      "episode: 511  empirical returns: 12.92228528628528  predicted returns: 12.922285286285199\n",
      "episode: 512  empirical returns: 12.922285286285282  predicted returns: 12.922285286287279\n",
      "episode: 513  empirical returns: 12.922285286285282  predicted returns: 12.922285286283948\n",
      "episode: 514  empirical returns: 12.922285286285282  predicted returns: 12.922285286285822\n",
      "episode: 515  empirical returns: 12.92228528628528  predicted returns: 12.922285286284932\n",
      "episode: 516  empirical returns: 12.922285286285282  predicted returns: 12.92228528628897\n",
      "episode: 517  empirical returns: 12.922285286285282  predicted returns: 12.92228528628759\n",
      "episode: 518  empirical returns: 12.92228528628528  predicted returns: 12.922285286290993\n",
      "episode: 519  empirical returns: 12.922285286285282  predicted returns: 12.922285286292706\n",
      "episode: 520  empirical returns: 12.922285286285282  predicted returns: 12.922285286295677\n",
      "episode: 521  empirical returns: 12.92228528628528  predicted returns: 12.92228528628807\n",
      "episode: 522  empirical returns: 12.922285286285282  predicted returns: 12.92228528629127\n",
      "episode: 523  empirical returns: 12.922285286285282  predicted returns: 12.922285286278713\n",
      "episode: 524  empirical returns: 12.922285286285282  predicted returns: 12.922285286286623\n",
      "episode: 525  empirical returns: 12.922285286285282  predicted returns: 12.922285286283682\n",
      "episode: 526  empirical returns: 12.922285286285282  predicted returns: 12.922285286291821\n",
      "episode: 527  empirical returns: 12.922285286285284  predicted returns: 12.922285286296171\n",
      "episode: 528  empirical returns: 12.922285286285282  predicted returns: 12.92228528625634\n",
      "episode: 529  empirical returns: 12.922285286285282  predicted returns: 12.922285286290943\n",
      "episode: 530  empirical returns: 12.922285286285284  predicted returns: 12.922285286289661\n",
      "episode: 531  empirical returns: 12.922285286285284  predicted returns: 12.922285286286309\n",
      "episode: 532  empirical returns: 12.922285286285284  predicted returns: 12.922285286299804\n",
      "episode: 533  empirical returns: 12.922285286285284  predicted returns: 12.922285286292748\n",
      "episode: 534  empirical returns: 12.922285286285284  predicted returns: 12.92228528629148\n",
      "episode: 535  empirical returns: 12.922285286285286  predicted returns: 12.92228528627771\n",
      "episode: 536  empirical returns: 12.922285286285284  predicted returns: 12.922285286322563\n",
      "episode: 537  empirical returns: 12.922285286285284  predicted returns: 12.922285286285552\n",
      "episode: 538  empirical returns: 12.922285286285286  predicted returns: 12.922285286293128\n",
      "episode: 539  empirical returns: 12.922285286285286  predicted returns: 12.922285286356981\n",
      "episode: 540  empirical returns: 12.922285286285286  predicted returns: 12.922285286299804\n",
      "episode: 541  empirical returns: 12.922285286285284  predicted returns: 12.922285286294112\n",
      "episode: 542  empirical returns: 12.922285286285286  predicted returns: 12.922285286294208\n",
      "episode: 543  empirical returns: 12.922285286285287  predicted returns: 12.9222852862658\n",
      "episode: 544  empirical returns: 12.922285286285286  predicted returns: 12.922285286271865\n",
      "episode: 545  empirical returns: 12.922285286285286  predicted returns: 12.92228528624085\n",
      "episode: 546  empirical returns: 12.922285286285286  predicted returns: 12.922285286276262\n",
      "episode: 547  empirical returns: 12.922285286285287  predicted returns: 12.922285286279894\n",
      "episode: 548  empirical returns: 12.922285286285287  predicted returns: 12.922285286279326\n",
      "episode: 549  empirical returns: 12.922285286285286  predicted returns: 12.922285286344248\n",
      "episode: 550  empirical returns: 12.922285286285287  predicted returns: 12.922285286286362\n",
      "episode: 551  empirical returns: 12.922285286285286  predicted returns: 12.922285286284303\n",
      "episode: 552  empirical returns: 12.922285286285286  predicted returns: 12.922285286284874\n",
      "episode: 553  empirical returns: 12.922285286285287  predicted returns: 12.922285286283682\n",
      "episode: 554  empirical returns: 12.922285286285286  predicted returns: 12.922285286286902\n",
      "episode: 555  empirical returns: 12.922285286285286  predicted returns: 12.92228528628302\n",
      "episode: 556  empirical returns: 12.922285286285286  predicted returns: 12.922285286285\n",
      "episode: 557  empirical returns: 12.922285286285286  predicted returns: 12.922285286286067\n",
      "episode: 558  empirical returns: 12.922285286285286  predicted returns: 12.922285286287046\n",
      "episode: 559  empirical returns: 12.922285286285286  predicted returns: 12.922285286285963\n",
      "episode: 560  empirical returns: 12.922285286285287  predicted returns: 12.922285286290752\n",
      "episode: 561  empirical returns: 12.922285286285287  predicted returns: 12.922285286287757\n",
      "episode: 562  empirical returns: 12.922285286285286  predicted returns: 12.922285286280617\n",
      "episode: 563  empirical returns: 12.922285286285287  predicted returns: 12.922285286268984\n",
      "episode: 564  empirical returns: 12.922285286285286  predicted returns: 12.922285286263367\n",
      "episode: 565  empirical returns: 12.922285286285286  predicted returns: 12.922285286285256\n",
      "episode: 566  empirical returns: 12.922285286285287  predicted returns: 12.92228528628246\n",
      "episode: 567  empirical returns: 12.922285286285286  predicted returns: 12.922285286287073\n",
      "episode: 568  empirical returns: 12.922285286285286  predicted returns: 12.922285286279713\n",
      "episode: 569  empirical returns: 12.922285286285286  predicted returns: 12.922285286279552\n",
      "episode: 570  empirical returns: 12.922285286285286  predicted returns: 12.922285286391164\n",
      "episode: 571  empirical returns: 12.922285286285286  predicted returns: 12.922285286302312\n",
      "episode: 572  empirical returns: 12.922285286285286  predicted returns: 12.92228528634324\n",
      "episode: 573  empirical returns: 12.922285286285286  predicted returns: 12.922285286275395\n",
      "episode: 574  empirical returns: 12.922285286285286  predicted returns: 12.922285286285376\n",
      "episode: 575  empirical returns: 12.922285286285286  predicted returns: 12.922285286285252\n",
      "episode: 576  empirical returns: 12.922285286285287  predicted returns: 12.922285286283921\n",
      "episode: 577  empirical returns: 12.922285286285286  predicted returns: 12.922285286282742\n",
      "episode: 578  empirical returns: 12.922285286285286  predicted returns: 12.922285286287789\n",
      "episode: 579  empirical returns: 12.922285286285287  predicted returns: 12.922285286282978\n",
      "episode: 580  empirical returns: 12.922285286285286  predicted returns: 12.922285286283053\n",
      "episode: 581  empirical returns: 12.922285286285286  predicted returns: 12.92228528628764\n",
      "episode: 582  empirical returns: 12.922285286285286  predicted returns: 12.922285286281708\n",
      "episode: 583  empirical returns: 12.922285286285286  predicted returns: 12.92228528628351\n",
      "episode: 584  empirical returns: 12.922285286285286  predicted returns: 12.922285286278537\n",
      "episode: 585  empirical returns: 12.922285286285286  predicted returns: 12.922285286278159\n",
      "episode: 586  empirical returns: 12.922285286285287  predicted returns: 12.922285286279674\n",
      "episode: 587  empirical returns: 12.922285286285286  predicted returns: 12.922285286281316\n",
      "episode: 588  empirical returns: 12.922285286285286  predicted returns: 12.922285286276875\n",
      "episode: 589  empirical returns: 12.922285286285287  predicted returns: 12.92228528627852\n",
      "episode: 590  empirical returns: 12.922285286285287  predicted returns: 12.922285286279854\n",
      "episode: 591  empirical returns: 12.922285286285286  predicted returns: 12.922285286279497\n",
      "episode: 592  empirical returns: 12.922285286285286  predicted returns: 12.922285286277862\n",
      "episode: 593  empirical returns: 12.922285286285286  predicted returns: 12.922285286271745\n",
      "episode: 594  empirical returns: 12.922285286285286  predicted returns: 12.922285286277704\n",
      "episode: 595  empirical returns: 12.922285286285286  predicted returns: 12.922285286281312\n",
      "episode: 596  empirical returns: 12.922285286285287  predicted returns: 12.92228528627848\n",
      "episode: 597  empirical returns: 12.922285286285286  predicted returns: 12.922285286277635\n",
      "episode: 598  empirical returns: 12.922285286285286  predicted returns: 12.922285286273215\n",
      "episode: 599  empirical returns: 12.922285286285286  predicted returns: 12.9222852862566\n",
      "episode: 600  empirical returns: 12.922285286285284  predicted returns: 12.922285286288783\n",
      "episode: 601  empirical returns: 12.922285286285286  predicted returns: 12.92228528729288\n",
      "episode: 602  empirical returns: 12.922285286285287  predicted returns: 12.922285286280058\n",
      "episode: 603  empirical returns: 12.922285286285286  predicted returns: 12.922285286297416\n",
      "episode: 604  empirical returns: 12.922285286285286  predicted returns: 12.922285286304565\n",
      "episode: 605  empirical returns: 12.922285286285286  predicted returns: 12.922285286290368\n",
      "episode: 606  empirical returns: 12.922285286285287  predicted returns: 12.922285286286849\n",
      "episode: 607  empirical returns: 12.922285286285286  predicted returns: 12.922285286292404\n",
      "episode: 608  empirical returns: 12.922285286285286  predicted returns: 12.922285286278749\n",
      "episode: 609  empirical returns: 12.922285286285286  predicted returns: 12.922285286291814\n",
      "episode: 610  empirical returns: 12.922285286285286  predicted returns: 12.922285286291824\n",
      "episode: 611  empirical returns: 12.922285286285286  predicted returns: 12.92228528631604\n",
      "episode: 612  empirical returns: 12.922285286285287  predicted returns: 12.922285286286389\n",
      "episode: 613  empirical returns: 12.922285286285286  predicted returns: 12.922285286324524\n",
      "episode: 614  empirical returns: 12.922285286285286  predicted returns: 12.922285286287803\n",
      "episode: 615  empirical returns: 12.922285286285286  predicted returns: 12.922285286316068\n",
      "episode: 616  empirical returns: 12.922285286285284  predicted returns: 12.922285286286229\n",
      "episode: 617  empirical returns: 12.922285286285286  predicted returns: 12.92228528628559\n",
      "episode: 618  empirical returns: 12.922285286285286  predicted returns: 12.922285286289561\n",
      "episode: 619  empirical returns: 12.922285286285286  predicted returns: 12.92228528628665\n",
      "episode: 620  empirical returns: 12.922285286285286  predicted returns: 12.922285286282845\n",
      "episode: 621  empirical returns: 12.922285286285286  predicted returns: 12.922285286284069\n",
      "episode: 622  empirical returns: 12.922285286285287  predicted returns: 12.922285286282472\n",
      "episode: 623  empirical returns: 12.922285286285286  predicted returns: 12.922285286285476\n",
      "episode: 624  empirical returns: 12.922285286285286  predicted returns: 12.922285286284648\n",
      "episode: 625  empirical returns: 12.922285286285286  predicted returns: 12.922285286283579\n",
      "episode: 626  empirical returns: 12.922285286285284  predicted returns: 12.922285286281408\n",
      "episode: 627  empirical returns: 12.922285286285286  predicted returns: 12.922285286280324\n",
      "episode: 628  empirical returns: 12.922285286285286  predicted returns: 12.922285286281172\n",
      "episode: 629  empirical returns: 12.922285286285286  predicted returns: 12.92228528628236\n",
      "episode: 630  empirical returns: 12.922285286285287  predicted returns: 12.922285286284476\n",
      "episode: 631  empirical returns: 12.922285286285284  predicted returns: 12.922285286281484\n",
      "episode: 632  empirical returns: 12.922285286285286  predicted returns: 12.92228528627902\n",
      "episode: 633  empirical returns: 12.922285286285286  predicted returns: 12.922285286276\n",
      "episode: 634  empirical returns: 12.922285286285284  predicted returns: 12.922285286281523\n",
      "episode: 635  empirical returns: 12.922285286285287  predicted returns: 12.922285286280497\n",
      "episode: 636  empirical returns: 12.922285286285286  predicted returns: 12.922285286278262\n",
      "episode: 637  empirical returns: 12.922285286285286  predicted returns: 12.922285286277168\n",
      "episode: 638  empirical returns: 12.922285286285287  predicted returns: 12.922285286283353\n",
      "episode: 639  empirical returns: 12.922285286285284  predicted returns: 12.92228528624537\n",
      "episode: 640  empirical returns: 12.922285286285286  predicted returns: 12.922285286274724\n",
      "episode: 641  empirical returns: 12.922285286285286  predicted returns: 12.922285286279298\n",
      "episode: 642  empirical returns: 12.922285286285284  predicted returns: 12.922285286280275\n",
      "episode: 643  empirical returns: 12.922285286285286  predicted returns: 12.922285286286101\n",
      "episode: 644  empirical returns: 12.922285286285286  predicted returns: 12.922285286277969\n",
      "episode: 645  empirical returns: 12.922285286285284  predicted returns: 12.922285286282893\n",
      "episode: 646  empirical returns: 12.922285286285286  predicted returns: 12.922285286281951\n",
      "episode: 647  empirical returns: 12.922285286285286  predicted returns: 12.922285286282216\n",
      "episode: 648  empirical returns: 12.922285286285284  predicted returns: 12.922285286276608\n",
      "episode: 649  empirical returns: 12.922285286285284  predicted returns: 12.922285286283284\n",
      "episode: 650  empirical returns: 12.922285286285286  predicted returns: 12.922285286286526\n",
      "episode: 651  empirical returns: 12.922285286285284  predicted returns: 12.922285286287432\n",
      "episode: 652  empirical returns: 12.922285286285284  predicted returns: 12.92228528630039\n",
      "episode: 653  empirical returns: 12.922285286285286  predicted returns: 12.922285286285325\n",
      "episode: 654  empirical returns: 12.922285286285286  predicted returns: 12.922285286277653\n",
      "episode: 655  empirical returns: 12.922285286285284  predicted returns: 12.922285286286598\n",
      "episode: 656  empirical returns: 12.922285286285286  predicted returns: 12.922285286284652\n",
      "episode: 657  empirical returns: 12.922285286285284  predicted returns: 12.922285286296944\n",
      "episode: 658  empirical returns: 12.922285286285287  predicted returns: 12.922285286284433\n",
      "episode: 659  empirical returns: 12.922285286285286  predicted returns: 12.922285286278672\n",
      "episode: 660  empirical returns: 12.922285286285284  predicted returns: 12.92228528627885\n",
      "episode: 661  empirical returns: 12.922285286285287  predicted returns: 12.92228528627038\n",
      "episode: 662  empirical returns: 12.922285286285284  predicted returns: 12.92228528628241\n",
      "episode: 663  empirical returns: 12.922285286285282  predicted returns: 12.922285286281467\n",
      "episode: 664  empirical returns: 12.922285286285286  predicted returns: 12.92228528627786\n",
      "episode: 665  empirical returns: 12.922285286285284  predicted returns: 12.922285286279163\n",
      "episode: 666  empirical returns: 12.922285286285282  predicted returns: 12.922285286282591\n",
      "episode: 667  empirical returns: 12.922285286285286  predicted returns: 12.922285286281152\n",
      "episode: 668  empirical returns: 12.922285286285284  predicted returns: 12.922285286282323\n",
      "episode: 669  empirical returns: 12.922285286285286  predicted returns: 12.922285286282499\n",
      "episode: 670  empirical returns: 12.922285286285286  predicted returns: 12.922285286282314\n",
      "episode: 671  empirical returns: 12.922285286285284  predicted returns: 12.922285286284028\n",
      "episode: 672  empirical returns: 12.922285286285286  predicted returns: 12.92228528628361\n",
      "episode: 673  empirical returns: 12.922285286285286  predicted returns: 12.922285286285309\n",
      "episode: 674  empirical returns: 12.922285286285284  predicted returns: 12.922285286282658\n",
      "episode: 675  empirical returns: 12.922285286285284  predicted returns: 12.922285286284392\n",
      "episode: 676  empirical returns: 12.922285286285286  predicted returns: 12.922285286283255\n",
      "episode: 677  empirical returns: 12.922285286285284  predicted returns: 12.922285286118722\n",
      "episode: 678  empirical returns: 12.922285286285284  predicted returns: 12.922285286284463\n",
      "episode: 679  empirical returns: 12.922285286285286  predicted returns: 12.922285286287803\n",
      "episode: 680  empirical returns: 12.922285286285284  predicted returns: 12.922285286285561\n",
      "episode: 681  empirical returns: 12.922285286285284  predicted returns: 12.922285286286247\n",
      "episode: 682  empirical returns: 12.922285286285286  predicted returns: 12.922285286682381\n",
      "episode: 683  empirical returns: 12.922285286285284  predicted returns: 12.922285286247813\n",
      "episode: 684  empirical returns: 12.922285286285284  predicted returns: 12.922285286283056\n",
      "episode: 685  empirical returns: 12.922285286285286  predicted returns: 12.92228528628575\n",
      "episode: 686  empirical returns: 12.922285286285286  predicted returns: 12.922285286277042\n",
      "episode: 687  empirical returns: 12.922285286285284  predicted returns: 12.922285286287554\n",
      "episode: 688  empirical returns: 12.922285286285284  predicted returns: 12.922285286280658\n",
      "episode: 689  empirical returns: 12.922285286285282  predicted returns: 12.922285286252333\n",
      "episode: 690  empirical returns: 12.922285286285286  predicted returns: 12.922285286316455\n",
      "episode: 691  empirical returns: 12.922285286285284  predicted returns: 12.922285286288835\n",
      "episode: 692  empirical returns: 12.922285286285282  predicted returns: 12.922285286286687\n",
      "episode: 693  empirical returns: 12.922285286285286  predicted returns: 12.9222852862882\n",
      "episode: 694  empirical returns: 12.922285286285284  predicted returns: 12.922285286280333\n",
      "episode: 695  empirical returns: 12.922285286285282  predicted returns: 12.922285286285486\n",
      "episode: 696  empirical returns: 12.922285286285286  predicted returns: 12.922285286286286\n",
      "episode: 697  empirical returns: 12.922285286285284  predicted returns: 12.922285286285437\n",
      "episode: 698  empirical returns: 12.922285286285282  predicted returns: 12.922285286285195\n",
      "episode: 699  empirical returns: 12.922285286285286  predicted returns: 12.922285286283188\n",
      "episode: 700  empirical returns: 12.922285286285284  predicted returns: 12.922285286286089\n",
      "episode: 701  empirical returns: 12.922285286285284  predicted returns: 12.922285286287012\n",
      "episode: 702  empirical returns: 12.922285286285286  predicted returns: 12.922285286280584\n",
      "episode: 703  empirical returns: 12.922285286285284  predicted returns: 12.922285286278836\n",
      "episode: 704  empirical returns: 12.922285286285284  predicted returns: 12.922285286286714\n",
      "episode: 705  empirical returns: 12.922285286285286  predicted returns: 12.922285286286481\n",
      "episode: 706  empirical returns: 12.922285286285284  predicted returns: 12.922285286281564\n",
      "episode: 707  empirical returns: 12.922285286285284  predicted returns: 12.922285286297772\n",
      "episode: 708  empirical returns: 12.922285286285286  predicted returns: 12.922285286283657\n",
      "episode: 709  empirical returns: 12.922285286285284  predicted returns: 12.922285286284957\n",
      "episode: 710  empirical returns: 12.922285286285284  predicted returns: 12.922285286330009\n",
      "episode: 711  empirical returns: 12.922285286285284  predicted returns: 12.922285286285115\n",
      "episode: 712  empirical returns: 12.922285286285282  predicted returns: 12.922285286283788\n",
      "episode: 713  empirical returns: 12.922285286285284  predicted returns: 12.922285286285808\n",
      "episode: 714  empirical returns: 12.922285286285284  predicted returns: 12.92228528628497\n",
      "episode: 715  empirical returns: 12.922285286285282  predicted returns: 12.922285286284499\n",
      "episode: 716  empirical returns: 12.922285286285284  predicted returns: 12.922285286285572\n",
      "episode: 717  empirical returns: 12.922285286285284  predicted returns: 12.922285286286971\n",
      "episode: 718  empirical returns: 12.922285286285286  predicted returns: 12.922285286287753\n",
      "episode: 719  empirical returns: 12.922285286285284  predicted returns: 12.922285286287282\n",
      "episode: 720  empirical returns: 12.922285286285284  predicted returns: 12.922285286288043\n",
      "episode: 721  empirical returns: 12.922285286285282  predicted returns: 12.922285286288076\n",
      "episode: 722  empirical returns: 12.922285286285286  predicted returns: 12.922285286286133\n",
      "episode: 723  empirical returns: 12.922285286285284  predicted returns: 12.922285286291423\n",
      "episode: 724  empirical returns: 12.922285286285282  predicted returns: 12.922285286291968\n",
      "episode: 725  empirical returns: 12.922285286285286  predicted returns: 12.922285286288698\n",
      "episode: 726  empirical returns: 12.922285286285284  predicted returns: 12.922285286286625\n",
      "episode: 727  empirical returns: 12.922285286285282  predicted returns: 12.922285286296965\n",
      "episode: 728  empirical returns: 12.922285286285286  predicted returns: 12.92228528628224\n",
      "episode: 729  empirical returns: 12.922285286285284  predicted returns: 12.922285286290633\n",
      "episode: 730  empirical returns: 12.922285286285282  predicted returns: 12.9222852862903\n",
      "episode: 731  empirical returns: 12.922285286285286  predicted returns: 12.922285286280287\n",
      "episode: 732  empirical returns: 12.922285286285284  predicted returns: 12.922285286279749\n",
      "episode: 733  empirical returns: 12.922285286285284  predicted returns: 12.92228528628674\n",
      "episode: 734  empirical returns: 12.922285286285286  predicted returns: 12.922285286283511\n",
      "episode: 735  empirical returns: 12.922285286285284  predicted returns: 12.922285286286758\n",
      "episode: 736  empirical returns: 12.922285286285284  predicted returns: 12.922285286285645\n",
      "episode: 737  empirical returns: 12.922285286285284  predicted returns: 12.922285286286508\n",
      "episode: 738  empirical returns: 12.922285286285282  predicted returns: 12.92228528629661\n",
      "episode: 739  empirical returns: 12.922285286285284  predicted returns: 12.92228528634513\n",
      "episode: 740  empirical returns: 12.922285286285284  predicted returns: 12.922285286288131\n",
      "episode: 741  empirical returns: 12.922285286285282  predicted returns: 12.922285286286025\n",
      "episode: 742  empirical returns: 12.922285286285284  predicted returns: 12.922285286293569\n",
      "episode: 743  empirical returns: 12.922285286285284  predicted returns: 12.922285286288842\n",
      "episode: 744  empirical returns: 12.922285286285282  predicted returns: 12.922285286289046\n",
      "episode: 745  empirical returns: 12.922285286285284  predicted returns: 12.92228528628555\n",
      "episode: 746  empirical returns: 12.922285286285284  predicted returns: 12.922285286287774\n",
      "episode: 747  empirical returns: 12.922285286285282  predicted returns: 12.92228528629133\n",
      "episode: 748  empirical returns: 12.922285286285284  predicted returns: 12.922285286332091\n",
      "episode: 749  empirical returns: 12.922285286285284  predicted returns: 12.922285286289195\n",
      "episode: 750  empirical returns: 12.922285286285284  predicted returns: 12.922285286285774\n",
      "episode: 751  empirical returns: 12.922285286285282  predicted returns: 12.922285286289238\n",
      "episode: 752  empirical returns: 12.922285286285284  predicted returns: 12.92228528628988\n",
      "episode: 753  empirical returns: 12.922285286285282  predicted returns: 12.92228528628831\n",
      "episode: 754  empirical returns: 12.922285286285286  predicted returns: 12.922285286289629\n",
      "episode: 755  empirical returns: 12.922285286285284  predicted returns: 12.92228528628847\n",
      "episode: 756  empirical returns: 12.922285286285282  predicted returns: 12.922285286288874\n",
      "episode: 757  empirical returns: 12.922285286285286  predicted returns: 12.922285286291402\n",
      "episode: 758  empirical returns: 12.922285286285284  predicted returns: 12.92228528629067\n",
      "episode: 759  empirical returns: 12.922285286285282  predicted returns: 12.922285286293558\n",
      "episode: 760  empirical returns: 12.922285286285284  predicted returns: 12.92228528628873\n",
      "episode: 761  empirical returns: 12.922285286285284  predicted returns: 12.922285286286781\n",
      "episode: 762  empirical returns: 12.922285286285282  predicted returns: 12.92228528629012\n",
      "episode: 763  empirical returns: 12.922285286285284  predicted returns: 12.92228528628802\n",
      "episode: 764  empirical returns: 12.922285286285282  predicted returns: 12.922285286286593\n",
      "episode: 765  empirical returns: 12.922285286285284  predicted returns: 12.922285286251277\n",
      "episode: 766  empirical returns: 12.922285286285284  predicted returns: 12.922285286294791\n",
      "episode: 767  empirical returns: 12.922285286285282  predicted returns: 12.92228528648453\n",
      "episode: 768  empirical returns: 12.922285286285284  predicted returns: 12.922285286282008\n",
      "episode: 769  empirical returns: 12.922285286285284  predicted returns: 12.92228528628343\n",
      "episode: 770  empirical returns: 12.922285286285282  predicted returns: 12.922285286284065\n",
      "episode: 771  empirical returns: 12.922285286285284  predicted returns: 12.92228528627908\n",
      "episode: 772  empirical returns: 12.922285286285284  predicted returns: 12.922285286282582\n",
      "episode: 773  empirical returns: 12.922285286285282  predicted returns: 12.922285286281937\n",
      "episode: 774  empirical returns: 12.922285286285284  predicted returns: 12.922285286285472\n",
      "episode: 775  empirical returns: 12.922285286285284  predicted returns: 12.922285286283795\n",
      "episode: 776  empirical returns: 12.922285286285282  predicted returns: 12.92228528628536\n",
      "episode: 777  empirical returns: 12.922285286285282  predicted returns: 12.922285286285538\n",
      "episode: 778  empirical returns: 12.922285286285284  predicted returns: 12.922285286284383\n",
      "episode: 779  empirical returns: 12.922285286285282  predicted returns: 12.92228528628404\n",
      "episode: 780  empirical returns: 12.922285286285282  predicted returns: 12.922285286284561\n",
      "episode: 781  empirical returns: 12.922285286285284  predicted returns: 12.922285286282511\n",
      "episode: 782  empirical returns: 12.922285286285284  predicted returns: 12.922285286283161\n",
      "episode: 783  empirical returns: 12.922285286285282  predicted returns: 12.922285286284124\n",
      "episode: 784  empirical returns: 12.922285286285284  predicted returns: 12.922285286283756\n",
      "episode: 785  empirical returns: 12.922285286285282  predicted returns: 12.922285286283055\n",
      "episode: 786  empirical returns: 12.922285286285284  predicted returns: 12.922285286282321\n",
      "episode: 787  empirical returns: 12.922285286285284  predicted returns: 12.922285286283657\n",
      "episode: 788  empirical returns: 12.922285286285282  predicted returns: 12.922285286062603\n",
      "episode: 789  empirical returns: 12.922285286285284  predicted returns: 12.922285285758221\n",
      "episode: 790  empirical returns: 12.922285286285282  predicted returns: 12.922285286278905\n",
      "episode: 791  empirical returns: 12.922285286285282  predicted returns: 12.922285286288005\n",
      "episode: 792  empirical returns: 12.922285286285284  predicted returns: 12.922285286283946\n",
      "episode: 793  empirical returns: 12.922285286285282  predicted returns: 12.922285286260658\n",
      "episode: 794  empirical returns: 12.92228528628528  predicted returns: 12.922285285633905\n",
      "episode: 795  empirical returns: 12.922285286285284  predicted returns: 12.922285286284986\n",
      "episode: 796  empirical returns: 12.922285286285282  predicted returns: 12.922285286285334\n",
      "episode: 797  empirical returns: 12.922285286285284  predicted returns: 12.922285286308654\n",
      "episode: 798  empirical returns: 12.922285286285284  predicted returns: 12.922285286287186\n",
      "episode: 799  empirical returns: 12.922285286285282  predicted returns: 12.922285286286003\n",
      "episode: 800  empirical returns: 12.922285286285284  predicted returns: 12.922285286284847\n",
      "episode: 801  empirical returns: 12.922285286285284  predicted returns: 12.922285286290425\n",
      "episode: 802  empirical returns: 12.922285286285282  predicted returns: 12.92228528628847\n",
      "episode: 803  empirical returns: 12.922285286285282  predicted returns: 12.922285286269426\n",
      "episode: 804  empirical returns: 12.922285286285284  predicted returns: 12.922285286277603\n",
      "episode: 805  empirical returns: 12.922285286285282  predicted returns: 12.922285286278985\n",
      "episode: 806  empirical returns: 12.922285286285282  predicted returns: 12.922285286275036\n",
      "episode: 807  empirical returns: 12.922285286285284  predicted returns: 12.922285286309231\n",
      "episode: 808  empirical returns: 12.922285286285282  predicted returns: 12.92228528628165\n",
      "episode: 809  empirical returns: 12.922285286285282  predicted returns: 12.922285286302351\n",
      "episode: 810  empirical returns: 12.922285286285284  predicted returns: 12.922285286286797\n",
      "episode: 811  empirical returns: 12.922285286285282  predicted returns: 12.922285286288762\n",
      "episode: 812  empirical returns: 12.922285286285282  predicted returns: 12.922285286291043\n",
      "episode: 813  empirical returns: 12.922285286285284  predicted returns: 12.922285286109513\n",
      "episode: 814  empirical returns: 12.922285286285284  predicted returns: 12.922285286279001\n",
      "episode: 815  empirical returns: 12.922285286285282  predicted returns: 12.922285286256823\n",
      "episode: 816  empirical returns: 12.922285286285282  predicted returns: 12.922285286275677\n",
      "episode: 817  empirical returns: 12.922285286285282  predicted returns: 12.922285286287046\n",
      "episode: 818  empirical returns: 12.922285286285284  predicted returns: 12.922285286368236\n",
      "episode: 819  empirical returns: 12.922285286285282  predicted returns: 12.92228528629154\n",
      "episode: 820  empirical returns: 12.92228528628528  predicted returns: 12.922285286434175\n",
      "episode: 821  empirical returns: 12.922285286285284  predicted returns: 12.922285286316214\n",
      "episode: 822  empirical returns: 12.922285286285282  predicted returns: 12.92228528626687\n",
      "episode: 823  empirical returns: 12.92228528628528  predicted returns: 12.922285286277152\n",
      "episode: 824  empirical returns: 12.922285286285284  predicted returns: 12.922285286270437\n",
      "episode: 825  empirical returns: 12.922285286285282  predicted returns: 12.92228528627954\n",
      "episode: 826  empirical returns: 12.92228528628528  predicted returns: 12.922285286279582\n",
      "episode: 827  empirical returns: 12.922285286285284  predicted returns: 12.92228528628019\n",
      "episode: 828  empirical returns: 12.922285286285282  predicted returns: 12.922285286281596\n",
      "episode: 829  empirical returns: 12.922285286285282  predicted returns: 12.922285286282067\n",
      "episode: 830  empirical returns: 12.922285286285284  predicted returns: 12.922285286281122\n",
      "episode: 831  empirical returns: 12.922285286285282  predicted returns: 12.922285286279255\n",
      "episode: 832  empirical returns: 12.922285286285282  predicted returns: 12.922285286281738\n",
      "episode: 833  empirical returns: 12.922285286285284  predicted returns: 12.922285286282502\n",
      "episode: 834  empirical returns: 12.922285286285282  predicted returns: 12.922285286283559\n",
      "episode: 835  empirical returns: 12.922285286285282  predicted returns: 12.922285286282799\n",
      "episode: 836  empirical returns: 12.922285286285284  predicted returns: 12.922285286283007\n",
      "episode: 837  empirical returns: 12.922285286285282  predicted returns: 12.922285286281689\n",
      "episode: 838  empirical returns: 12.922285286285282  predicted returns: 12.922285286285065\n",
      "episode: 839  empirical returns: 12.922285286285282  predicted returns: 12.922285286283662\n",
      "episode: 840  empirical returns: 12.922285286285282  predicted returns: 12.922285286283495\n",
      "episode: 841  empirical returns: 12.922285286285282  predicted returns: 12.922285286283595\n",
      "episode: 842  empirical returns: 12.922285286285282  predicted returns: 12.922285286285044\n",
      "episode: 843  empirical returns: 12.922285286285282  predicted returns: 12.92228528628562\n",
      "episode: 844  empirical returns: 12.922285286285282  predicted returns: 12.92228528628289\n",
      "episode: 845  empirical returns: 12.922285286285282  predicted returns: 12.922285286283522\n",
      "episode: 846  empirical returns: 12.922285286285284  predicted returns: 12.922285286283314\n",
      "episode: 847  empirical returns: 12.922285286285282  predicted returns: 12.922285286284849\n",
      "episode: 848  empirical returns: 12.922285286285282  predicted returns: 12.922285286284646\n",
      "episode: 849  empirical returns: 12.92228528628528  predicted returns: 12.922285286285465\n",
      "episode: 850  empirical returns: 12.922285286285284  predicted returns: 12.922285286292734\n",
      "episode: 851  empirical returns: 12.922285286285282  predicted returns: 12.922285286277992\n",
      "episode: 852  empirical returns: 12.92228528628528  predicted returns: 12.922285286282568\n",
      "episode: 853  empirical returns: 12.922285286285284  predicted returns: 12.922285286286789\n",
      "episode: 854  empirical returns: 12.922285286285282  predicted returns: 12.922285286271375\n",
      "episode: 855  empirical returns: 12.92228528628528  predicted returns: 12.922285286297349\n",
      "episode: 856  empirical returns: 12.922285286285284  predicted returns: 12.922285286294574\n",
      "episode: 857  empirical returns: 12.922285286285282  predicted returns: 12.92228528628894\n",
      "episode: 858  empirical returns: 12.92228528628528  predicted returns: 12.922285286276777\n",
      "episode: 859  empirical returns: 12.922285286285284  predicted returns: 12.922285286253619\n",
      "episode: 860  empirical returns: 12.922285286285282  predicted returns: 12.92228528627617\n",
      "episode: 861  empirical returns: 12.922285286285282  predicted returns: 12.922285286279816\n",
      "episode: 862  empirical returns: 12.922285286285284  predicted returns: 12.922285286281557\n",
      "episode: 863  empirical returns: 12.922285286285282  predicted returns: 12.922285286272633\n",
      "episode: 864  empirical returns: 12.922285286285282  predicted returns: 12.922285286272992\n",
      "episode: 865  empirical returns: 12.922285286285282  predicted returns: 12.922285286280115\n",
      "episode: 866  empirical returns: 12.922285286285282  predicted returns: 12.922285286281843\n",
      "episode: 867  empirical returns: 12.922285286285282  predicted returns: 12.922285286281095\n",
      "episode: 868  empirical returns: 12.922285286285282  predicted returns: 12.922285286282234\n",
      "episode: 869  empirical returns: 12.922285286285282  predicted returns: 12.922285286281184\n",
      "episode: 870  empirical returns: 12.922285286285282  predicted returns: 12.922285286281875\n",
      "episode: 871  empirical returns: 12.922285286285282  predicted returns: 12.922285286283085\n",
      "episode: 872  empirical returns: 12.92228528628528  predicted returns: 12.92228528628772\n",
      "episode: 873  empirical returns: 12.922285286285282  predicted returns: 12.922285286281706\n",
      "episode: 874  empirical returns: 12.922285286285282  predicted returns: 12.922285286294986\n",
      "episode: 875  empirical returns: 12.92228528628528  predicted returns: 12.922285286285543\n",
      "episode: 876  empirical returns: 12.922285286285282  predicted returns: 12.922285286298461\n",
      "episode: 877  empirical returns: 12.922285286285282  predicted returns: 12.922285286298592\n",
      "episode: 878  empirical returns: 12.922285286285282  predicted returns: 12.922285286304525\n",
      "episode: 879  empirical returns: 12.922285286285282  predicted returns: 12.922285286275171\n",
      "episode: 880  empirical returns: 12.922285286285282  predicted returns: 12.922285286281516\n",
      "episode: 881  empirical returns: 12.92228528628528  predicted returns: 12.922285286284938\n",
      "episode: 882  empirical returns: 12.922285286285284  predicted returns: 12.922285286298774\n",
      "episode: 883  empirical returns: 12.922285286285282  predicted returns: 12.922285286288805\n",
      "episode: 884  empirical returns: 12.92228528628528  predicted returns: 12.92228528631237\n",
      "episode: 885  empirical returns: 12.922285286285284  predicted returns: 12.92228528631017\n",
      "episode: 886  empirical returns: 12.922285286285282  predicted returns: 12.922285286293542\n",
      "episode: 887  empirical returns: 12.92228528628528  predicted returns: 12.922285287482794\n",
      "episode: 888  empirical returns: 12.922285286285284  predicted returns: 12.922285286306913\n",
      "episode: 889  empirical returns: 12.922285286285282  predicted returns: 12.922285286409142\n",
      "episode: 890  empirical returns: 12.92228528628528  predicted returns: 12.922285284405234\n",
      "episode: 891  empirical returns: 12.922285286285282  predicted returns: 12.922285286301161\n",
      "episode: 892  empirical returns: 12.922285286285282  predicted returns: 12.92228528629694\n",
      "episode: 893  empirical returns: 12.922285286285282  predicted returns: 12.922285286392352\n",
      "episode: 894  empirical returns: 12.922285286285282  predicted returns: 12.922285286294155\n",
      "episode: 895  empirical returns: 12.922285286285282  predicted returns: 12.922285286293842\n",
      "episode: 896  empirical returns: 12.922285286285282  predicted returns: 12.92228528629084\n",
      "episode: 897  empirical returns: 12.922285286285282  predicted returns: 12.922285286291519\n",
      "episode: 898  empirical returns: 12.92228528628528  predicted returns: 12.922285286290432\n",
      "episode: 899  empirical returns: 12.922285286285282  predicted returns: 12.922285286295619\n",
      "episode: 900  empirical returns: 12.922285286285282  predicted returns: 12.922285286291391\n",
      "episode: 901  empirical returns: 12.92228528628528  predicted returns: 12.922285286282772\n",
      "episode: 902  empirical returns: 12.922285286285282  predicted returns: 12.9222852862801\n",
      "episode: 903  empirical returns: 12.922285286285282  predicted returns: 12.922285286286694\n",
      "episode: 904  empirical returns: 12.92228528628528  predicted returns: 12.92228528629138\n",
      "episode: 905  empirical returns: 12.922285286285282  predicted returns: 12.922285286300095\n",
      "episode: 906  empirical returns: 12.922285286285282  predicted returns: 12.92228528628841\n",
      "episode: 907  empirical returns: 12.92228528628528  predicted returns: 12.922285286302799\n",
      "episode: 908  empirical returns: 12.922285286285282  predicted returns: 12.922285286293175\n",
      "episode: 909  empirical returns: 12.922285286285282  predicted returns: 12.922285286291942\n",
      "episode: 910  empirical returns: 12.922285286285282  predicted returns: 12.92228528629296\n",
      "episode: 911  empirical returns: 12.92228528628528  predicted returns: 12.922285286446396\n",
      "episode: 912  empirical returns: 12.922285286285282  predicted returns: 12.922285286058468\n",
      "episode: 913  empirical returns: 12.92228528628528  predicted returns: 12.92228528623631\n",
      "episode: 914  empirical returns: 12.922285286285284  predicted returns: 12.922285285800172\n",
      "episode: 915  empirical returns: 12.922285286285282  predicted returns: 12.922285286285945\n",
      "episode: 916  empirical returns: 12.92228528628528  predicted returns: 12.922285286224366\n",
      "episode: 917  empirical returns: 12.922285286285282  predicted returns: 12.922285286163937\n",
      "episode: 918  empirical returns: 12.922285286285282  predicted returns: 12.922285286213466\n",
      "episode: 919  empirical returns: 12.92228528628528  predicted returns: 12.92228528629915\n",
      "episode: 920  empirical returns: 12.922285286285282  predicted returns: 12.922285286277965\n",
      "episode: 921  empirical returns: 12.922285286285282  predicted returns: 12.92228528627476\n",
      "episode: 922  empirical returns: 12.92228528628528  predicted returns: 12.922285286255665\n",
      "episode: 923  empirical returns: 12.922285286285282  predicted returns: 12.922285286267076\n",
      "episode: 924  empirical returns: 12.92228528628528  predicted returns: 12.922285286277416\n",
      "episode: 925  empirical returns: 12.922285286285282  predicted returns: 12.922285286274168\n",
      "episode: 926  empirical returns: 12.922285286285282  predicted returns: 12.922285286270906\n",
      "episode: 927  empirical returns: 12.92228528628528  predicted returns: 12.922285286270213\n",
      "episode: 928  empirical returns: 12.922285286285282  predicted returns: 12.922285286272054\n",
      "episode: 929  empirical returns: 12.922285286285282  predicted returns: 12.922285286275901\n",
      "episode: 930  empirical returns: 12.92228528628528  predicted returns: 12.922285286280488\n",
      "episode: 931  empirical returns: 12.922285286285282  predicted returns: 12.9222852862768\n",
      "episode: 932  empirical returns: 12.922285286285282  predicted returns: 12.922285286279585\n",
      "episode: 933  empirical returns: 12.92228528628528  predicted returns: 12.922285286274255\n",
      "episode: 934  empirical returns: 12.922285286285282  predicted returns: 12.92228528627703\n",
      "episode: 935  empirical returns: 12.922285286285282  predicted returns: 12.922285286276775\n",
      "episode: 936  empirical returns: 12.92228528628528  predicted returns: 12.922285286274967\n",
      "episode: 937  empirical returns: 12.922285286285282  predicted returns: 12.922285286279447\n",
      "episode: 938  empirical returns: 12.922285286285282  predicted returns: 12.922285286274105\n",
      "episode: 939  empirical returns: 12.92228528628528  predicted returns: 12.922285286276233\n",
      "episode: 940  empirical returns: 12.92228528628528  predicted returns: 12.922285286274757\n",
      "episode: 941  empirical returns: 12.922285286285282  predicted returns: 12.922285286271999\n",
      "episode: 942  empirical returns: 12.922285286285282  predicted returns: 12.92228528627183\n",
      "episode: 943  empirical returns: 12.92228528628528  predicted returns: 12.922285286276058\n",
      "episode: 944  empirical returns: 12.922285286285282  predicted returns: 12.922285286277386\n",
      "episode: 945  empirical returns: 12.92228528628528  predicted returns: 12.922285286278303\n",
      "episode: 946  empirical returns: 12.922285286285282  predicted returns: 12.922285286278525\n",
      "episode: 947  empirical returns: 12.922285286285282  predicted returns: 12.922285286276924\n",
      "episode: 948  empirical returns: 12.92228528628528  predicted returns: 12.922285286276658\n",
      "episode: 949  empirical returns: 12.922285286285282  predicted returns: 12.922285286266758\n",
      "episode: 950  empirical returns: 12.922285286285282  predicted returns: 12.922285286282388\n",
      "episode: 951  empirical returns: 12.92228528628528  predicted returns: 12.922285286277653\n",
      "episode: 952  empirical returns: 12.922285286285282  predicted returns: 12.922285286276654\n",
      "episode: 953  empirical returns: 12.92228528628528  predicted returns: 12.922285286277797\n",
      "episode: 954  empirical returns: 12.92228528628528  predicted returns: 12.92228528632134\n",
      "episode: 955  empirical returns: 12.922285286285282  predicted returns: 12.92228528627503\n",
      "episode: 956  empirical returns: 12.92228528628528  predicted returns: 12.922285286339743\n",
      "episode: 957  empirical returns: 12.922285286285282  predicted returns: 12.922285286279383\n",
      "episode: 958  empirical returns: 12.922285286285282  predicted returns: 12.922285286286197\n",
      "episode: 959  empirical returns: 12.92228528628528  predicted returns: 12.922285286270364\n",
      "episode: 960  empirical returns: 12.922285286285282  predicted returns: 12.922285286284229\n",
      "episode: 961  empirical returns: 12.922285286285282  predicted returns: 12.922285286286417\n",
      "episode: 962  empirical returns: 12.92228528628528  predicted returns: 12.9222852862699\n",
      "episode: 963  empirical returns: 12.922285286285282  predicted returns: 12.922285286283493\n",
      "episode: 964  empirical returns: 12.922285286285282  predicted returns: 12.92228528627578\n",
      "episode: 965  empirical returns: 12.92228528628528  predicted returns: 12.922285286275711\n",
      "episode: 966  empirical returns: 12.92228528628528  predicted returns: 12.922285286278761\n",
      "episode: 967  empirical returns: 12.922285286285282  predicted returns: 12.922285286306693\n",
      "episode: 968  empirical returns: 12.92228528628528  predicted returns: 12.922285286267474\n",
      "episode: 969  empirical returns: 12.92228528628528  predicted returns: 12.922285283914789\n",
      "episode: 970  empirical returns: 12.922285286285282  predicted returns: 12.922285286310986\n",
      "episode: 971  empirical returns: 12.92228528628528  predicted returns: 12.922285286291395\n",
      "episode: 972  empirical returns: 12.92228528628528  predicted returns: 12.92228528629348\n",
      "episode: 973  empirical returns: 12.922285286285282  predicted returns: 12.922285286289089\n",
      "episode: 974  empirical returns: 12.922285286285282  predicted returns: 12.92228528628755\n",
      "episode: 975  empirical returns: 12.92228528628528  predicted returns: 12.922285286290291\n",
      "episode: 976  empirical returns: 12.922285286285282  predicted returns: 12.922285286287712\n",
      "episode: 977  empirical returns: 12.92228528628528  predicted returns: 12.922285286286295\n",
      "episode: 978  empirical returns: 12.922285286285282  predicted returns: 12.922285286287126\n",
      "episode: 979  empirical returns: 12.92228528628528  predicted returns: 12.922285286287803\n",
      "episode: 980  empirical returns: 12.92228528628528  predicted returns: 12.922285286282815\n",
      "episode: 981  empirical returns: 12.922285286285282  predicted returns: 12.922285286286147\n",
      "episode: 982  empirical returns: 12.92228528628528  predicted returns: 12.922285286284847\n",
      "episode: 983  empirical returns: 12.92228528628528  predicted returns: 12.922285286288162\n",
      "episode: 984  empirical returns: 12.922285286285282  predicted returns: 12.922285286284065\n",
      "episode: 985  empirical returns: 12.92228528628528  predicted returns: 12.922285286284865\n",
      "episode: 986  empirical returns: 12.922285286285282  predicted returns: 12.922285286288997\n",
      "episode: 987  empirical returns: 12.922285286285282  predicted returns: 12.922285286281191\n",
      "episode: 988  empirical returns: 12.92228528628528  predicted returns: 12.922285286278765\n",
      "episode: 989  empirical returns: 12.922285286285282  predicted returns: 12.922285286283394\n",
      "episode: 990  empirical returns: 12.922285286285282  predicted returns: 12.922285286301175\n",
      "episode: 991  empirical returns: 12.92228528628528  predicted returns: 12.922285286353485\n",
      "episode: 992  empirical returns: 12.92228528628528  predicted returns: 12.922285286280925\n",
      "episode: 993  empirical returns: 12.922285286285282  predicted returns: 12.922285286283639\n",
      "episode: 994  empirical returns: 12.92228528628528  predicted returns: 12.922285286299712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m phi\u001b[38;5;241m.\u001b[39mget_fourier_feature([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 44\u001b[0m     weights, targets, features, returns \u001b[38;5;241m=\u001b[39m \u001b[43mols_monte_carlo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m returns:\n\u001b[1;32m     46\u001b[0m         Returns\u001b[38;5;241m.\u001b[39mappend(returns)\n",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m, in \u001b[0;36mols_monte_carlo\u001b[0;34m(env, phi, weights, targets, features, x0, gamma)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x\u001b[38;5;241m-\u001b[39mx0) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.00001\u001b[39m:\n\u001b[1;32m     11\u001b[0m         returns\u001b[38;5;241m.\u001b[39mappend(G)\n\u001b[0;32m---> 12\u001b[0m     features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     targets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m G \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m     14\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(features, targets)\n",
      "File \u001b[0;32m~/envs/adaptive-time-env/lib/python3.9/site-packages/numpy/core/numeric.py:925\u001b[0m, in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    923\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a)\n\u001b[1;32m    924\u001b[0m b \u001b[38;5;241m=\u001b[39m asarray(b)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def ols_monte_carlo(env, phi, weights, targets, features, x0, gamma = 0.999):\n",
    "    trajectory = generate_trajectory(env)\n",
    "    N = len(trajectory)\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for t in range(N-1,-1,-1):\n",
    "        state, _, reward, _ = trajectory[t]\n",
    "        G = gamma*G + reward\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        if np.linalg.norm(x-x0) < 0.00001:\n",
    "            returns.append(G)\n",
    "        features += np.outer(x,x)\n",
    "        targets += G * x\n",
    "    weights = np.linalg.solve(features, targets)\n",
    "    return weights, targets, features, np.mean(returns)\n",
    "\n",
    "\n",
    "def gradient_monte_carlo(env, phi, weights, x0, gamma = 1, alpha = 0.001):\n",
    "    trajectory = generate_trajectory(env)\n",
    "    N = len(trajectory)\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for t in range(N-1,-1,-1):\n",
    "        state, _, reward, _ = trajectory[t]\n",
    "        G = gamma * G + reward\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        if np.linalg.norm(x-x0) < 0.00001:\n",
    "            returns.append(G)\n",
    "        weights = weights + alpha * (G - np.inner(x, weights)) * x \n",
    "    return weights, np.mean(returns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "Returns = []\n",
    "observation, _ = env.reset()\n",
    "d = len(phi.get_fourier_feature(observation))\n",
    "features = np.zeros((d,d))\n",
    "targets = np.zeros(d)\n",
    "weights = np.zeros(d)\n",
    "x = phi.get_fourier_feature([0,0,0,0])\n",
    "for episode in range(num_episodes):\n",
    "    weights, targets, features, returns = ols_monte_carlo(env, phi, weights, targets, features, x)\n",
    "    if returns:\n",
    "        Returns.append(returns)\n",
    "    if Returns:\n",
    "        print('episode:',episode, ' empirical returns:' , np.mean(Returns), ' predicted returns:' , np.inner(x,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_monte_carlo_control(env, phi, weights, targets, features, x0, gamma = 199/200):\n",
    "    trajectory = generate_trajectory(env)\n",
    "    N = len(trajectory)\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for t in range(N-1,-1,-1):\n",
    "        state, action, reward, _ = trajectory[t]\n",
    "        G = gamma*G + reward\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        features[action] += np.outer(x,x)\n",
    "        targets[action] += G * x\n",
    "    for action in range(2)\n",
    "    weights = np.linalg.solve(features, targets)\n",
    "    return weights, targets, features, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([1., 0., 0.], dtype=float32),\n",
       "  array([1.683155], dtype=float32),\n",
       "  -0.002833010554083572,\n",
       "  array([0.9999203 , 0.01262333, 0.25247324], dtype=float32)],\n",
       " [array([0.9999203 , 0.01262333, 0.25247324], dtype=float32),\n",
       "  array([-1.1154854], dtype=float32),\n",
       "  -0.007777938334370556,\n",
       "  array([0.99984944, 0.01735369, 0.09461792], dtype=float32)],\n",
       " [array([0.99984944, 0.01735369, 0.09461792], dtype=float32),\n",
       "  array([-0.27207616], dtype=float32),\n",
       "  -0.0012704612358089731,\n",
       "  array([0.99978584, 0.02069417, 0.06682176], dtype=float32)],\n",
       " [array([0.99978584, 0.02069417, 0.06682176], dtype=float32),\n",
       "  array([-1.2429843], dtype=float32),\n",
       "  -0.002419834533924224,\n",
       "  array([ 0.99988   ,  0.01548976, -0.10410526], dtype=float32)],\n",
       " [array([ 0.99988   ,  0.01548976, -0.10410526], dtype=float32),\n",
       "  array([0.7708461], dtype=float32),\n",
       "  -0.001917946138216929,\n",
       "  array([0.9998614 , 0.01664656, 0.02313899], dtype=float32)],\n",
       " [array([0.9998614 , 0.01664656, 0.02313899], dtype=float32),\n",
       "  array([1.5309985], dtype=float32),\n",
       "  -0.002674631280615725,\n",
       "  array([0.9995527 , 0.02990656, 0.2652737 ], dtype=float32)],\n",
       " [array([0.9995527 , 0.02990656, 0.2652737 ], dtype=float32),\n",
       "  array([0.0636156], dtype=float32),\n",
       "  -0.007935728472830545,\n",
       "  array([0.99899787, 0.04475836, 0.29724595], dtype=float32)],\n",
       " [array([0.99899787, 0.04475836, 0.29724595], dtype=float32),\n",
       "  array([-0.1537691], dtype=float32),\n",
       "  -0.010863809383658923,\n",
       "  array([0.9981909 , 0.0601245 , 0.30774933], dtype=float32)],\n",
       " [array([0.9981909 , 0.0601245 , 0.30774933], dtype=float32),\n",
       "  array([-1.7270974], dtype=float32),\n",
       "  -0.016073150594624577,\n",
       "  array([0.997898  , 0.06480424, 0.0937781 ], dtype=float32)],\n",
       " [array([0.997898  , 0.06480424, 0.0937781 ], dtype=float32),\n",
       "  array([0.4686143], dtype=float32),\n",
       "  -0.005304514449322417,\n",
       "  array([0.9971525 , 0.0754117 , 0.21267343], dtype=float32)],\n",
       " [array([0.9971525 , 0.0754117 , 0.21267343], dtype=float32),\n",
       "  array([-0.15941904], dtype=float32),\n",
       "  -0.010246150597217637,\n",
       "  array([0.9961525 , 0.08763676, 0.24531935], dtype=float32)],\n",
       " [array([0.9961525 , 0.08763676, 0.24531935], dtype=float32),\n",
       "  array([0.9105883], dtype=float32),\n",
       "  -0.014547273431551908,\n",
       "  array([0.9939417 , 0.10990859, 0.44763517], dtype=float32)],\n",
       " [array([0.9939417 , 0.10990859, 0.44763517], dtype=float32),\n",
       "  array([1.1390798], dtype=float32),\n",
       "  -0.0334640824656115,\n",
       "  array([0.98948026, 0.14466807, 0.70092857], dtype=float32)],\n",
       " [array([0.98948026, 0.14466807, 0.70092857], dtype=float32),\n",
       "  array([-1.1343527], dtype=float32),\n",
       "  -0.07149335354174213,\n",
       "  array([0.98435146, 0.17621638, 0.63927674], dtype=float32)],\n",
       " [array([0.98435146, 0.17621638, 0.63927674], dtype=float32),\n",
       "  array([-1.0306522], dtype=float32),\n",
       "  -0.07330877587969754,\n",
       "  array([0.9784493, 0.2064872, 0.6168412], dtype=float32)],\n",
       " [array([0.9784493, 0.2064872, 0.6168412], dtype=float32),\n",
       "  array([1.2212427], dtype=float32),\n",
       "  -0.08279783808620672,\n",
       "  array([0.9674794 , 0.25294986, 0.954893  ], dtype=float32)],\n",
       " [array([0.9674794 , 0.25294986, 0.954893  ], dtype=float32),\n",
       "  array([-0.89857644], dtype=float32),\n",
       "  -0.15738634109287586,\n",
       "  array([0.9534802, 0.3014557, 1.0098189], dtype=float32)],\n",
       " [array([0.9534802, 0.3014557, 1.0098189], dtype=float32),\n",
       "  array([-1.3561716], dtype=float32),\n",
       "  -0.19758270376444786,\n",
       "  array([0.9366544 , 0.35025492, 1.0324849 ], dtype=float32)],\n",
       " [array([0.9366544 , 0.35025492, 1.0324849 ], dtype=float32),\n",
       "  array([-1.2384211], dtype=float32),\n",
       "  -0.23618799521053135,\n",
       "  array([0.91579485, 0.40164638, 1.109413  ], dtype=float32)],\n",
       " [array([0.91579485, 0.40164638, 1.109413  ], dtype=float32),\n",
       "  array([0.53321403], dtype=float32),\n",
       "  -0.2941924114092565,\n",
       "  array([0.8833448, 0.4687237, 1.4906299], dtype=float32)],\n",
       " [array([0.8833448, 0.4687237, 1.4906299], dtype=float32),\n",
       "  array([1.2864397], dtype=float32),\n",
       "  -0.4618457932234979,\n",
       "  array([0.83116186, 0.5560305 , 2.0351386 ], dtype=float32)],\n",
       " [array([0.83116186, 0.5560305 , 2.0351386 ], dtype=float32),\n",
       "  array([-0.5496144], dtype=float32),\n",
       "  -0.7621118868743728,\n",
       "  array([0.7596066, 0.6503828, 2.3697193], dtype=float32)],\n",
       " [array([0.7596066, 0.6503828, 2.3697193], dtype=float32),\n",
       "  array([-1.9644824], dtype=float32),\n",
       "  -1.0668051893495998,\n",
       "  array([0.6702654, 0.7421215, 2.562834 ], dtype=float32)],\n",
       " [array([0.6702654, 0.7421215, 2.562834 ], dtype=float32),\n",
       "  array([-0.7094432], dtype=float32),\n",
       "  -1.3565957637748534,\n",
       "  array([0.5512952 , 0.83431023, 3.0130088 ], dtype=float32)],\n",
       " [array([0.5512952 , 0.83431023, 3.0130088 ], dtype=float32),\n",
       "  array([0.31540728], dtype=float32),\n",
       "  -1.8818546180859663,\n",
       "  array([0.3890621 , 0.92121154, 3.6860526 ], dtype=float32)],\n",
       " [array([0.3890621 , 0.92121154, 3.6860526 ], dtype=float32),\n",
       "  array([1.7140976], dtype=float32),\n",
       "  -2.733306170946505,\n",
       "  array([0.1671216, 0.9859363, 4.6340756], dtype=float32)],\n",
       " [array([0.1671216, 0.9859363, 4.6340756], dtype=float32),\n",
       "  array([0.35085833], dtype=float32),\n",
       "  -4.115680422443874,\n",
       "  array([-0.10321421,  0.9946591 ,  5.4261565 ], dtype=float32)],\n",
       " [array([-0.10321421,  0.9946591 ,  5.4261565 ], dtype=float32),\n",
       "  array([-0.3916473], dtype=float32),\n",
       "  -5.747398911965193,\n",
       "  array([-0.3977549 ,  0.91749173,  6.113404  ], dtype=float32)],\n",
       " [array([-0.3977549 ,  0.91749173,  6.113404  ], dtype=float32),\n",
       "  array([-0.01802846], dtype=float32),\n",
       "  -7.657236126364136,\n",
       "  array([-0.6809137,  0.7323637,  6.7988186], dtype=float32)],\n",
       " [array([-0.6809137,  0.7323637,  6.7988186], dtype=float32),\n",
       "  array([0.44729754], dtype=float32),\n",
       "  -10.004092380934267,\n",
       "  array([-0.8999997 ,  0.43589056,  7.415186  ], dtype=float32)],\n",
       " [array([-0.8999997 ,  0.43589056,  7.415186  ], dtype=float32),\n",
       "  array([0.6333334], dtype=float32),\n",
       "  -12.738039898271255,\n",
       "  array([-0.9982498 ,  0.05913784,  7.837104  ], dtype=float32)],\n",
       " [array([-0.9982498 ,  0.05913784,  7.837104  ], dtype=float32),\n",
       "  array([-1.8248631], dtype=float32),\n",
       "  -15.646664663826353,\n",
       "  array([-0.94885284, -0.31571874,  7.6077275 ], dtype=float32)],\n",
       " [array([-0.94885284, -0.31571874,  7.6077275 ], dtype=float32),\n",
       "  array([0.65424854], dtype=float32),\n",
       "  -13.742715795256942,\n",
       "  array([-0.7682664, -0.6401303,  7.469076 ], dtype=float32)],\n",
       " [array([-0.7682664, -0.6401303,  7.469076 ], dtype=float32),\n",
       "  array([-0.3581257], dtype=float32),\n",
       "  -11.566278974859841,\n",
       "  array([-0.5049858 , -0.86312765,  6.9352593 ], dtype=float32)],\n",
       " [array([-0.5049858 , -0.86312765,  6.9352593 ], dtype=float32),\n",
       "  array([-1.5195719], dtype=float32),\n",
       "  -9.222771249258246,\n",
       "  array([-0.22443834, -0.9744883 ,  6.059978  ], dtype=float32)],\n",
       " [array([-0.22443834, -0.9744883 ,  6.059978  ], dtype=float32),\n",
       "  array([1.3866532], dtype=float32),\n",
       "  -6.904050736086173,\n",
       "  array([ 0.05046741, -0.9987257 ,  5.53711   ], dtype=float32)],\n",
       " [array([ 0.05046741, -0.9987257 ,  5.53711   ], dtype=float32),\n",
       "  array([-0.92381686], dtype=float32),\n",
       "  -5.378146606086366,\n",
       "  array([ 0.27920252, -0.96023226,  4.6494927 ], dtype=float32)],\n",
       " [array([ 0.27920252, -0.96023226,  4.6494927 ], dtype=float32),\n",
       "  array([0.04560879], dtype=float32),\n",
       "  -3.8202938683796854,\n",
       "  array([ 0.4615765, -0.8871004,  3.93616  ], dtype=float32)],\n",
       " [array([ 0.4615765, -0.8871004,  3.93616  ], dtype=float32),\n",
       "  array([-1.9272026], dtype=float32),\n",
       "  -2.74338484670939,\n",
       "  array([ 0.58822256, -0.8086991 ,  2.9817543 ], dtype=float32)],\n",
       " [array([ 0.58822256, -0.8086991 ,  2.9817543 ], dtype=float32),\n",
       "  array([0.7888782], dtype=float32),\n",
       "  -1.776953729766757,\n",
       "  array([ 0.6842227 , -0.72927314,  2.4935617 ], dtype=float32)],\n",
       " [array([ 0.6842227 , -0.72927314,  2.4935617 ], dtype=float32),\n",
       "  array([1.9002979], dtype=float32),\n",
       "  -1.2933084617845623,\n",
       "  array([ 0.761173 , -0.6485489,  2.2316515], dtype=float32)],\n",
       " [array([ 0.761173 , -0.6485489,  2.2316515], dtype=float32),\n",
       "  array([1.1582464], dtype=float32),\n",
       "  -0.9973476558040024,\n",
       "  array([ 0.819804 , -0.5726442,  1.9189769], dtype=float32)],\n",
       " [array([ 0.819804 , -0.5726442,  1.9189769], dtype=float32),\n",
       "  array([-1.9929368], dtype=float32),\n",
       "  -0.7439867728300558,\n",
       "  array([ 0.85242   , -0.52285767,  1.1905532 ], dtype=float32)],\n",
       " [array([ 0.85242   , -0.52285767,  1.1905532 ], dtype=float32),\n",
       "  array([-1.9214301], dtype=float32),\n",
       "  -0.44815356998684575,\n",
       "  array([ 0.8654792 , -0.5009449 ,  0.51019543], dtype=float32)],\n",
       " [array([ 0.8654792 , -0.5009449 ,  0.51019543], dtype=float32),\n",
       "  array([-0.777839], dtype=float32),\n",
       "  -0.301934753575931,\n",
       "  array([ 0.86592495, -0.5001739 ,  0.01781091], dtype=float32)],\n",
       " [array([ 0.86592495, -0.5001739 ,  0.01781091], dtype=float32),\n",
       "  array([0.16315831], dtype=float32),\n",
       "  -0.274424398829821,\n",
       "  array([ 0.85748136, -0.514515  , -0.3328458 ], dtype=float32)],\n",
       " [array([ 0.85748136, -0.514515  , -0.3328458 ], dtype=float32),\n",
       "  array([1.687844], dtype=float32),\n",
       "  -0.30600492183480554,\n",
       "  array([ 0.8452734 , -0.53433406, -0.46555543], dtype=float32)],\n",
       " [array([ 0.8452734 , -0.53433406, -0.46555543], dtype=float32),\n",
       "  array([1.4308783], dtype=float32),\n",
       "  -0.3415014919737951,\n",
       "  array([ 0.82741725, -0.5615877 , -0.6516742 ], dtype=float32)],\n",
       " [array([ 0.82741725, -0.5615877 , -0.6516742 ], dtype=float32),\n",
       "  array([-0.87678], dtype=float32),\n",
       "  -0.39881442320783195,\n",
       "  array([ 0.79211956, -0.6103659 , -1.204382  ], dtype=float32)],\n",
       " [array([ 0.79211956, -0.6103659 , -1.204382  ], dtype=float32),\n",
       "  array([-0.5481978], dtype=float32),\n",
       "  -0.5763759163739963,\n",
       "  array([ 0.73594034, -0.6770464 , -1.7443861 ], dtype=float32)],\n",
       " [array([ 0.73594034, -0.6770464 , -1.7443861 ], dtype=float32),\n",
       "  array([1.1880592], dtype=float32),\n",
       "  -0.8588516602161479,\n",
       "  array([ 0.66190434, -0.7495883 , -2.073962  ], dtype=float32)],\n",
       " [array([ 0.66190434, -0.7495883 , -2.073962  ], dtype=float32),\n",
       "  array([1.0769955], dtype=float32),\n",
       "  -1.149446177748111,\n",
       "  array([ 0.5643339, -0.8255466, -2.474604 ], dtype=float32)],\n",
       " [array([ 0.5643339, -0.8255466, -2.474604 ], dtype=float32),\n",
       "  array([-0.64954543], dtype=float32),\n",
       "  -1.5559597693835752,\n",
       "  array([ 0.42599955, -0.90472335, -3.1911957 ], dtype=float32)],\n",
       " [array([ 0.42599955, -0.90472335, -3.1911957 ], dtype=float32),\n",
       "  array([-0.47835624], dtype=float32),\n",
       "  -2.297151946852315,\n",
       "  array([ 0.24060765, -0.9706225 , -3.9414916 ], dtype=float32)],\n",
       " [array([ 0.24060765, -0.9706225 , -3.9414916 ], dtype=float32),\n",
       "  array([0.05950388], dtype=float32),\n",
       "  -3.316603961828112,\n",
       "  array([ 0.00996503, -0.99995035, -4.660533  ], dtype=float32)],\n",
       " [array([ 0.00996503, -0.99995035, -4.660533  ], dtype=float32),\n",
       "  array([-1.3771452], dtype=float32),\n",
       "  -4.610147089738244,\n",
       "  array([-0.26758733, -0.96353364, -5.6170673 ], dtype=float32)],\n",
       " [array([-0.26758733, -0.96353364, -5.6170673 ], dtype=float32),\n",
       "  array([-0.4252207], dtype=float32),\n",
       "  -6.547127350472664,\n",
       "  array([-0.5572442, -0.8303487, -6.403501 ], dtype=float32)],\n",
       " [array([-0.5572442, -0.8303487, -6.403501 ], dtype=float32),\n",
       "  array([-1.4169494], dtype=float32),\n",
       "  -8.77612669665305,\n",
       "  array([-0.8151589, -0.5792374, -7.238805 ], dtype=float32)],\n",
       " [array([-0.8151589, -0.5792374, -7.238805 ], dtype=float32),\n",
       "  array([-1.4116733], dtype=float32),\n",
       "  -11.611587538959016,\n",
       "  array([-0.975118  , -0.22168651, -7.884984  ], dtype=float32)],\n",
       " [array([-0.975118  , -0.22168651, -7.884984  ], dtype=float32),\n",
       "  array([-1.1575361], dtype=float32),\n",
       "  -14.733646601786813,\n",
       "  array([-0.9844719 ,  0.17554203, -8.        ], dtype=float32)],\n",
       " [array([-0.9844719 ,  0.17554203, -8.        ], dtype=float32),\n",
       "  array([1.4364227], dtype=float32),\n",
       "  -15.194096807599106,\n",
       "  array([-0.84773266,  0.5304237 , -7.65288   ], dtype=float32)],\n",
       " [array([-0.84773266,  0.5304237 , -7.65288   ], dtype=float32),\n",
       "  array([0.09094265], dtype=float32),\n",
       "  -12.525932266025913,\n",
       "  array([-0.6048878,  0.7963107, -7.2414207], dtype=float32)],\n",
       " [array([-0.6048878,  0.7963107, -7.2414207], dtype=float32),\n",
       "  array([-1.4997264], dtype=float32),\n",
       "  -10.17633752178305,\n",
       "  array([-0.3014067,  0.9534957, -6.869147 ], dtype=float32)],\n",
       " [array([-0.3014067,  0.9534957, -6.869147 ], dtype=float32),\n",
       "  array([-1.9071345], dtype=float32),\n",
       "  -8.245148619947196,\n",
       "  array([ 0.01583648,  0.9998746 , -6.4400954 ], dtype=float32)],\n",
       " [array([ 0.01583648,  0.9998746 , -6.4400954 ], dtype=float32),\n",
       "  array([0.78120095], dtype=float32),\n",
       "  -6.565991014059088,\n",
       "  array([ 0.29024956,  0.956951  , -5.573009  ], dtype=float32)],\n",
       " [array([ 0.29024956,  0.956951  , -5.573009  ], dtype=float32),\n",
       "  array([1.0436058], dtype=float32),\n",
       "  -4.735896183283114,\n",
       "  array([ 0.5050374,  0.8630974, -4.6987553], dtype=float32)],\n",
       " [array([ 0.5050374,  0.8630974, -4.6987553], dtype=float32),\n",
       "  array([0.12619872], dtype=float32),\n",
       "  -3.2922993261824818,\n",
       "  array([ 0.667652  ,  0.74447346, -4.032502  ], dtype=float32)],\n",
       " [array([ 0.667652  ,  0.74447346, -4.032502  ], dtype=float32),\n",
       "  array([0.09903611], dtype=float32),\n",
       "  -2.331290293985831,\n",
       "  array([ 0.7858164,  0.6184599, -3.4592917], dtype=float32)],\n",
       " [array([ 0.7858164,  0.6184599, -3.4592917], dtype=float32),\n",
       "  array([-1.6225222], dtype=float32),\n",
       "  -1.6438997225045506,\n",
       "  array([ 0.8752518 ,  0.48366743, -3.238825  ], dtype=float32)],\n",
       " [array([ 0.8752518 ,  0.48366743, -3.238825  ], dtype=float32),\n",
       "  array([-1.5660648], dtype=float32),\n",
       "  -1.3063147945184903,\n",
       "  array([ 0.93961567,  0.34223154, -3.110984  ], dtype=float32)],\n",
       " [array([ 0.93961567,  0.34223154, -3.110984  ], dtype=float32),\n",
       "  array([1.0513152], dtype=float32),\n",
       "  -1.0909316151681545,\n",
       "  array([ 0.97709143,  0.21282   , -2.6966133 ], dtype=float32)],\n",
       " [array([ 0.97709143,  0.21282   , -2.6966133 ], dtype=float32),\n",
       "  array([-0.14043604], dtype=float32),\n",
       "  -0.7731851948779436,\n",
       "  array([ 0.9962563 ,  0.08644894, -2.5580637 ], dtype=float32)],\n",
       " [array([ 0.9962563 ,  0.08644894, -2.5580637 ], dtype=float32),\n",
       "  array([0.03377432], dtype=float32),\n",
       "  -0.6618622312395092,\n",
       "  array([ 0.99928373, -0.03784202, -2.4881608 ], dtype=float32)],\n",
       " [array([ 0.99928373, -0.03784202, -2.4881608 ], dtype=float32),\n",
       "  array([-0.8052094], dtype=float32),\n",
       "  -0.6211754976845515,\n",
       "  array([ 0.98563254, -0.16890366, -2.6373239 ], dtype=float32)],\n",
       " [array([ 0.98563254, -0.16890366, -2.6373239 ], dtype=float32),\n",
       "  array([1.9314753], dtype=float32),\n",
       "  -0.7280822042887278,\n",
       "  array([ 0.95725703, -0.2892385 , -2.47428   ], dtype=float32)],\n",
       " [array([ 0.95725703, -0.2892385 , -2.47428   ], dtype=float32),\n",
       "  array([-0.15264148], dtype=float32),\n",
       "  -0.698331466551834,\n",
       "  array([ 0.90932536, -0.41608578, -2.7141054 ], dtype=float32)],\n",
       " [array([ 0.90932536, -0.41608578, -2.7141054 ], dtype=float32),\n",
       "  array([1.9169797], dtype=float32),\n",
       "  -0.9244697480593854,\n",
       "  array([ 0.8440165, -0.5363172, -2.7386227], dtype=float32)],\n",
       " [array([ 0.8440165, -0.5363172, -2.7386227], dtype=float32),\n",
       "  array([1.1997668], dtype=float32),\n",
       "  -1.0718774818921748,\n",
       "  array([ 0.75567484, -0.654947  , -2.9608955 ], dtype=float32)],\n",
       " [array([ 0.75567484, -0.654947  , -2.9608955 ], dtype=float32),\n",
       "  array([1.5615494], dtype=float32),\n",
       "  -1.389085324815557,\n",
       "  array([ 0.64099216, -0.7675474 , -3.2178736 ], dtype=float32)],\n",
       " [array([ 0.64099216, -0.7675474 , -3.2178736 ], dtype=float32),\n",
       "  array([1.9833913], dtype=float32),\n",
       "  -1.805040511521858,\n",
       "  array([ 0.49773812, -0.8673274 , -3.4960253 ], dtype=float32)],\n",
       " [array([ 0.49773812, -0.8673274 , -3.4960253 ], dtype=float32),\n",
       "  array([-0.14958975], dtype=float32),\n",
       "  -2.324337241705611,\n",
       "  array([ 0.3074775, -0.9515554, -4.168959 ], dtype=float32)],\n",
       " [array([ 0.3074775, -0.9515554, -4.168959 ], dtype=float32),\n",
       "  array([1.856669], dtype=float32),\n",
       "  -3.3246759398368977,\n",
       "  array([ 0.08224171, -0.9966124 , -4.6041255 ], dtype=float32)],\n",
       " [array([ 0.08224171, -0.9966124 , -4.6041255 ], dtype=float32),\n",
       "  array([-0.5603374], dtype=float32),\n",
       "  -4.335629155068506,\n",
       "  array([-0.18831588, -0.98210853, -5.4356356 ], dtype=float32)],\n",
       " [array([-0.18831588, -0.98210853, -5.4356356 ], dtype=float32),\n",
       "  array([-0.1491133], dtype=float32),\n",
       "  -6.053092300566209,\n",
       "  array([-0.4787025, -0.8779772, -6.194584 ], dtype=float32)],\n",
       " [array([-0.4787025, -0.8779772, -6.194584 ], dtype=float32),\n",
       "  array([-0.6141926], dtype=float32),\n",
       "  -8.122450593540735,\n",
       "  array([-0.7489235, -0.6626565, -6.9451957], dtype=float32)],\n",
       " [array([-0.7489235, -0.6626565, -6.9451957], dtype=float32),\n",
       "  array([-0.8831272], dtype=float32),\n",
       "  -10.667366393289015,\n",
       "  array([-0.9408631 , -0.33878708, -7.574657  ], dtype=float32)],\n",
       " [array([-0.9408631 , -0.33878708, -7.574657  ], dtype=float32),\n",
       "  array([-1.4716889], dtype=float32),\n",
       "  -13.557130268778401,\n",
       "  array([-0.99852216,  0.05434578, -8.        ], dtype=float32)],\n",
       " [array([-0.99852216,  0.05434578, -8.        ], dtype=float32),\n",
       "  array([-1.1291517], dtype=float32),\n",
       "  -15.932202857572268,\n",
       "  array([-0.89853656,  0.43889862, -8.        ], dtype=float32)],\n",
       " [array([-0.89853656,  0.43889862, -8.        ], dtype=float32),\n",
       "  array([-1.6624093], dtype=float32),\n",
       "  -13.623915408013307,\n",
       "  array([-0.65969616,  0.75153244, -7.9201875 ], dtype=float32)],\n",
       " [array([-0.65969616,  0.75153244, -7.9201875 ], dtype=float32),\n",
       "  array([0.17365345], dtype=float32),\n",
       "  -11.522613591116714,\n",
       "  array([-0.34654924,  0.9380318 , -7.33049   ], dtype=float32)],\n",
       " [array([-0.34654924,  0.9380318 , -7.33049   ], dtype=float32),\n",
       "  array([1.4717479], dtype=float32),\n",
       "  -9.080191555921608,\n",
       "  array([-0.03357337,  0.99943626, -6.406204  ], dtype=float32)],\n",
       " [array([-0.03357337,  0.99943626, -6.406204  ], dtype=float32),\n",
       "  array([1.6752946], dtype=float32),\n",
       "  -6.680774057763291,\n",
       "  array([ 0.23448324,  0.97212017, -5.4053326 ], dtype=float32)],\n",
       " [array([ 0.23448324,  0.97212017, -5.4053326 ], dtype=float32),\n",
       "  array([-0.03239063], dtype=float32),\n",
       "  -4.701611020735086,\n",
       "  array([ 0.45354772,  0.891232  , -4.6811013 ], dtype=float32)],\n",
       " [array([ 0.45354772,  0.891232  , -4.6811013 ], dtype=float32),\n",
       "  array([-1.6892685], dtype=float32),\n",
       "  -3.4042439360872,\n",
       "  array([ 0.63193345,  0.7750226 , -4.2660675 ], dtype=float32)],\n",
       " [array([ 0.63193345,  0.7750226 , -4.2660675 ], dtype=float32),\n",
       "  array([-1.242095], dtype=float32),\n",
       "  -2.6078031741949173,\n",
       "  array([ 0.7692082,  0.6389982, -3.8711147], dtype=float32)],\n",
       " [array([ 0.7692082,  0.6389982, -3.8711147], dtype=float32),\n",
       "  array([0.12455221], dtype=float32),\n",
       "  -1.9790880093845327,\n",
       "  array([ 0.86555636,  0.50081146, -3.3731833 ], dtype=float32)],\n",
       " [array([ 0.86555636,  0.50081146, -3.3731833 ], dtype=float32),\n",
       "  array([-0.31985545], dtype=float32),\n",
       "  -1.4130768626260342,\n",
       "  array([ 0.9315083,  0.3637201, -3.045553 ], dtype=float32)],\n",
       " [array([ 0.9315083,  0.3637201, -3.045553 ], dtype=float32),\n",
       "  array([1.29279], dtype=float32),\n",
       "  -1.0677868736357015,\n",
       "  array([ 0.9705444 ,  0.24092245, -2.5788443 ], dtype=float32)],\n",
       " [array([ 0.9705444 ,  0.24092245, -2.5788443 ], dtype=float32),\n",
       "  array([-1.4899367], dtype=float32),\n",
       "  -0.7264664882201135,\n",
       "  array([ 0.9937084 ,  0.11199856, -2.621643  ], dtype=float32)],\n",
       " [array([ 0.9937084 ,  0.11199856, -2.621643  ], dtype=float32),\n",
       "  array([0.03194094], dtype=float32),\n",
       "  -0.6998987015345461,\n",
       "  array([ 0.99989617, -0.01440811, -2.532853  ], dtype=float32)],\n",
       " [array([ 0.99989617, -0.01440811, -2.532853  ], dtype=float32),\n",
       "  array([1.7979206], dtype=float32),\n",
       "  -0.6449745276285549,\n",
       "  array([ 0.9918055 , -0.12775704, -2.2739708 ], dtype=float32)],\n",
       " [array([ 0.9918055 , -0.12775704, -2.2739708 ], dtype=float32),\n",
       "  array([1.8405157], dtype=float32),\n",
       "  -0.5368933226264938,\n",
       "  array([ 0.9730259 , -0.23069581, -2.0937114 ], dtype=float32)],\n",
       " [array([ 0.9730259 , -0.23069581, -2.0937114 ], dtype=float32),\n",
       "  array([1.9651214], dtype=float32),\n",
       "  -0.49641687764026543,\n",
       "  array([ 0.9455907 , -0.32535863, -1.971965  ], dtype=float32)],\n",
       " [array([ 0.9455907 , -0.32535863, -1.971965  ], dtype=float32),\n",
       "  array([0.14429207], dtype=float32),\n",
       "  -0.4987053957609929,\n",
       "  array([ 0.9042792 , -0.42694166, -2.1943402 ], dtype=float32)],\n",
       " [array([ 0.9042792 , -0.42694166, -2.1943402 ], dtype=float32),\n",
       "  array([1.628495], dtype=float32),\n",
       "  -0.6787411261599392,\n",
       "  array([ 0.8500998 , -0.52662164, -2.2702723 ], dtype=float32)],\n",
       " [array([ 0.8500998 , -0.52662164, -2.2702723 ], dtype=float32),\n",
       "  array([-1.6310765], dtype=float32),\n",
       "  -0.8256790882062197,\n",
       "  array([ 0.76476705, -0.64430684, -2.9099    ], dtype=float32)],\n",
       " [array([ 0.76476705, -0.64430684, -2.9099    ], dtype=float32),\n",
       "  array([-1.9516876], dtype=float32),\n",
       "  -1.3407240407580803,\n",
       "  array([ 0.63374543, -0.7735417 , -3.685883  ], dtype=float32)],\n",
       " [array([ 0.63374543, -0.7735417 , -3.685883  ], dtype=float32),\n",
       "  array([-0.5122939], dtype=float32),\n",
       "  -2.141018259509628,\n",
       "  array([ 0.45220983, -0.89191157, -4.3428836 ], dtype=float32)],\n",
       " [array([ 0.45220983, -0.89191157, -4.3428836 ], dtype=float32),\n",
       "  array([-1.2711525], dtype=float32),\n",
       "  -3.1011027584182207,\n",
       "  array([ 0.20759612, -0.9782146 , -5.20249   ], dtype=float32)],\n",
       " [array([ 0.20759612, -0.9782146 , -5.20249   ], dtype=float32),\n",
       "  array([0.98615146], dtype=float32),\n",
       "  -4.561733604922239,\n",
       "  array([-0.08020826, -0.99677813, -5.7882285 ], dtype=float32)],\n",
       " [array([-0.08020826, -0.99677813, -5.7882285 ], dtype=float32),\n",
       "  array([1.0105827], dtype=float32),\n",
       "  -6.077480900237679,\n",
       "  array([-0.38896304, -0.9212534 , -6.3842244 ], dtype=float32)],\n",
       " [array([-0.38896304, -0.9212534 , -6.3842244 ], dtype=float32),\n",
       "  array([-0.8395609], dtype=float32),\n",
       "  -7.958627297001754,\n",
       "  array([-0.6886029, -0.7251386, -7.2010984], dtype=float32)],\n",
       " [array([-0.6886029, -0.7251386, -7.2010984], dtype=float32),\n",
       "  array([0.31789085], dtype=float32),\n",
       "  -10.616246932643572,\n",
       "  array([-0.9104721 , -0.41357052, -7.697269  ], dtype=float32)],\n",
       " [array([-0.9104721 , -0.41357052, -7.697269  ], dtype=float32),\n",
       "  array([-1.3168551], dtype=float32),\n",
       "  -13.298951113566705,\n",
       "  array([-0.99965227, -0.02636916, -8.        ], dtype=float32)],\n",
       " [array([-0.99965227, -0.02636916, -8.        ], dtype=float32),\n",
       "  array([-1.4997908], dtype=float32),\n",
       "  -16.10684775618012,\n",
       "  array([-0.93100935,  0.36499533, -8.        ], dtype=float32)],\n",
       " [array([-0.93100935,  0.36499533, -8.        ], dtype=float32),\n",
       "  array([0.16914088], dtype=float32),\n",
       "  -14.061658119925637,\n",
       "  array([-0.7257503,  0.6879582, -7.7008824], dtype=float32)],\n",
       " [array([-0.7257503,  0.6879582, -7.7008824], dtype=float32),\n",
       "  array([-1.170299], dtype=float32),\n",
       "  -11.61003982960837,\n",
       "  array([-0.4296467,  0.9029971, -7.3604584], dtype=float32)],\n",
       " [array([-0.4296467,  0.9029971, -7.3604584], dtype=float32),\n",
       "  array([-1.3442674], dtype=float32),\n",
       "  -9.479255346360532,\n",
       "  array([-0.09969307,  0.99501824, -6.884851  ], dtype=float32)],\n",
       " [array([-0.09969307,  0.99501824, -6.884851  ], dtype=float32),\n",
       "  array([-1.2573525], dtype=float32),\n",
       "  -7.532787166434396,\n",
       "  array([ 0.21481319,  0.9766551 , -6.32719   ], dtype=float32)],\n",
       " [array([ 0.21481319,  0.9766551 , -6.32719   ], dtype=float32),\n",
       "  array([-0.3783728], dtype=float32),\n",
       "  -5.8375936400228,\n",
       "  array([ 0.47861207,  0.8780265 , -5.6514544 ], dtype=float32)],\n",
       " [array([ 0.47861207,  0.8780265 , -5.6514544 ], dtype=float32),\n",
       "  array([1.8970915], dtype=float32),\n",
       "  -4.346083181726222,\n",
       "  array([ 0.6702101,  0.7421714, -4.708371 ], dtype=float32)],\n",
       " [array([ 0.6702101,  0.7421714, -4.708371 ], dtype=float32),\n",
       "  array([-1.2528695], dtype=float32),\n",
       "  -2.9178506761242957,\n",
       "  array([ 0.8142729 ,  0.58048224, -4.339673  ], dtype=float32)],\n",
       " [array([ 0.8142729 ,  0.58048224, -4.339673  ], dtype=float32),\n",
       "  array([-1.5364325], dtype=float32),\n",
       "  -2.2691950270276178,\n",
       "  array([ 0.9160885 ,  0.40097603, -4.134776  ], dtype=float32)],\n",
       " [array([ 0.9160885 ,  0.40097603, -4.134776  ], dtype=float32),\n",
       "  array([0.16967367], dtype=float32),\n",
       "  -1.879890051531008,\n",
       "  array([ 0.9754255 ,  0.22032963, -3.808593  ], dtype=float32)],\n",
       " [array([ 0.9754255 ,  0.22032963, -3.808593  ], dtype=float32),\n",
       "  array([-1.6537955], dtype=float32),\n",
       "  -1.5026247905912513,\n",
       "  array([ 0.99961966,  0.02757813, -3.891415  ], dtype=float32)],\n",
       " [array([ 0.99961966,  0.02757813, -3.891415  ], dtype=float32),\n",
       "  array([-0.88288146], dtype=float32),\n",
       "  -1.5158513816743477,\n",
       "  array([ 0.98514557, -0.1717212 , -4.003164  ], dtype=float32)],\n",
       " [array([ 0.98514557, -0.1717212 , -4.003164  ], dtype=float32),\n",
       "  array([-1.605288], dtype=float32),\n",
       "  -1.634891601179789,\n",
       "  array([ 0.9244468 , -0.38131094, -4.372748  ], dtype=float32)],\n",
       " [array([ 0.9244468 , -0.38131094, -4.372748  ], dtype=float32),\n",
       "  array([0.6072586], dtype=float32),\n",
       "  -2.0655094759271186,\n",
       "  array([ 0.8141131 , -0.58070636, -4.567642  ], dtype=float32)],\n",
       " [array([ 0.8141131 , -0.58070636, -4.567642  ], dtype=float32),\n",
       "  array([-1.0628446], dtype=float32),\n",
       "  -2.4713644724395443,\n",
       "  array([ 0.6389022, -0.769288 , -5.1625986], dtype=float32)],\n",
       " [array([ 0.6389022, -0.769288 , -5.1625986], dtype=float32),\n",
       "  array([-0.27598554], dtype=float32),\n",
       "  -3.4357216159618127,\n",
       "  array([ 0.39311987, -0.91948724, -5.7809625 ], dtype=float32)],\n",
       " [array([ 0.39311987, -0.91948724, -5.7809625 ], dtype=float32),\n",
       "  array([-1.4494001], dtype=float32),\n",
       "  -4.705415380939597,\n",
       "  array([ 0.06956658, -0.9975773 , -6.687988  ], dtype=float32)],\n",
       " [array([ 0.06956658, -0.9975773 , -6.687988  ], dtype=float32),\n",
       "  array([1.5293592], dtype=float32),\n",
       "  -6.7287791609619045,\n",
       "  array([-0.2866378 , -0.95803905, -7.206767  ], dtype=float32)],\n",
       " [array([-0.2866378 , -0.95803905, -7.206767  ], dtype=float32),\n",
       "  array([1.8676497], dtype=float32),\n",
       "  -8.662463700827626,\n",
       "  array([-0.62331355, -0.78197205, -7.6451488 ], dtype=float32)],\n",
       " [array([-0.62331355, -0.78197205, -7.6451488 ], dtype=float32),\n",
       "  array([-0.44973654], dtype=float32),\n",
       "  -10.879533121951322,\n",
       "  array([-0.878624 , -0.4775142, -8.       ], dtype=float32)],\n",
       " [array([-0.878624 , -0.4775142, -8.       ], dtype=float32),\n",
       "  array([1.772095], dtype=float32),\n",
       "  -13.392656507010278,\n",
       "  array([-0.9952191, -0.0976674, -8.       ], dtype=float32)],\n",
       " [array([-0.9952191, -0.0976674, -8.       ], dtype=float32),\n",
       "  array([0.5820702], dtype=float32),\n",
       "  -15.664870443450942,\n",
       "  array([-0.95489997,  0.29692772, -7.98594   ], dtype=float32)],\n",
       " [array([-0.95489997,  0.29692772, -7.98594   ], dtype=float32),\n",
       "  array([1.9354466], dtype=float32),\n",
       "  -14.447545643955534,\n",
       "  array([-0.78063196,  0.624991  , -7.472927  ], dtype=float32)],\n",
       " [array([-0.78063196,  0.624991  , -7.472927  ], dtype=float32),\n",
       "  array([-1.6215786], dtype=float32),\n",
       "  -11.670580894864672,\n",
       "  array([-0.5083824,  0.8611314, -7.247421 ], dtype=float32)],\n",
       " [array([-0.5083824,  0.8611314, -7.247421 ], dtype=float32),\n",
       "  array([1.2880766], dtype=float32),\n",
       "  -9.68141341562211,\n",
       "  array([-0.21128283,  0.977425  , -6.4083605 ], dtype=float32)],\n",
       " [array([-0.21128283,  0.977425  , -6.4083605 ], dtype=float32),\n",
       "  array([1.9291474], dtype=float32),\n",
       "  -7.291957309183403,\n",
       "  array([ 0.05637885,  0.99840945, -5.38592   ], dtype=float32)],\n",
       " [array([ 0.05637885,  0.99840945, -5.38592   ], dtype=float32),\n",
       "  array([1.7403402], dtype=float32),\n",
       "  -5.197211749152307,\n",
       "  array([ 0.2717508 ,  0.96236765, -4.376062  ], dtype=float32)],\n",
       " [array([ 0.2717508 ,  0.96236765, -4.376062  ], dtype=float32),\n",
       "  array([-1.0400888], dtype=float32),\n",
       "  -3.594612555397838,\n",
       "  array([ 0.44907233,  0.8934954 , -3.8102994 ], dtype=float32)],\n",
       " [array([ 0.44907233,  0.8934954 , -3.8102994 ], dtype=float32),\n",
       "  array([1.4455235], dtype=float32),\n",
       "  -2.675106251851274,\n",
       "  array([ 0.5744191,  0.8185614, -2.9233491], dtype=float32)],\n",
       " [array([ 0.5744191,  0.8185614, -2.9233491], dtype=float32),\n",
       "  array([-0.7541161], dtype=float32),\n",
       "  -1.7746589078979866,\n",
       "  array([ 0.66911817,  0.743156  , -2.4225457 ], dtype=float32)],\n",
       " [array([ 0.66911817,  0.743156  , -2.4225457 ], dtype=float32),\n",
       "  array([0.4976532], dtype=float32),\n",
       "  -1.2889869505818576,\n",
       "  array([ 0.7328818 ,  0.68035597, -1.7905306 ], dtype=float32)],\n",
       " [array([ 0.7328818 ,  0.68035597, -1.7905306 ], dtype=float32),\n",
       "  array([1.0639124], dtype=float32),\n",
       "  -0.881607328847997,\n",
       "  array([ 0.7698346,  0.6382435, -1.1206768], dtype=float32)],\n",
       " [array([ 0.7698346,  0.6382435, -1.1206768], dtype=float32),\n",
       "  array([1.0942289], dtype=float32),\n",
       "  -0.6059497595398609,\n",
       "  array([ 0.78486294,  0.6196694 , -0.47785985], dtype=float32)],\n",
       " [array([ 0.78486294,  0.6196694 , -0.47785985], dtype=float32),\n",
       "  array([-1.0783992], dtype=float32),\n",
       "  -0.4706514547851466,\n",
       "  array([ 0.7902509 ,  0.61278343, -0.17486767], dtype=float32)],\n",
       " [array([ 0.7902509 ,  0.61278343, -0.17486767], dtype=float32),\n",
       "  array([0.5814776], dtype=float32),\n",
       "  -0.4384391586864412,\n",
       "  array([0.7787189 , 0.627373  , 0.37194154], dtype=float32)],\n",
       " [array([0.7787189 , 0.627373  , 0.37194154], dtype=float32),\n",
       "  array([-0.58607733], dtype=float32),\n",
       "  -0.47409900765860646,\n",
       "  array([0.75450087, 0.65629905, 0.7545597 ], dtype=float32)],\n",
       " [array([0.75450087, 0.65629905, 0.7545597 ], dtype=float32),\n",
       "  array([1.507654], dtype=float32),\n",
       "  -0.5717262815237673,\n",
       "  array([0.7041651, 0.7100363, 1.4729321], dtype=float32)],\n",
       " [array([0.7041651, 0.7100363, 1.4729321], dtype=float32),\n",
       "  array([1.0762278], dtype=float32),\n",
       "  -0.8414998689533566,\n",
       "  array([0.623258 , 0.7820163, 2.1668935], dtype=float32)],\n",
       " [array([0.623258 , 0.7820163, 2.1668935], dtype=float32),\n",
       "  array([0.3618305], dtype=float32),\n",
       "  -1.275887919805021,\n",
       "  array([0.50770426, 0.86153144, 2.8076801 ], dtype=float32)],\n",
       " [array([0.50770426, 0.86153144, 2.8076801 ], dtype=float32),\n",
       "  array([-1.5529054], dtype=float32),\n",
       "  -1.8687403173446397,\n",
       "  array([0.36298865, 0.9317936 , 3.220893  ], dtype=float32)],\n",
       " [array([0.36298865, 0.9317936 , 3.220893  ], dtype=float32),\n",
       "  array([-1.2639365], dtype=float32),\n",
       "  -2.477388401703688,\n",
       "  array([0.18391304, 0.9829425 , 3.7301476 ], dtype=float32)],\n",
       " [array([0.18391304, 0.9829425 , 3.7301476 ], dtype=float32),\n",
       "  array([-1.5565925], dtype=float32),\n",
       "  -3.314349056383702,\n",
       "  array([-0.02672418,  0.99964285,  4.2338657 ], dtype=float32)],\n",
       " [array([-0.02672418,  0.99964285,  4.2338657 ], dtype=float32),\n",
       "  array([0.7251777], dtype=float32),\n",
       "  -4.345169689673649,\n",
       "  array([-0.27764907,  0.9606826 ,  5.0923743 ], dtype=float32)],\n",
       " [array([-0.27764907,  0.9606826 ,  5.0923743 ], dtype=float32),\n",
       "  array([-1.7888155], dtype=float32),\n",
       "  -6.02685916527036,\n",
       "  array([-0.52997774,  0.84801155,  5.5445642 ], dtype=float32)],\n",
       " [array([-0.52997774,  0.84801155,  5.5445642 ], dtype=float32),\n",
       "  array([-0.56634444], dtype=float32),\n",
       "  -7.6087590590226535,\n",
       "  array([-0.76002747,  0.64989096,  6.095621  ], dtype=float32)],\n",
       " [array([-0.76002747,  0.64989096,  6.095621  ], dtype=float32),\n",
       "  array([0.8352742], dtype=float32),\n",
       "  -9.641451694351828,\n",
       "  array([-0.93159336,  0.36350214,  6.7083306 ], dtype=float32)],\n",
       " [array([-0.93159336,  0.36350214,  6.7083306 ], dtype=float32),\n",
       "  array([-0.77182513], dtype=float32),\n",
       "  -12.171273451474505,\n",
       "  array([-0.9995863 ,  0.02876133,  6.8651834 ], dtype=float32)],\n",
       " [array([-0.9995863 ,  0.02876133,  6.8651834 ], dtype=float32),\n",
       "  array([1.7988698], dtype=float32),\n",
       "  -14.406004274298137,\n",
       "  array([-0.94634527, -0.32315734,  7.1565847 ], dtype=float32)],\n",
       " [array([-0.94634527, -0.32315734,  7.1565847 ], dtype=float32),\n",
       "  array([-0.95716405], dtype=float32),\n",
       "  -13.032904456420146,\n",
       "  array([-0.78531224, -0.6190999 ,  6.7706423 ], dtype=float32)],\n",
       " [array([-0.78531224, -0.6190999 ,  6.7706423 ], dtype=float32),\n",
       "  array([0.7810979], dtype=float32),\n",
       "  -10.705428817992797,\n",
       "  array([-0.5497176, -0.8353505,  6.423482 ], dtype=float32)],\n",
       " [array([-0.5497176, -0.8353505,  6.423482 ], dtype=float32),\n",
       "  array([1.0789996], dtype=float32),\n",
       "  -8.761920948498807,\n",
       "  array([-0.28027946, -0.95991844,  5.958819  ], dtype=float32)],\n",
       " [array([-0.28027946, -0.95991844,  5.958819  ], dtype=float32),\n",
       "  array([0.04209379], dtype=float32),\n",
       "  -6.991339682836934,\n",
       "  array([-0.02182379, -0.9997618 ,  5.245194  ], dtype=float32)],\n",
       " [array([-0.02182379, -0.9997618 ,  5.245194  ], dtype=float32),\n",
       "  array([0.28529683], dtype=float32),\n",
       "  -5.287731943610288,\n",
       "  array([ 0.20364827, -0.97904414,  4.5381675 ], dtype=float32)],\n",
       " [array([ 0.20364827, -0.97904414,  4.5381675 ], dtype=float32),\n",
       "  array([1.0304348], dtype=float32),\n",
       "  -3.9257313818763104,\n",
       "  array([ 0.39218464, -0.9198865 ,  3.9584494 ], dtype=float32)],\n",
       " [array([ 0.39218464, -0.9198865 ,  3.9584494 ], dtype=float32),\n",
       "  array([-1.2111348], dtype=float32),\n",
       "  -2.9321349075395626,\n",
       "  array([ 0.5289379, -0.8486605,  3.0868645], dtype=float32)],\n",
       " [array([ 0.5289379, -0.8486605,  3.0868645], dtype=float32),\n",
       "  array([-0.71309674], dtype=float32),\n",
       "  -1.9804580942940324,\n",
       "  array([ 0.62452155, -0.7810076 ,  2.3434045 ], dtype=float32)],\n",
       " [array([ 0.62452155, -0.7810076 ,  2.3434045 ], dtype=float32),\n",
       "  array([-1.8533174], dtype=float32),\n",
       "  -1.3559027050652366,\n",
       "  array([ 0.6805414, -0.7327096,  1.4796511], dtype=float32)],\n",
       " [array([ 0.6805414, -0.7327096,  1.4796511], dtype=float32),\n",
       "  array([-0.51755244], dtype=float32),\n",
       "  -0.8953737008460341,\n",
       "  array([ 0.7111451, -0.7030453,  0.8524861], dtype=float32)],\n",
       " [array([ 0.7111451, -0.7030453,  0.8524861], dtype=float32),\n",
       "  array([0.5672789], dtype=float32),\n",
       "  -0.6808814739253051,\n",
       "  array([ 0.7254172 , -0.68830943,  0.41029394], dtype=float32)],\n",
       " [array([ 0.7254172 , -0.68830943,  0.41029394], dtype=float32),\n",
       "  array([1.3802702], dtype=float32),\n",
       "  -0.5950571141444583,\n",
       "  array([ 0.7288874 , -0.6846336 ,  0.10110238], dtype=float32)],\n",
       " [array([ 0.7288874 , -0.6846336 ,  0.10110238], dtype=float32),\n",
       "  array([1.91816], dtype=float32),\n",
       "  -0.5733696701350232,\n",
       "  array([ 0.72460634, -0.689163  , -0.12464882], dtype=float32)],\n",
       " [array([ 0.72460634, -0.689163  , -0.12464882], dtype=float32),\n",
       "  array([-1.4438422], dtype=float32),\n",
       "  -0.5817451990411562,\n",
       "  array([ 0.6943801 , -0.7196084 , -0.85809743], dtype=float32)],\n",
       " [array([ 0.6943801 , -0.7196084 , -0.85809743], dtype=float32),\n",
       "  array([1.4236258], dtype=float32),\n",
       "  -0.7208514498505781,\n",
       "  array([ 0.6505779, -0.7594395, -1.1842599], dtype=float32)],\n",
       " [array([ 0.6505779, -0.7594395, -1.1842599], dtype=float32),\n",
       "  array([-1.7146155], dtype=float32),\n",
       "  -0.8870091217133775,\n",
       "  array([ 0.57105756, -0.82091004, -2.0110319 ], dtype=float32)],\n",
       " [array([ 0.57105756, -0.82091004, -2.0110319 ], dtype=float32),\n",
       "  array([1.5338074], dtype=float32),\n",
       "  -1.334151845879676,\n",
       "  array([ 0.46882617, -0.88329047, -2.3966432 ], dtype=float32)],\n",
       " [array([ 0.46882617, -0.88329047, -2.3966432 ], dtype=float32),\n",
       "  array([-1.4385165], dtype=float32),\n",
       "  -1.7489907538976406,\n",
       "  array([ 0.31856662, -0.9479005 , -3.2748885 ], dtype=float32)],\n",
       " [array([ 0.31856662, -0.9479005 , -3.2748885 ], dtype=float32),\n",
       "  array([-0.57186913], dtype=float32),\n",
       "  -2.626776701183862,\n",
       "  array([ 0.12034483, -0.99273217, -4.071594  ], dtype=float32)],\n",
       " [array([ 0.12034483, -0.99273217, -4.071594  ], dtype=float32),\n",
       "  array([-1.7955757], dtype=float32),\n",
       "  -3.7639735021881333,\n",
       "  array([-0.13323934, -0.99108386, -5.0854797 ], dtype=float32)],\n",
       " [array([-0.13323934, -0.99108386, -5.0854797 ], dtype=float32),\n",
       "  array([-1.2024268], dtype=float32),\n",
       "  -5.492748409526277,\n",
       "  array([-0.42058915, -0.90725124, -6.0091567 ], dtype=float32)],\n",
       " [array([-0.42058915, -0.90725124, -6.0091567 ], dtype=float32),\n",
       "  array([0.13962424], dtype=float32),\n",
       "  -7.63060351304533,\n",
       "  array([-0.6943578, -0.7196299, -6.6686516], dtype=float32)],\n",
       " [array([-0.6943578, -0.7196299, -6.6686516], dtype=float32),\n",
       "  array([1.7207713], dtype=float32),\n",
       "  -9.917809059652706,\n",
       "  array([-0.8979284, -0.4401415, -6.9502583], dtype=float32)],\n",
       " [array([-0.8979284, -0.4401415, -6.9502583], dtype=float32),\n",
       "  array([0.78344434], dtype=float32),\n",
       "  -12.044939874948849,\n",
       "  array([-0.99523956, -0.09745892, -7.1628475 ], dtype=float32)],\n",
       " [array([-0.99523956, -0.09745892, -7.1628475 ], dtype=float32),\n",
       "  array([0.1747257], dtype=float32),\n",
       "  -14.396476060489407,\n",
       "  array([-0.96564746,  0.25985572, -7.209733  ], dtype=float32)],\n",
       " [array([-0.96564746,  0.25985572, -7.209733  ], dtype=float32),\n",
       "  array([-1.2370846], dtype=float32),\n",
       "  -13.4865834693495,\n",
       "  array([-0.8121941 ,  0.58338726, -7.2004037 ], dtype=float32)],\n",
       " [array([-0.8121941 ,  0.58338726, -7.2004037 ], dtype=float32),\n",
       "  array([-0.5526469], dtype=float32),\n",
       "  -11.528735117762777,\n",
       "  array([-0.56926805,  0.82215196, -6.8457603 ], dtype=float32)],\n",
       " [array([-0.56926805,  0.82215196, -6.8457603 ], dtype=float32),\n",
       "  array([1.047027], dtype=float32),\n",
       "  -9.424307613563947,\n",
       "  array([-0.2974405 ,  0.95474035, -6.0720925 ], dtype=float32)],\n",
       " [array([-0.2974405 ,  0.95474035, -6.0720925 ], dtype=float32),\n",
       "  array([0.2650248], dtype=float32),\n",
       "  -7.194507092966951,\n",
       "  array([-0.03618862,  0.999345  , -5.3162837 ], dtype=float32)],\n",
       " [array([-0.03618862,  0.999345  , -5.3162837 ], dtype=float32),\n",
       "  array([0.46990198], dtype=float32),\n",
       "  -5.40893384020918,\n",
       "  array([ 0.18750153,  0.9822643 , -4.4962893 ], dtype=float32)],\n",
       " [array([ 0.18750153,  0.9822643 , -4.4962893 ], dtype=float32),\n",
       "  array([-0.67789793], dtype=float32),\n",
       "  -3.93253850127002,\n",
       "  array([ 0.37248173,  0.92803955, -3.861276  ], dtype=float32)],\n",
       " [array([ 0.37248173,  0.92803955, -3.861276  ], dtype=float32),\n",
       "  array([-1.466834], dtype=float32),\n",
       "  -2.90709025947568,\n",
       "  array([ 0.5234929 ,  0.85203004, -3.3852713 ], dtype=float32)],\n",
       " [array([ 0.5234929 ,  0.85203004, -3.3852713 ], dtype=float32),\n",
       "  array([-1.7465625], dtype=float32),\n",
       "  -2.189152789255996,\n",
       "  array([ 0.64525497,  0.7639673 , -3.0082333 ], dtype=float32)],\n",
       " [array([ 0.64525497,  0.7639673 , -3.0082333 ], dtype=float32),\n",
       "  array([-0.09789965], dtype=float32),\n",
       "  -1.6608810748673277,\n",
       "  array([ 0.7337698 ,  0.67939824, -2.4499426 ], dtype=float32)],\n",
       " [array([ 0.7337698 ,  0.67939824, -2.4499426 ], dtype=float32),\n",
       "  array([1.7599591], dtype=float32),\n",
       "  -1.1612420482117047,\n",
       "  array([ 0.7880741,  0.6155804, -1.6764002], dtype=float32)],\n",
       " [array([ 0.7880741,  0.6155804, -1.6764002], dtype=float32),\n",
       "  array([-1.3291149], dtype=float32),\n",
       "  -0.7225293583550904,\n",
       "  array([ 0.8295929,  0.5583687, -1.414082 ], dtype=float32)],\n",
       " [array([ 0.8295929,  0.5583687, -1.414082 ], dtype=float32),\n",
       "  array([0.32362476], dtype=float32),\n",
       "  -0.5510267638245776,\n",
       "  array([ 0.8550858 ,  0.5184865 , -0.94676185], dtype=float32)],\n",
       " [array([ 0.8550858 ,  0.5184865 , -0.94676185], dtype=float32),\n",
       "  array([-1.2828947], dtype=float32),\n",
       "  -0.3883938437691004,\n",
       "  array([ 0.8739314 ,  0.48604932, -0.75033116], dtype=float32)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-time-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
