{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from adaptive_time.utils import set_directory_in_project\n",
    "\n",
    "from importlib import reload\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive_time import plot_utils\n",
    "from adaptive_time import utils\n",
    "from adaptive_time import run_lib\n",
    "from adaptive_time import value_est\n",
    "from adaptive_time.value_est import approx_integrators\n",
    "\n",
    "approx_integrators = reload(approx_integrators)\n",
    "run_lib = reload(run_lib)\n",
    "value_est = reload(value_est)\n",
    "plot_utils = reload(plot_utils)\n",
    "utils = reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /Users/chanb/research/ualberta/adaptive_time\n",
      "['inverted_double_pendulum-v2', 'swimmer-v3', 'hopper-v3', 'ant-v3', 'cheetah-v3', 'pusher-v2']\n"
     ]
    }
   ],
   "source": [
    "set_directory_in_project()\n",
    "data_dir = \"./data\"\n",
    "env_names = [env_name for env_name in os.listdir(data_dir) if not env_name.startswith(\".DS_Store\")]\n",
    "print(env_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = \"original\"\n",
    "# samplers_tried = dict(\n",
    "#     q100=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=100),\n",
    "#     q10=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=10),\n",
    "#     q1=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=1),\n",
    "#     q0=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=0),\n",
    "#     u1=approx_integrators.UniformlySpacedIntegrator(1),\n",
    "#     u10=approx_integrators.UniformlySpacedIntegrator(50),\n",
    "#     u500=approx_integrators.UniformlySpacedIntegrator(500),\n",
    "# )\n",
    "\n",
    "# Uniform samplers only\n",
    "prefix = \"uniform\"\n",
    "spacings = (2 ** np.arange(1, 10, 0.5)).astype(int)\n",
    "samplers_tried = {\n",
    "    \"u{}\".format(spacing): approx_integrators.UniformlySpacedIntegrator(spacing) for spacing in spacings\n",
    "}\n",
    "\n",
    "# Adaptive samplers only\n",
    "# prefix = \"adaptive\"\n",
    "# tolerances = np.power(10, np.arange(-2.0, 4.0, 1.0)) * 5\n",
    "# samplers_tried = {\n",
    "#     \"q{}\".format(tolerance): approx_integrators.AdaptiveQuadratureIntegrator(tolerance=tolerance) for tolerance in tolerances\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute approximate integrals using different samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_approx_integrals(\n",
    "    reward_file: str,\n",
    "    samplers_tried: dict,\n",
    "):\n",
    "    print(reward_file)\n",
    "    reward_sequences = np.load(reward_file).T\n",
    "    idxes = np.where(reward_sequences[:, 0][:, None] - reward_sequences[:, 0][None, :] == 0)\n",
    "    \n",
    "    if len(idxes[0]) == len(idxes[1]):\n",
    "        assert np.sum(idxes[0] - idxes[1]) == 0\n",
    "    else:\n",
    "        assert 0\n",
    "\n",
    "    approx_integrals = {}\n",
    "    num_pivots = {}\n",
    "    for sampler_name, sampler in samplers_tried.items():\n",
    "        approx_integrals[sampler_name] = []\n",
    "        num_pivots[sampler_name] = []\n",
    "        for idx, reward_seq in enumerate(reward_sequences):\n",
    "            integral, all_pivots = sampler.integrate(reward_seq)\n",
    "            approx_integrals[sampler_name].append(integral)\n",
    "            num_pivots[sampler_name].append(len(all_pivots))\n",
    "        approx_integrals[sampler_name] = np.array(approx_integrals[sampler_name])\n",
    "        num_pivots[sampler_name] = np.array(num_pivots[sampler_name])\n",
    "\n",
    "    return {\n",
    "        \"reward_file\": np.array([reward_file]),\n",
    "        \"approx_integrals\": approx_integrals,\n",
    "        \"num_pivots\": num_pivots,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d6d2a4ef354c8d9796caad3999415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: inverted_double_pendulum-v2\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_9.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_8.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_6.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_7.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_5.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_4.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_0.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_1.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_3.npy\n",
      "./data/inverted_double_pendulum-v2/Rewards_50000_1000_2.npy\n",
      "env: swimmer-v3\n",
      "./data/swimmer-v3/Rewards_40000_1000_8.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_9.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_2.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_3.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_1.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_0.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_4.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_5.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_7.npy\n",
      "./data/swimmer-v3/Rewards_40000_1000_6.npy\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./{}-mujoco_val_est.pkl\".format(prefix)\n",
    "if not os.path.isfile(checkpoint):\n",
    "    for env_name in tqdm(env_names):\n",
    "        if env_name in all_results:\n",
    "            continue\n",
    "        print(\"env: {}\".format(env_name))\n",
    "\n",
    "        env_dir = os.path.join(data_dir, env_name)\n",
    "        all_results.setdefault(env_name, {})\n",
    "        run_files = [run_file for run_file in os.listdir(env_dir) if not run_file.startswith(\".DS_Store\")]\n",
    "        all_results[env_name] = Parallel(\n",
    "            n_jobs=len(run_files)\n",
    "        )(\n",
    "            delayed(compute_approx_integrals)(\n",
    "                os.path.join(env_dir, run_file),\n",
    "                samplers_tried,\n",
    "            )\n",
    "            for run_file in run_files\n",
    "        )\n",
    "    pickle.dump(all_results, open(checkpoint, \"wb\"))\n",
    "else:\n",
    "    all_results = pickle.load(open(checkpoint, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = {\n",
    "    env_name: jax.tree_util.tree_map(\n",
    "        lambda *args: np.concatenate(args),\n",
    "        *all_results[env_name]\n",
    "    ) for env_name in all_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_checkpoint = \"./{}-combined_mujoco_val_est.pkl\".format(prefix)\n",
    "pickle.dump(agg_results, open(combined_checkpoint, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample trajectories and estimate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_budget = 10_000_000\n",
    "sample_block = 1_000_000\n",
    "num_seeds = 10\n",
    "estimator_checkpoint = \"./{}-estimator_mujoco_val_est\".format(prefix)\n",
    "estimator_main_checkpoint = \"{}.pkl\".format(estimator_checkpoint)\n",
    "\n",
    "\n",
    "has_all_files = {\n",
    "    env_name: any([filename.startswith(estimator_checkpoint) and env_name in filename for filename in os.listdir(\".\")]) for env_name in agg_results\n",
    "}\n",
    "if not os.path.isfile(estimator_main_checkpoint) and not all(has_all_files.values()):\n",
    "    est_results = {}\n",
    "    for env_name in agg_results:\n",
    "        if has_all_files[env_name]:\n",
    "            continue\n",
    "        print(env_name)\n",
    "        vals_per_state = np.concatenate(\n",
    "            [np.sum(np.load(reward_file).T, axis=-1) for reward_file in agg_results[env_name][\"reward_file\"]]\n",
    "        )\n",
    "        print(vals_per_state.shape)\n",
    "\n",
    "        approx_integrals = agg_results[env_name][\"approx_integrals\"]\n",
    "        num_pivots = agg_results[env_name][\"num_pivots\"]\n",
    "\n",
    "        weights = np.ones(len(vals_per_state)) / len(vals_per_state)\n",
    "        true_value = np.sum(weights * vals_per_state)\n",
    "\n",
    "        est_results[env_name] = {\n",
    "            \"weights\": weights,\n",
    "            \"true_value\": true_value,\n",
    "            \"runs\": []\n",
    "        }\n",
    "\n",
    "        for seed in tqdm(range(num_seeds)):\n",
    "            estimated_values_by_episode = {}\n",
    "            number_of_pivots_by_episode = {}\n",
    "            all_values_by_episode = {}\n",
    "\n",
    "            rng = np.random.RandomState(seed)\n",
    "            start_states = rng.choice(len(vals_per_state), p=weights, size=(update_budget))\n",
    "\n",
    "            for sampler_name, sampler in samplers_tried.items():\n",
    "                # print(\"sampler_name:\", sampler_name)\n",
    "                # Update the value estimate with new samples until we run out of budget.\n",
    "                used_updates = 0\n",
    "                value_estimate = 0\n",
    "                num_episodes = 0\n",
    "                all_values_by_episode[sampler_name] = []\n",
    "\n",
    "                estimated_values_by_episode[sampler_name] = []\n",
    "                number_of_pivots_by_episode[sampler_name] = []\n",
    "\n",
    "                # pbar = tqdm(total = update_budget)\n",
    "                while used_updates < update_budget:\n",
    "                    num_episodes += 1\n",
    "                    if num_episodes % sample_block == 0:\n",
    "                        start_states = rng.choice(len(vals_per_state), p=weights, size=(update_budget))\n",
    "                    start_state = start_states[(num_episodes - 1) % sample_block]\n",
    "                    val_sample = approx_integrals[sampler_name][start_state]\n",
    "                    all_values_by_episode[sampler_name].append(val_sample)\n",
    "                    \n",
    "                    value_estimate += (1.0/num_episodes) * (val_sample - value_estimate)\n",
    "                    used_updates += num_pivots[sampler_name][start_state]\n",
    "\n",
    "                    estimated_values_by_episode[sampler_name].append(value_estimate)\n",
    "                    number_of_pivots_by_episode[sampler_name].append(used_updates)\n",
    "                #     pbar.update(num_pivots[sampler_name][start_state])\n",
    "                # pbar.close()\n",
    "            est_results[env_name][\"runs\"].append({\n",
    "                \"estimated_values_by_episode\": estimated_values_by_episode,\n",
    "                \"number_of_pivots_by_episode\": number_of_pivots_by_episode,\n",
    "                \"all_values_by_episode\": all_values_by_episode,\n",
    "            })\n",
    "        # pickle.dump(est_results[env_name], open(\"{}-{}.pkl\".format(estimator_checkpoint, env_name), \"wb\"))\n",
    "else:\n",
    "    if os.path.isfile(estimator_main_checkpoint):\n",
    "        est_results = pickle.load(open(estimator_main_checkpoint, \"rb\"))\n",
    "    else:\n",
    "        est_results = {env_name: pickle.load(open(\"{}-{}.pkl\".format(estimator_checkpoint, env_name), \"rb\")) for env_name in agg_results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile plot statistics using interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_across_runs(\n",
    "    runs,\n",
    "    samplers_tried,\n",
    "    true_value,\n",
    "):\n",
    "    results = {}\n",
    "    for sampler in samplers_tried:\n",
    "        interpolated_results = np.zeros((len(runs), update_budget + 1))\n",
    "        for run_i, run in enumerate(runs):\n",
    "            number_of_pivots_by_episode = run[\"number_of_pivots_by_episode\"][sampler]\n",
    "            estimated_values_by_episode = run[\"estimated_values_by_episode\"][sampler]\n",
    "\n",
    "            interpolated_results[run_i] = np.interp(\n",
    "                np.arange(update_budget + 1),\n",
    "                number_of_pivots_by_episode,\n",
    "                np.abs(estimated_values_by_episode - true_value)[:number_of_pivots_by_episode[-1]]\n",
    "            )\n",
    "        results[sampler] = interpolated_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats_checkpoint = \"./{}-plot_stats_mujoco_val_est.pkl\".format(prefix)\n",
    "\n",
    "if not os.path.isfile(plot_stats_checkpoint):\n",
    "    plot_stats = {}\n",
    "    for env_i, env_name in enumerate(tqdm(est_results)):\n",
    "        true_value = est_results[env_name][\"true_value\"]\n",
    "        env_result = process_across_runs(\n",
    "            est_results[env_name][\"runs\"],\n",
    "            samplers_tried,\n",
    "            true_value\n",
    "        )\n",
    "        plot_stats[env_name] = env_result\n",
    "    # pickle.dump(plot_stats, open(plot_stats_checkpoint, \"wb\"))\n",
    "else:\n",
    "    plot_stats = pickle.load(open(plot_stats_checkpoint, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot error vs number of updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot per seed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in est_results:\n",
    "    true_value = est_results[env_name][\"true_value\"]\n",
    "\n",
    "    ncols = 5\n",
    "    fig, axes = plt.subplots(nrows=num_seeds // ncols, ncols=ncols, figsize=(20, 8), layout=\"constrained\")\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        number_of_pivots_by_episode = est_results[env_name][\"runs\"][seed][\"number_of_pivots_by_episode\"]\n",
    "        estimated_values_by_episode = est_results[env_name][\"runs\"][seed][\"estimated_values_by_episode\"]\n",
    "        \n",
    "        ax = axes[seed // ncols, seed % ncols]\n",
    "        max_diff = 0\n",
    "        for s in samplers_tried.keys():\n",
    "            ax.plot(\n",
    "                number_of_pivots_by_episode[s],\n",
    "                np.abs(estimated_values_by_episode[s]-true_value),\n",
    "                label=s if seed == 0 else \"\")\n",
    "            curr_max_diff = np.max(np.abs(estimated_values_by_episode[s]-true_value))\n",
    "            if curr_max_diff > max_diff:\n",
    "                max_diff = curr_max_diff\n",
    "\n",
    "        ax.set_title(\"Seed: {}\".format(seed))\n",
    "        order = np.log10(max_diff)\n",
    "        ax.set_ylim(-2 * order, max_diff + 2 * order)\n",
    "    fig.supylabel(\"Error in value estimate\")\n",
    "    fig.supxlabel(\"Number of Samples\")\n",
    "    fig.legend()\n",
    "    fig.suptitle(\"Env: {}\".format(env_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot aggregated result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_to_plot = [\"q1\", \"q10\", \"q100\", \"u1\", \"u10\", \"u500\"]\n",
    "# s_to_plot = [\"q0\", \"u1\"]\n",
    "s_to_plot = samplers_tried.keys()\n",
    "\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows=len(est_results) // ncols, ncols=ncols, figsize=(15, 8), layout=\"constrained\")\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = -1\n",
    "for env_i, env_name in enumerate(est_results):\n",
    "    env_result = plot_stats[env_name]\n",
    "    ax = axes[env_i // ncols, env_i % ncols]\n",
    "    max_val = 0\n",
    "    for s in tqdm(s_to_plot):\n",
    "        if s == \"u1024\":\n",
    "            continue\n",
    "        y_means = np.nanmean(env_result[s], axis=0)\n",
    "        y_stderrs = np.nanstd(env_result[s], axis=0) / np.sqrt(num_seeds)\n",
    "\n",
    "        y_means = np.log10(y_means, out=np.zeros_like(y_means), where=(y_means != 0))\n",
    "        y_stderrs = np.log10(y_stderrs, out=np.zeros_like(y_stderrs), where=(y_stderrs != 0))\n",
    "\n",
    "        ax.plot(\n",
    "            np.arange(update_budget + 1)[start_idx:end_idx],\n",
    "            y_means[start_idx:end_idx],\n",
    "            label=s if env_i == 0 else \"\"\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            np.arange(update_budget + 1)[start_idx:end_idx],\n",
    "            (y_means - y_stderrs)[start_idx:end_idx],\n",
    "            (y_means + y_stderrs)[start_idx:end_idx],\n",
    "            alpha=0.1\n",
    "        )\n",
    "        curr_max_val = np.max(y_means)\n",
    "        if curr_max_val > max_val:\n",
    "            max_val = curr_max_val\n",
    "    ax.set_title(\"Env: {}\".format(env_name))\n",
    "    order = 1e-5\n",
    "    if max_val > 0:\n",
    "        order = np.log10(max_val)\n",
    "    ax.set_ylim(-order, max_val + 2 * order)\n",
    "\n",
    "fig.supylabel(\"Error in value estimate\")\n",
    "fig.supxlabel(\"Number of Samples\")\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot error vs hyperparameter (i.e. tolerance/discretization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive_time",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
