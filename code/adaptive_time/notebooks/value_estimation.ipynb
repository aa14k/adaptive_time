{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Estimation -- quadrature vs uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szepi1991/Code/adaptive_time/.venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CartPole-OURS-v2 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from importlib import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from adaptive_time import plot_utils\n",
    "from adaptive_time import utils\n",
    "from adaptive_time import run_lib\n",
    "from adaptive_time.environments import cartpole2\n",
    "from adaptive_time import value_est\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "cartpole2 = reload(cartpole2)\n",
    "value_est = reload(value_est)\n",
    "plot_utils = reload(plot_utils)\n",
    "utils = reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /Users/szepi1991/Code/adaptive_time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/szepi1991/Code/adaptive_time'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.set_directory_in_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a goodish policy, generate data to process\n",
    "\n",
    "Recall, we want:\n",
    "\n",
    "1. The policy to stay up for ~10k steps, while we interact for 20k steps.\n",
    "2. To generate trajectories from 100 different initial states.\n",
    "\n",
    "We just need to store the fine grained rewards for each of these trajectories, all processing will happen on this after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_NEW_DATA = True\n",
    "SAVE_TRAJECTORIES = True\n",
    "load_data_from = \"many_good_trajs.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (100, 4)\n",
      "max [0.01849613 0.03401106 0.00259208 0.00238675]\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Did 20_000 steps! 20000\n",
      "Saved data to many_good_trajs.pkl\n"
     ]
    }
   ],
   "source": [
    "if GENERATE_NEW_DATA:\n",
    "    seed = 13\n",
    "    STEPS_MAX = 20_000\n",
    "    STEPS_BREAK = 9_000\n",
    "    NUM_TRAJS = 100\n",
    "\n",
    "    from adaptive_time import mc2\n",
    "    import adaptive_time.utils\n",
    "    import gymnasium as gym\n",
    "    import random\n",
    "\n",
    "    env = gym.make('CartPole-OURS-v2', discrete_reward=True)\n",
    "    _NUM_ACTIONS = 2\n",
    "\n",
    "    phi = run_lib.make_features()\n",
    "\n",
    "    weights_good_policy = np.load(\"cartpole_weights_20240227-102913_ret92516.44719752521.npy\")\n",
    "\n",
    "    # implement epsilon-greedy action sampling. \n",
    "    def policy(state, num_step, weights, epsilon):\n",
    "        \"\"\"Returns the action to take, and maybe the prob of all actions\"\"\"\n",
    "        if num_step >= STEPS_BREAK:\n",
    "            # If we are the the failing case, make this much more likely.\n",
    "            epsilon = 0.06\n",
    "        if random.random() < epsilon:\n",
    "        # if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(_NUM_ACTIONS)\n",
    "        for action in range(_NUM_ACTIONS):\n",
    "            x_sa = mc2.phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        \n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    run_lib.reset_randomness(seed, env)\n",
    "\n",
    "    def _random_start_state(num):\n",
    "        rand = np.random.standard_normal((num, 4))\n",
    "        rand *= np.array([[0.01, 0.01, 0.001, 0.001]])\n",
    "        return rand\n",
    "\n",
    "    start_states = _random_start_state(NUM_TRAJS)\n",
    "    print(\"shape\", start_states.shape)\n",
    "    print(\"max\", np.max(start_states, axis=0))\n",
    "\n",
    "    total_rewards = []\n",
    "    reward_sequences = []\n",
    "    traj_lengths = []\n",
    "    for idx in range(NUM_TRAJS):\n",
    "        start_state = tuple(start_states[idx])\n",
    "        # Tuple[float, float, float, float]\n",
    "        trajectory, early_term = value_est.generate_trajectory(\n",
    "                env, start_state=start_state,\n",
    "                policy=lambda st, sn: policy(st, sn, weights_good_policy, 0.0),\n",
    "                termination_prob=0.0, max_steps=STEPS_MAX)\n",
    "\n",
    "        traj_lengths.append(len(trajectory))\n",
    "        rewards = [r for _, _, r, _ in trajectory]\n",
    "        reward_sequences.append(rewards)\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "    total_rewards = np.array(total_rewards)\n",
    "    reward_sequences = np.array(reward_sequences)\n",
    "    traj_lengths = np.array(traj_lengths)\n",
    "\n",
    "    if SAVE_TRAJECTORIES:\n",
    "        with open(load_data_from, \"wb\") as f:\n",
    "            pickle.dump((total_rewards, reward_sequences, traj_lengths), f)\n",
    "        print(\"Saved data to\", load_data_from)\n",
    "\n",
    "else:\n",
    "    np.load\n",
    "    with open(load_data_from, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    total_rewards, reward_sequences, traj_lengths = data\n",
    "\n",
    "    print(\"Loaded data from\", load_data_from)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9564., 11828.,  9723., 12029.,  9441.,  9283.,  9515., 11781.,\n",
       "       11517.,  9112.,  9193., 10553.,  9231., 13828.,  9831., 11665.,\n",
       "        9726.,  9243.,  9342.,  9321.,  9844., 10532.,  9465., 10929.,\n",
       "       11433., 10190., 10488.,  9660.,  9078., 11085., 10102.,  9302.,\n",
       "        9712., 10843.,  9751., 10436., 10490., 10579.,  9970., 10050.,\n",
       "       10328.,  9921.,  9765., 10185.,  9596.,  9825.,  9045., 11476.,\n",
       "        9417., 10730.,  9381., 11138.,  9417.,  9532.,  9187., 11204.,\n",
       "        9341.,  9449., 10203., 10600., 11353., 10876., 13146., 10028.,\n",
       "       11178.,  9332., 10300.,  9229.,  9746.,  9073., 10724.,  9575.,\n",
       "       10833.,  9437.,  9299., 12054.,  9100.,  9292., 10665.,  9482.,\n",
       "       10504., 10365., 11089.,  9422.,  9607.,  9611., 12038.,  9754.,\n",
       "        9168.,  9108.,  9368., 10059.,  9300., 11889., 10222., 10455.,\n",
       "        9102.,  9229.,  9114.,  9912.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup 2: the weights across initial states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "run_lib.reset_randomness(seed, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
