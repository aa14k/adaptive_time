{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Estimation -- quadrature vs uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szepi1991/Code/adaptive_time/.venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CartPole-OURS-v2 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from importlib import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from adaptive_time import plot_utils\n",
    "from adaptive_time import utils\n",
    "from adaptive_time import run_lib\n",
    "from adaptive_time.environments import cartpole2\n",
    "from adaptive_time import value_est\n",
    "from adaptive_time.value_est import approx_integrators\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "approx_integrators = reload(approx_integrators)\n",
    "run_lib = reload(run_lib)\n",
    "cartpole2 = reload(cartpole2)\n",
    "value_est = reload(value_est)\n",
    "plot_utils = reload(plot_utils)\n",
    "utils = reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to /Users/szepi1991/Code/adaptive_time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/szepi1991/Code/adaptive_time'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.set_directory_in_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a goodish policy, generate data to process\n",
    "\n",
    "Recall, we want:\n",
    "\n",
    "1. The policy to stay up for ~10k steps, while we interact for 20k steps.\n",
    "2. To generate trajectories from 100 different initial states.\n",
    "\n",
    "We just need to store the fine grained rewards for each of these trajectories, all processing will happen on this after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_NEW_DATA = False\n",
    "SAVE_TRAJECTORIES = False\n",
    "load_data_from = \"many_good_trajs.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from many_good_trajs.pkl\n",
      "total_rewards for each traj: [ 9564. 11828.  9723. 12029.  9441.  9283.  9515. 11781. 11517.  9112.\n",
      "  9193. 10553.  9231. 13828.  9831. 11665.  9726.  9243.  9342.  9321.\n",
      "  9844. 10532.  9465. 10929. 11433. 10190. 10488.  9660.  9078. 11085.\n",
      " 10102.  9302.  9712. 10843.  9751. 10436. 10490. 10579.  9970. 10050.\n",
      " 10328.  9921.  9765. 10185.  9596.  9825.  9045. 11476.  9417. 10730.\n",
      "  9381. 11138.  9417.  9532.  9187. 11204.  9341.  9449. 10203. 10600.\n",
      " 11353. 10876. 13146. 10028. 11178.  9332. 10300.  9229.  9746.  9073.\n",
      " 10724.  9575. 10833.  9437.  9299. 12054.  9100.  9292. 10665.  9482.\n",
      " 10504. 10365. 11089.  9422.  9607.  9611. 12038.  9754.  9168.  9108.\n",
      "  9368. 10059.  9300. 11889. 10222. 10455.  9102.  9229.  9114.  9912.]\n",
      "all rewards shape: (100, 20001)\n"
     ]
    }
   ],
   "source": [
    "if GENERATE_NEW_DATA:\n",
    "    seed = 13\n",
    "    STEPS_MAX = 20_000\n",
    "    STEPS_BREAK = 9_000\n",
    "    NUM_TRAJS = 100\n",
    "\n",
    "    from adaptive_time import mc2\n",
    "    import adaptive_time.utils\n",
    "    import gymnasium as gym\n",
    "    import random\n",
    "\n",
    "    env = gym.make('CartPole-OURS-v2', discrete_reward=True)\n",
    "    _NUM_ACTIONS = 2\n",
    "\n",
    "    phi = run_lib.make_features()\n",
    "\n",
    "    weights_good_policy = np.load(\"cartpole_weights_20240227-102913_ret92516.44719752521.npy\")\n",
    "\n",
    "    # implement epsilon-greedy action sampling. \n",
    "    def policy(state, num_step, weights, epsilon):\n",
    "        \"\"\"Returns the action to take, and maybe the prob of all actions\"\"\"\n",
    "        if num_step >= STEPS_BREAK:\n",
    "            # If we are the the failing case, make this much more likely.\n",
    "            epsilon = 0.06\n",
    "        if random.random() < epsilon:\n",
    "        # if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(_NUM_ACTIONS)\n",
    "        for action in range(_NUM_ACTIONS):\n",
    "            x_sa = mc2.phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        \n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    run_lib.reset_randomness(seed, env)\n",
    "\n",
    "    def _random_start_state(num):\n",
    "        rand = np.random.standard_normal((num, 4))\n",
    "        rand *= np.array([[0.01, 0.01, 0.001, 0.001]])\n",
    "        return rand\n",
    "\n",
    "    start_states = _random_start_state(NUM_TRAJS)\n",
    "    print(\"shape\", start_states.shape)\n",
    "    print(\"max\", np.max(start_states, axis=0))\n",
    "\n",
    "    total_rewards = []\n",
    "    reward_sequences = []\n",
    "    traj_lengths = []\n",
    "    for idx in range(NUM_TRAJS):\n",
    "        start_state = tuple(start_states[idx])\n",
    "        # Tuple[float, float, float, float]\n",
    "        trajectory, early_term = value_est.generate_trajectory(\n",
    "                env, start_state=start_state,\n",
    "                policy=lambda st, sn: policy(st, sn, weights_good_policy, 0.0),\n",
    "                termination_prob=0.0, max_steps=STEPS_MAX)\n",
    "\n",
    "        traj_lengths.append(len(trajectory))\n",
    "        rewards = [r for _, _, r, _ in trajectory]\n",
    "        reward_sequences.append(rewards)\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "    total_rewards = np.array(total_rewards)\n",
    "    reward_sequences = np.array(reward_sequences)\n",
    "    traj_lengths = np.array(traj_lengths)\n",
    "\n",
    "    if SAVE_TRAJECTORIES:\n",
    "        with open(load_data_from, \"wb\") as f:\n",
    "            pickle.dump((total_rewards, reward_sequences, traj_lengths), f)\n",
    "        print(\"Saved data to\", load_data_from)\n",
    "\n",
    "else:\n",
    "    with open(load_data_from, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    total_rewards, reward_sequences, traj_lengths = data\n",
    "\n",
    "    print(\"Loaded data from\", load_data_from)\n",
    "\n",
    "\n",
    "print(\"total_rewards for each traj:\", total_rewards)\n",
    "print(\"all rewards shape:\", reward_sequences.shape)\n",
    "num_trajs = len(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup 2: the weights across initial states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [2.22313387e-03 1.17686459e-02 2.57743764e-03 4.64877622e-03\n",
      " 6.32112168e-03 1.66006245e-02 1.28704819e-02 1.04565693e-02\n",
      " 5.60617607e-04 1.41778005e-02 7.63139430e-03 1.54974997e-02\n",
      " 4.91603453e-03 1.09915524e-03 1.67457582e-02 4.27081742e-03\n",
      " 7.82538048e-03 6.10775211e-03 1.48130970e-03 1.62931506e-02\n",
      " 1.64035677e-02 1.87710668e-02 7.44641866e-03 1.84430305e-02\n",
      " 8.61313497e-03 1.29407081e-02 1.59410148e-03 1.73341250e-02\n",
      " 5.75815194e-03 5.06836672e-03 9.91153555e-05 1.04959929e-02\n",
      " 9.18967777e-03 1.22962862e-02 1.89012814e-02 1.75575730e-02\n",
      " 1.75863696e-02 1.01492162e-02 2.00989893e-03 3.49570970e-03\n",
      " 1.84150535e-02 7.95993291e-03 1.67142332e-02 1.29880988e-02\n",
      " 1.21496699e-02 5.32446532e-03 1.73273368e-02 3.99764415e-03\n",
      " 7.81409222e-03 1.91982355e-02 1.42160354e-02 8.59966823e-03\n",
      " 1.08333752e-02 7.94645562e-03 1.40471750e-02 7.71345410e-03\n",
      " 1.29488334e-02 1.36168184e-02 1.17781783e-02 1.04347784e-02\n",
      " 3.98200225e-03 3.84828380e-03 1.53756089e-02 5.60993493e-03\n",
      " 1.26747962e-02 5.78932434e-03 2.79167343e-03 7.80542464e-03\n",
      " 5.99516138e-03 4.70304126e-03 1.13635995e-02 4.74062796e-03\n",
      " 1.44487457e-02 1.39149836e-02 1.34341298e-02 1.98524020e-03\n",
      " 1.82334653e-02 9.72574510e-03 1.73838810e-02 3.83704602e-03\n",
      " 1.14862031e-02 1.86540403e-02 1.92972775e-02 4.66996578e-04\n",
      " 9.29995023e-03 5.63099468e-03 1.23123626e-03 1.10065279e-02\n",
      " 9.82214155e-05 1.18113688e-02 1.68140212e-02 1.70733279e-02\n",
      " 1.84397719e-02 1.42959673e-02 3.56910329e-03 8.39904167e-03\n",
      " 1.71177315e-02 4.92811404e-03 8.56587875e-03 1.19207219e-02]\n",
      "sum: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "seed = 43\n",
    "run_lib.reset_randomness(seed, env=None)\n",
    "\n",
    "weights = np.random.random((num_trajs,))\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "print(\"weights:\", weights)\n",
    "print(\"sum:\", np.sum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true value: 10076.089908727237\n"
     ]
    }
   ],
   "source": [
    "true_value = total_rewards @ weights\n",
    "print(\"true value:\", true_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn from samples; with diff samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers_tried = dict(\n",
    "    q100=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=100),\n",
    "    q10=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=10),\n",
    "    q1=approx_integrators.AdaptiveQuadratureIntegrator(tolerance=1),\n",
    "    u1=approx_integrators.UniformlySpacedIntegrator(1),\n",
    "    u10=approx_integrators.UniformlySpacedIntegrator(10),\n",
    "    u100=approx_integrators.UniformlySpacedIntegrator(100),\n",
    "    u1000=approx_integrators.UniformlySpacedIntegrator(1000),\n",
    "    u10000=approx_integrators.UniformlySpacedIntegrator(10000),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since everything other than the start state is deterministic, we can just calculate the approximate integrals for each trajectory with each integrator and store these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2776f481ace044e0a476c61c0dfbba63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_name: q100\n",
      "sampler_name: q10\n",
      "sampler_name: q1\n",
      "sampler_name: u1\n",
      "sampler_name: u10\n",
      "sampler_name: u100\n",
      "sampler_name: u1000\n",
      "sampler_name: u10000\n"
     ]
    }
   ],
   "source": [
    "approx_integrals = {}\n",
    "num_pivots = {}\n",
    "\n",
    "for sampler_name, sampler in tqdm(samplers_tried.items()):\n",
    "    print(\"sampler_name:\", sampler_name)\n",
    "    approx_integrals[sampler_name] = []\n",
    "    num_pivots[sampler_name] = []\n",
    "    for idx, reward_seq in enumerate(reward_sequences[:1]):\n",
    "        integral, all_pivots = sampler.integrate(reward_seq)\n",
    "        approx_integrals[sampler_name].append(integral)\n",
    "        num_pivots[sampler_name].append(len(all_pivots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q100': [9559.0],\n",
       " 'q10': [9559.0],\n",
       " 'q1': [9559.0],\n",
       " 'u1': [9564.0],\n",
       " 'u10': [1911.0],\n",
       " 'u100': [191.0],\n",
       " 'u1000': [19.0],\n",
       " 'u10000': [1.0]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q100': [53],\n",
       " 'q10': [53],\n",
       " 'q1': [53],\n",
       " 'u1': [20001],\n",
       " 'u10': [4001],\n",
       " 'u100': [401],\n",
       " 'u1000': [41],\n",
       " 'u10000': [5]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q100': 9838.0,\n",
       " 'q10': 9838.0,\n",
       " 'q1': 9838.0,\n",
       " 'u1': 9912.0,\n",
       " 'u10': 1977.0,\n",
       " 'u100': 197.0,\n",
       " 'u1000': 19.0,\n",
       " 'u10000': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q100': 48,\n",
       " 'q10': 48,\n",
       " 'q1': 48,\n",
       " 'u1': 20001,\n",
       " 'u10': 4001,\n",
       " 'u100': 401,\n",
       " 'u1000': 41,\n",
       " 'u10000': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pivots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just keep sampling from the initial states and observe empirical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
