{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the ground truth for policy evaluation\n",
    "\n",
    "We want a good and a bad policy, and we'll evaluate the mixture of these policies.\n",
    "\n",
    "At the start of the episode we pick one of the policies with prob `p` (and\n",
    "the other with prob `1-p`), and stick to it for episode. Repeat and rinse.\n",
    "\n",
    "We will evaluate different value estimation methods on this mixed policy. We will\n",
    "see how well they approximate the true value (in the start state) of this mixed policy.\n",
    "But for this, we need to know the true value of the mixed policy; thankfully this is\n",
    "easy to calculate in our setting.\n",
    "\n",
    "Each of these policies is deterministic, and the environment is deterministic. Therefore\n",
    "we can easily find, by running the policies once, $V^{\\pi_g}$, the value of the good policy in the start state, and $V^{\\pi_b}$, the value of the bad policy in the start state.\n",
    "\n",
    "The value of the mixed policy will be $p ^{\\pi_g} + (1-p) ^{\\pi_b}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "import adaptive_time.utils\n",
    "from adaptive_time import run_lib\n",
    "from adaptive_time import environments\n",
    "from adaptive_time import mc2\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running good policy\n",
      "running bad policy\n",
      "[13811.823415783701] [26.83463996869155]\n"
     ]
    }
   ],
   "source": [
    "seed = 13\n",
    "\n",
    "run_lib.register_gym_envs()\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "env.stepTime(0.02)\n",
    "\n",
    "run_lib.reset_randomness(seed, env)\n",
    "phi = run_lib.make_features()\n",
    "\n",
    "weights_good_policy = np.load(\n",
    "    '/Users/alexayoub/adaptive_time/code/adaptive_time/notebooks/cartpole_weights_20240227-102913_ret92516.44719752521.npy')\n",
    "weights_bad_policy = np.random.uniform(size = len(weights_good_policy))\n",
    "\n",
    "\n",
    "pi_good = []\n",
    "pi_bad = []\n",
    "\n",
    "def policy(state, weights, pi_for_storing):\n",
    "    if random.random() < 0.02:\n",
    "    # if random.random() < 0.01:\n",
    "        a = env.action_space.sample()\n",
    "        pi_for_storing.append(a)\n",
    "        return a\n",
    "    # Otherwise calculate the best action.\n",
    "    x = phi.get_fourier_feature(state)\n",
    "    qs = np.zeros(2)\n",
    "    for action in [0, 1]:\n",
    "        x_sa = mc2.phi_sa(x, action)\n",
    "        qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "    # adaptive_time.utils.softmax(qs, 1)\n",
    "    a = adaptive_time.utils.argmax(qs)\n",
    "    pi_for_storing.append(a)\n",
    "    return a\n",
    "\n",
    "policy_good = lambda s: policy(state=s, weights=weights_good_policy, pi_for_storing=pi_good)\n",
    "policy_bad = lambda s: policy(state=s, weights=weights_bad_policy, pi_for_storing=pi_bad)\n",
    "\n",
    "print('running good policy')\n",
    "run_lib.reset_randomness(seed, env)\n",
    "traj_good, early_term = environments.generate_trajectory(env, seed, policy_good)\n",
    "\n",
    "print('running bad policy')\n",
    "run_lib.reset_randomness(seed, env)\n",
    "traj_bad, early_term = environments.generate_trajectory(env, seed, policy_bad)\n",
    "\n",
    "np.save('policy_to_eval_good.npy', pi_good)\n",
    "np.save('policy_to_eval_bad.npy', pi_bad)\n",
    "\n",
    "def get_returns(trajectory, x0, gamma = 0.99999):\n",
    "    returns = []\n",
    "    G = 0.0\n",
    "    for t in range(len(trajectory)-1,-1,-1):\n",
    "        traj = trajectory[t]\n",
    "        s, _, r, _ = traj\n",
    "        x = phi.get_fourier_feature(s)\n",
    "        G = gamma * G + r\n",
    "        if np.linalg.norm(x - x0) <= 0.0001:\n",
    "            returns.append(G)\n",
    "    return returns\n",
    "\n",
    "x0 = phi.get_fourier_feature([0,0,0,0])\n",
    "returns_good = get_returns(traj_good, x0)\n",
    "returns_bad = get_returns(traj_bad, x0)\n",
    "\n",
    "print(returns_good, returns_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory_lengths: 14874 37\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
