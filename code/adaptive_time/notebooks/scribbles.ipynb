{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c28d52f",
   "metadata": {},
   "source": [
    "# The purpose of this notebook is for experimenting with code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39e63e",
   "metadata": {},
   "source": [
    "Monte Carlo in MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6738b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive_time.environment import MountainCar\n",
    "from adaptive_time.features import MountainCarTileCoder\n",
    "from adaptive_time.utils import argmax\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3070f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your results:\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[ 8  9 10 11 12 13 14 15]\n",
      "[16 17 18 19 20 21 22 23]\n",
      "[ 0 24  2  3  4  5  6  7]\n",
      "[16 17 18 19 20 21 22 23]\n",
      "\n",
      "Expected results:\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[ 8  9 10 11 12 13 14 15]\n",
      "[16 17 18 19 20 21 22 23]\n",
      "[ 0 24  2  3  4  5  6  7]\n",
      "[16 17 18 19 20 21 22 23]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [DO NOT CHANGE]\n",
    "tests = [[-1.0, 0.01], [0.1, -0.01], [0.2, -0.05], [-1.0, 0.011], [0.2, -0.05]]\n",
    "\n",
    "mctc = MountainCarTileCoder(iht_size=1024, num_tilings=8, num_tiles=8)\n",
    "\n",
    "t = []\n",
    "for test in tests:\n",
    "    position, velocity = test\n",
    "    tiles = mctc.get_tiles(position=position, velocity=velocity)\n",
    "    t.append(tiles)\n",
    "\n",
    "print(\"Your results:\")\n",
    "for tiles in t:\n",
    "    print(tiles)\n",
    "\n",
    "print()\n",
    "print(\"Expected results:\")\n",
    "expected = \"\"\"[0 1 2 3 4 5 6 7]\n",
    "[ 8  9 10 11 12 13 14 15]\n",
    "[16 17 18 19 20 21 22 23]\n",
    "[ 0 24  2  3  4  5  6  7]\n",
    "[16 17 18 19 20 21 22 23]\n",
    "\"\"\"\n",
    "print(expected)\n",
    "\n",
    "np.random.seed(1)\n",
    "mctc_test = MountainCarTileCoder(iht_size=1024, num_tilings=8, num_tiles=8)\n",
    "test = [mctc_test.get_tiles(np.random.uniform(-1.2, 0.5), np.random.uniform(-0.07, 0.07)) for _ in range(10)]\n",
    "np.save(\"tiles_test\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d834482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        self.epsilon = None\n",
    "        self.gamma = None\n",
    "        self.iht_size = None\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.num_tilings = None\n",
    "        self.num_tiles = None\n",
    "        self.mctc = None\n",
    "        self.initial_weights = None\n",
    "        self.num_actions = None\n",
    "        self.previous_tiles = None\n",
    "\n",
    "\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.num_tilings =  8\n",
    "        self.num_tiles =  8\n",
    "        self.iht_size =  4096\n",
    "        self.epsilon =  0.0\n",
    "        self.gamma = 1.0\n",
    "        self.alpha = 0.5 / self.num_tilings\n",
    "        self.initial_weights =  0.0\n",
    "        self.num_actions =  3\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We initialize self.w to three times the iht_size. Recall this is because\n",
    "        # we need to have one set of weights for each action.\n",
    "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
    "        \n",
    "        \n",
    "        # We initialize self.mctc to the mountaincar verions of the \n",
    "        # tile coder that we created\n",
    "        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n",
    "                                         num_tilings=self.num_tilings, \n",
    "                                         num_tiles=self.num_tiles)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def select_action(self, tiles):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon greedy\n",
    "        Args:\n",
    "        tiles - np.array, an array of active tiles\n",
    "        Returns:\n",
    "        (chosen_action, action_value) - (int, float), tuple of the chosen action\n",
    "                                        and it's value\n",
    "        \"\"\"\n",
    "        action_values = []\n",
    "        chosen_action = None\n",
    "        \n",
    "        # First loop through the weights of each action and populate action_values\n",
    "        # with the action value for each action and tiles instance\n",
    "        \n",
    "        # Use np.random.random to decide if an exploritory action should be taken\n",
    "        # and set chosen_action to a random action if it is\n",
    "        # Otherwise choose the greedy action using the given argmax \n",
    "        # function and the action values (don't use numpy's armax)\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        action_values = np.zeros(self.num_actions)\n",
    "        for action in range(self.num_actions):\n",
    "            action_values[action] = np.sum(self.w[action][tiles])\n",
    "        chosen_action = argmax(action_values)\n",
    "        ### END CODE HERE ###\n",
    "        return chosen_action, action_values[chosen_action]\n",
    "    \n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Use self.tc to set active_tiles using position and velocity\n",
    "        # set current_action to the epsilon greedy chosen action using\n",
    "        # the select_action function above with the active tiles\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        current_action, _ = self.select_action(active_tiles)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.last_action = current_action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "    \n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        # choose the action here\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Use self.tc to set active_tiles using position and velocity\n",
    "        # set current_action and action_value to the epsilon greedy chosen action using\n",
    "        # the select_action function above with the active tiles\n",
    "        \n",
    "        # Update self.w at self.previous_tiles and self.previous action\n",
    "        # using the reward, action_value, self.gamma, self.w,\n",
    "        # self.alpha, and the Sarsa update from the textbook\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        current_action, action_value = self.select_action(active_tiles)\n",
    "        last_action_value = np.sum(self.w[self.last_action][self.previous_tiles])\n",
    "        self.w[self.last_action][self.previous_tiles] += self.alpha * (reward + self.gamma * action_value - last_action_value) * 1\n",
    "        \n",
    "        \n",
    "        self.last_action = current_action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "    \n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates. Same as above except action_value = 0.0\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Update self.w at self.previous_tiles and self.previous action\n",
    "        # using the reward, self.gamma, self.w,\n",
    "        # self.alpha, and the Sarsa update from the textbook\n",
    "        # Hint - there is no action_value used here because this is the end\n",
    "        # of the episode.\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        last_action_value = np.sum(self.w[self.last_action][self.previous_tiles])\n",
    "        \n",
    "        \n",
    "        self.w[self.last_action][self.previous_tiles] += self.alpha * (reward - last_action_value) * 1\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d94ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -200.0\n",
      "1 -200.0\n",
      "2 -200.0\n",
      "3 -200.0\n",
      "4 -200.0\n",
      "5 -200.0\n",
      "6 -200.0\n",
      "7 -200.0\n",
      "8 -200.0\n",
      "9 -200.0\n",
      "10 -200.0\n",
      "11 -200.0\n",
      "12 -200.0\n",
      "13 -200.0\n",
      "14 -200.0\n",
      "15 -200.0\n",
      "16 -200.0\n",
      "17 -200.0\n",
      "18 -200.0\n",
      "19 -200.0\n",
      "20 -200.0\n",
      "21 -200.0\n",
      "22 -185.0\n",
      "23 -200.0\n",
      "24 -200.0\n",
      "25 -175.0\n",
      "26 -200.0\n",
      "27 -200.0\n",
      "28 -200.0\n",
      "29 -200.0\n",
      "30 -200.0\n",
      "31 -200.0\n",
      "32 -169.0\n",
      "33 -200.0\n",
      "34 -200.0\n",
      "35 -200.0\n",
      "36 -173.0\n",
      "37 -200.0\n",
      "38 -200.0\n",
      "39 -200.0\n",
      "40 -200.0\n",
      "41 -195.0\n",
      "42 -191.0\n",
      "43 -173.0\n",
      "44 -189.0\n",
      "45 -200.0\n",
      "46 -184.0\n",
      "47 -179.0\n",
      "48 -177.0\n",
      "49 -174.0\n",
      "50 -176.0\n",
      "51 -168.0\n",
      "52 -166.0\n",
      "53 -200.0\n",
      "54 -200.0\n",
      "55 -200.0\n",
      "56 -200.0\n",
      "57 -175.0\n",
      "58 -200.0\n",
      "59 -200.0\n",
      "60 -200.0\n",
      "61 -199.0\n",
      "62 -198.0\n",
      "63 -175.0\n",
      "64 -172.0\n",
      "65 -161.0\n",
      "66 -159.0\n",
      "67 -159.0\n",
      "68 -200.0\n",
      "69 -154.0\n",
      "70 -200.0\n",
      "71 -137.0\n",
      "72 -200.0\n",
      "73 -161.0\n",
      "74 -169.0\n",
      "75 -155.0\n",
      "76 -200.0\n",
      "77 -200.0\n",
      "78 -200.0\n",
      "79 -200.0\n",
      "80 -200.0\n",
      "81 -200.0\n",
      "82 -165.0\n",
      "83 -200.0\n",
      "84 -200.0\n",
      "85 -200.0\n",
      "86 -200.0\n",
      "87 -200.0\n",
      "88 -200.0\n",
      "89 -200.0\n",
      "90 -200.0\n",
      "91 -200.0\n",
      "92 -200.0\n",
      "93 -200.0\n",
      "94 -118.0\n",
      "95 -117.0\n",
      "96 -116.0\n",
      "97 -115.0\n",
      "98 -115.0\n",
      "99 -121.0\n",
      "100 -155.0\n",
      "101 -148.0\n",
      "102 -143.0\n",
      "103 -144.0\n",
      "104 -146.0\n",
      "105 -123.0\n",
      "106 -144.0\n",
      "107 -145.0\n",
      "108 -173.0\n",
      "109 -139.0\n",
      "110 -142.0\n",
      "111 -148.0\n",
      "112 -140.0\n",
      "113 -107.0\n",
      "114 -148.0\n",
      "115 -172.0\n",
      "116 -162.0\n",
      "117 -156.0\n",
      "118 -162.0\n",
      "119 -156.0\n",
      "120 -159.0\n",
      "121 -160.0\n",
      "122 -166.0\n",
      "123 -149.0\n",
      "124 -156.0\n",
      "125 -155.0\n",
      "126 -158.0\n",
      "127 -162.0\n",
      "128 -140.0\n",
      "129 -120.0\n",
      "130 -176.0\n",
      "131 -110.0\n",
      "132 -119.0\n",
      "133 -118.0\n",
      "134 -200.0\n",
      "135 -118.0\n",
      "136 -151.0\n",
      "137 -157.0\n",
      "138 -148.0\n",
      "139 -147.0\n",
      "140 -150.0\n",
      "141 -147.0\n",
      "142 -153.0\n",
      "143 -152.0\n",
      "144 -151.0\n",
      "145 -156.0\n",
      "146 -148.0\n",
      "147 -159.0\n",
      "148 -148.0\n",
      "149 -154.0\n",
      "150 -154.0\n",
      "151 -160.0\n",
      "152 -169.0\n",
      "153 -166.0\n",
      "154 -160.0\n",
      "155 -163.0\n",
      "156 -160.0\n",
      "157 -158.0\n",
      "158 -157.0\n",
      "159 -159.0\n",
      "160 -162.0\n",
      "161 -160.0\n",
      "162 -161.0\n",
      "163 -165.0\n",
      "164 -160.0\n",
      "165 -159.0\n",
      "166 -154.0\n",
      "167 -155.0\n",
      "168 -158.0\n",
      "169 -149.0\n",
      "170 -151.0\n",
      "171 -148.0\n",
      "172 -154.0\n",
      "173 -156.0\n",
      "174 -150.0\n",
      "175 -156.0\n",
      "176 -160.0\n",
      "177 -200.0\n",
      "178 -168.0\n",
      "179 -155.0\n",
      "180 -158.0\n",
      "181 -161.0\n",
      "182 -158.0\n",
      "183 -164.0\n",
      "184 -166.0\n",
      "185 -166.0\n",
      "186 -200.0\n",
      "187 -110.0\n",
      "188 -111.0\n",
      "189 -111.0\n",
      "190 -113.0\n",
      "191 -116.0\n",
      "192 -117.0\n",
      "193 -110.0\n",
      "194 -173.0\n",
      "195 -170.0\n",
      "196 -167.0\n",
      "197 -164.0\n",
      "198 -148.0\n",
      "199 -144.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 200\n",
    "\n",
    "agent = SarsaAgent()\n",
    "env = MountainCar()\n",
    "for episode in range(num_episodes):\n",
    "    rewards = []\n",
    "    s = env.reset()\n",
    "    a = agent.agent_start(s)\n",
    "    done = False\n",
    "    while not done:\n",
    "        r, s_, _, done = env.step(a)\n",
    "        if done == True:\n",
    "            agent.agent_end(r)\n",
    "        else:\n",
    "            s = s_\n",
    "            a = agent.agent_step(r, s)\n",
    "        rewards.append(r)\n",
    "    print(episode, sum(rewards))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, )\n",
    "\n",
    "def gradient_MC(env, policy, ep, gamma, model, callback=None, trace=None):\n",
    "    \"\"\"Gradient Monte Carlo Algorithm\n",
    "    \n",
    "    Params:\n",
    "        env    - environment\n",
    "        policy - function in a form: policy(state)->action\n",
    "        ep     - number of episodes to run\n",
    "        gamma  - discount factor [0..1]\n",
    "        model  - function approximator, already initialised, with method:\n",
    "                     train(state, target) -> None\n",
    "        callback - function in a form: callback(episode, model, trace) -> None\n",
    "        trace  - passed to callback, so it can log data into it\n",
    "    \"\"\"\n",
    "    for e_ in range(ep):\n",
    "        traj, T = generate_episode(env, policy)\n",
    "        Gt = 0\n",
    "        for t in range(T-1,-1,-1):\n",
    "            St, _, _, _ = traj[t]      # (st, rew, done, act)\n",
    "            _, Rt_1, _, _ = traj[t+1]\n",
    "            \n",
    "            Gt = gamma * Gt + Rt_1\n",
    "            model.train(St, Gt)\n",
    "               \n",
    "        if callback is not None:\n",
    "            callback(e_, model, trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b1453-ada6-4ba1-8df0-fe06ab973745",
   "metadata": {},
   "source": [
    "# Quadrature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ab1c2c87-e592-4d09-9211-e2d5fade7718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "251436c2-f2bb-4921-a99f-9db216d73394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapeziod_rule(a,b):\n",
    "    return (b-a) * (function(a) + function(b)) / 2.0\n",
    "\n",
    "def integral_rule(a,b):\n",
    "    return trapeziod_rule(a,b)\n",
    "\n",
    "def function(x):\n",
    "    return x**5\n",
    "\n",
    "def adaptive_quadrature(a0, b0, tol0):\n",
    "    sums = 0.0\n",
    "    n = 1\n",
    "    a = np.zeros(100000)\n",
    "    b = np.zeros(100000)\n",
    "    tol = np.zeros(100000)\n",
    "    app = np.zeros(100000)\n",
    "    iters = 0\n",
    "    \n",
    "    a[1] = a0\n",
    "    b[1] = b0\n",
    "    tol[1] = tol0\n",
    "    app[1] = integral_rule(a0,b0)\n",
    "    \n",
    "    while n > 0:\n",
    "        iters += 1\n",
    "        c = (a[n] + b[n]) / 2\n",
    "        oldapp = app[n]\n",
    "        app[n] = integral_rule(a[n], c)\n",
    "        app[n+1] = integral_rule(c, b[n])\n",
    "        \n",
    "        if np.abs(oldapp - (app[n]+app[n+1])) < 3 * tol[n]:\n",
    "            sums = sums + app[n] + app[n+1] #success\n",
    "            n = n - 1 #done with interval\n",
    "            \n",
    "        else:    #divide into two intervals\n",
    "            b[n+1] = b[n] #setup new intervals\n",
    "            b[n] = c  #setup new intervals\n",
    "            a[n+1] = c #setup new intervals\n",
    "            tol[n] = tol[n] / 2\n",
    "            tol[n+1] = tol[n]\n",
    "            n = n + 1\n",
    "    return sums,iters\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c5b90e8b-21ab-4f81-8f19-ce1397ea9d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 10 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      2\u001b[0m truth \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 4\u001b[0m quad,iters \u001b[38;5;241m=\u001b[39m \u001b[43madaptive_quadrature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m h \u001b[38;5;241m=\u001b[39m iters\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, b, num\u001b[38;5;241m=\u001b[39mh)\n",
      "Cell \u001b[0;32mIn[173], line 22\u001b[0m, in \u001b[0;36madaptive_quadrature\u001b[0;34m(a0, b0, tol0)\u001b[0m\n\u001b[1;32m     20\u001b[0m b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m b0\n\u001b[1;32m     21\u001b[0m tol[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m tol0\n\u001b[0;32m---> 22\u001b[0m app[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mintegral_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[173], line 5\u001b[0m, in \u001b[0;36mintegral_rule\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintegral_rule\u001b[39m(a,b):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrapeziod_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 2\u001b[0m, in \u001b[0;36mtrapeziod_rule\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrapeziod_rule\u001b[39m(a,b):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (b\u001b[38;5;241m-\u001b[39ma) \u001b[38;5;241m*\u001b[39m (\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m function(b)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[173], line 8\u001b[0m, in \u001b[0;36mfunction\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(x):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/adaptive-time-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2953\u001b[0m, in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[1;32m   2837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2838\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2839\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/adaptive-time-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 10 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "b = 15\n",
    "truth = b**5/5\n",
    "\n",
    "quad,iters = adaptive_quadrature(0, b, 0.00005)\n",
    "\n",
    "h = iters\n",
    "x = np.linspace(0, b, num=h)\n",
    "y = function(x)\n",
    "trap = np.trapz(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e47a610c-d7d2-49b5-a223-6845a99d94d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151738.20744045908\n",
      "3.5363336792215705e-05\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(truth - quad))\n",
    "print(np.abs(truth - trap))\n",
    "\n",
    "print(iters - h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fd80460b-0e1b-450d-a201-3180938beb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84605"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "312cfe86-336d-4630-998f-04f042df7f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84605"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0269dd-6bda-475d-b06c-4b4b3d1359fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
