{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from adaptive_time.features import Fourier_Features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import adaptive_time.utils\n",
    "\n",
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"CartPole-OURS-v0\",\n",
    "    entry_point=\"adaptive_time.environments.cartpole:CartPoleEnv\",\n",
    "    vector_entry_point=\"adaptive_time.environments.cartpole:CartPoleVectorEnv\",\n",
    "    max_episode_steps=500,\n",
    "    reward_threshold=475.0,\n",
    ")\n",
    "\n",
    "def reset_randomness(seed, env):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # env.seed(seed)\n",
    "    env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.02\n",
    "env.stepTime(tau)\n",
    "\n",
    "def generate_trajectory(env, policy=None):\n",
    "    observation, _ = env.reset()\n",
    "    trajectory = []\n",
    "    terminated = False\n",
    "    steps = 0\n",
    "    if policy is None:\n",
    "        policy = lambda x: env.action_space.sample()\n",
    "    while not terminated:\n",
    "        steps += 1\n",
    "        action = policy(observation)\n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        trajectory.append([observation, action, reward, observation_])\n",
    "        observation = observation_\n",
    "\n",
    "        if steps % 5000 == 0:\n",
    "            print('Good trajectory!', steps)\n",
    "\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "trajectory = generate_trajectory(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_sa(phi_x, a, prev_phi_sa=None):\n",
    "    \"\"\"Form the (state, action) feature, potentially reusing memory.\n",
    "    \n",
    "    - phi_x: the state feature\n",
    "    - a: the action\n",
    "    - prev_phi_sa: the previous state,action feature, which can be\n",
    "      reused to avoid memory allocation.\n",
    "\n",
    "    Returns the feature as a (2, d) array. Use a flat copy.\n",
    "    \"\"\"\n",
    "    if prev_phi_sa is not None:\n",
    "        prev_phi_sa.fill(0)\n",
    "        phi_sa = prev_phi_sa\n",
    "    else:\n",
    "        phi_sa = np.zeros((2, phi_x.size))\n",
    "    phi_sa[a] = phi_x\n",
    "    return phi_sa\n",
    "\n",
    "\n",
    "def ols_monte_carlo_q(\n",
    "        env, phi, weights, targets, features, x0, policy=None, print_trajectory=False, gamma = 0.999):\n",
    "    trajectory = generate_trajectory(env, policy=policy)\n",
    "    if print_trajectory:\n",
    "        print(\"trajectory-len: \", len(trajectory), \"; trajectory:\")\n",
    "        for idx, (o, a, r, o_) in enumerate(trajectory):\n",
    "            # * ignore reward, as it is always the same here.\n",
    "            # * o_ is the same as the next o.\n",
    "            print(f\"* {idx:4d}: o: {o}\\n\\t --> action: {a}\")\n",
    "    N = len(trajectory)\n",
    "    G = 0\n",
    "    x_sa = np.zeros((2, phi.num_parameters))\n",
    "    returns_a0 = []  # from x0 (the initial state), action 0\n",
    "    returns_a1 = []  # from x0 (the initial state), action 1\n",
    "    for t in tqdm(range(N-1,-1,-1)):\n",
    "        state, action, reward, _ = trajectory[t]\n",
    "        G = gamma*G + reward\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        # Record empirical returns.\n",
    "        if np.linalg.norm(x-x0) < 0.00001:\n",
    "            if action == 0:\n",
    "                returns_a0.append(G)\n",
    "                returns_a1.append(-0)\n",
    "            elif action == 1:\n",
    "                returns_a1.append(G)\n",
    "                returns_a0.append(-0)\n",
    "\n",
    "        x_sa = phi_sa(x, action, x_sa)\n",
    "        x_sa_flat = x_sa.flatten()\n",
    "\n",
    "        features += np.outer(x_sa_flat, x_sa_flat)\n",
    "        targets += G * x_sa_flat\n",
    "    try:\n",
    "        weights = np.linalg.solve(features, targets)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Singular matrix in OLS. Using previous weights.\")\n",
    "    return weights, targets, features, (np.mean(returns_a0), np.mean(returns_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[array([ 0.19256485,  1.381852  , -0.20756072, -1.        ], dtype=float32), 0, 0.008930331461485835, array([ 0.22020188,  1.1900171 , -0.22756071, -0.7790094 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(len(trajectory))\n",
    "print(trajectory[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = Fourier_Features()\n",
    "phi.init_fourier_features(4,4)\n",
    "x_thres = 4.8\n",
    "theta_thres = 0.418\n",
    "phi.init_state_normalizers(np.array([x_thres,2.0,theta_thres,1]), np.array([-x_thres,-2.0,-theta_thres,-1]))\n",
    "phi.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  empirical returns: [215.00707851   0.        ]  predicted returns: [216.30639074 220.64720704]\n",
      "episode: 1  empirical returns: [ 0.         60.09221578]  predicted returns: [180.4547449  104.04889367]\n",
      "episode: 2  empirical returns: [248.76047141   0.        ]  predicted returns: [206.91793325 117.49730633]\n",
      "episode: 3  empirical returns: [256.08153032   0.        ]  predicted returns: [220.46624006 125.4473577 ]\n",
      "Good trajectory! 5000\n",
      "Good trajectory! 10000\n",
      "Good trajectory! 15000\n",
      "Good trajectory! 20000\n",
      "Good trajectory! 25000\n",
      "Good trajectory! 30000\n",
      "Good trajectory! 35000\n",
      "Good trajectory! 40000\n",
      "Good trajectory! 45000\n",
      "Good trajectory! 50000\n",
      "Good trajectory! 55000\n",
      "Good trajectory! 60000\n",
      "Good trajectory! 65000\n",
      "Good trajectory! 70000\n",
      "Good trajectory! 75000\n",
      "Good trajectory! 80000\n",
      "Good trajectory! 85000\n",
      "Good trajectory! 90000\n",
      "Good trajectory! 95000\n",
      "Good trajectory! 100000\n",
      "Good trajectory! 105000\n",
      "Good trajectory! 110000\n",
      "Good trajectory! 115000\n",
      "Good trajectory! 120000\n",
      "Good trajectory! 125000\n",
      "Good trajectory! 130000\n",
      "Good trajectory! 135000\n",
      "Good trajectory! 140000\n",
      "Good trajectory! 145000\n",
      "Good trajectory! 150000\n",
      "Good trajectory! 155000\n",
      "Good trajectory! 160000\n",
      "Good trajectory! 165000\n",
      "Good trajectory! 170000\n",
      "Good trajectory! 175000\n",
      "Good trajectory! 180000\n",
      "Good trajectory! 185000\n",
      "Good trajectory! 190000\n",
      "Good trajectory! 195000\n",
      "Good trajectory! 200000\n",
      "Good trajectory! 205000\n",
      "Good trajectory! 210000\n",
      "Good trajectory! 215000\n",
      "Good trajectory! 220000\n",
      "Good trajectory! 225000\n",
      "Good trajectory! 230000\n",
      "Good trajectory! 235000\n",
      "Good trajectory! 240000\n",
      "Good trajectory! 245000\n",
      "Good trajectory! 250000\n",
      "Good trajectory! 255000\n",
      "Good trajectory! 260000\n",
      "Good trajectory! 265000\n",
      "Good trajectory! 270000\n",
      "Good trajectory! 275000\n",
      "Good trajectory! 280000\n",
      "Good trajectory! 285000\n",
      "Good trajectory! 290000\n",
      "Good trajectory! 295000\n",
      "Good trajectory! 300000\n",
      "Good trajectory! 305000\n",
      "Good trajectory! 310000\n",
      "Good trajectory! 315000\n",
      "Good trajectory! 320000\n",
      "Good trajectory! 325000\n",
      "Good trajectory! 330000\n",
      "Good trajectory! 335000\n",
      "Good trajectory! 340000\n",
      "Good trajectory! 345000\n",
      "Good trajectory! 350000\n",
      "Good trajectory! 355000\n",
      "Good trajectory! 360000\n",
      "Good trajectory! 365000\n",
      "Good trajectory! 370000\n",
      "Good trajectory! 375000\n",
      "Good trajectory! 380000\n",
      "Good trajectory! 385000\n",
      "Good trajectory! 390000\n",
      "Good trajectory! 395000\n",
      "Good trajectory! 400000\n",
      "Good trajectory! 405000\n",
      "Good trajectory! 410000\n",
      "Good trajectory! 415000\n",
      "Good trajectory! 420000\n",
      "Good trajectory! 425000\n",
      "Good trajectory! 430000\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "epsilon = 0.1\n",
    "\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "# We record:\n",
    "returns_per_episode_q = np.zeros((2, num_episodes))\n",
    "average_returns_q = np.zeros((2, num_episodes))  # the cumulative average of the above\n",
    "predicted_returns_q = np.zeros((2, num_episodes))\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "d = len(phi.get_fourier_feature(observation))\n",
    "assert d == phi.num_parameters\n",
    "features = np.identity(2 * d)   # An estimate of A = xx^T\n",
    "targets = np.zeros(2 * d)  # An estimate of b = xG\n",
    "weights = np.zeros(2 * d)   # The weights that approximate A^{-1} b\n",
    "\n",
    "x_0 = phi.get_fourier_feature([0,0,0,0])  # the initial state\n",
    "x_sa0 = phi_sa(x_0, 0)\n",
    "x_sa1 = phi_sa(x_0, 1)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    def policy(state):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(2)\n",
    "        for action in [0, 1]:\n",
    "            x_sa = phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    weights, targets, features, cur_avr_returns = ols_monte_carlo_q(\n",
    "        env, phi, weights, targets, features, x_0, policy=policy, print_trajectory=False)\n",
    "    \n",
    "    # Store the empirical and predicted returns. For any episode, we may\n",
    "    # or may not have empirical returns for both actions. When we don't have an\n",
    "    # estimate, `nan` is returned.\n",
    "    returns_per_episode_q[:, episode] = cur_avr_returns\n",
    "    average_returns_q[:, episode] = np.nanmean(returns_per_episode_q[:, :episode+1], axis=1)\n",
    "\n",
    "    predicted_returns_q[0, episode] = np.inner(x_sa0.flatten(), weights)\n",
    "    predicted_returns_q[1, episode] = np.inner(x_sa1.flatten(), weights)\n",
    "    print(\n",
    "        'episode:', episode,\n",
    "        ' empirical returns:' , returns_per_episode_q[:, episode],\n",
    "        ' predicted returns:' , predicted_returns_q[:, episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20943951023931953"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12 * 2 * np.pi / 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-time-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
