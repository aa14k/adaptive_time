{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of Trade-offs\n",
    "\n",
    "In this notebook we compare different time discretization methods. First, we collect\n",
    "a trajectory data from the environment at a fine discretization level (this is also\n",
    "the discretization level we run the policy at -- right now, anyway). Then we compare:\n",
    "\n",
    "1. Using uniform discretization at different granularities, e.g. updating with every\n",
    "    1st, 10th, 100th, ...? interactions.\n",
    "2. Using the adaptive method with different tolarances.\n",
    "\n",
    "In order to average out randomness, we'll repeat each setting 3 times for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from adaptive_time.features import Fourier_Features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import adaptive_time.utils\n",
    "from adaptive_time import environments\n",
    "from adaptive_time import mc2\n",
    "from adaptive_time import samplers\n",
    "\n",
    "import enum\n",
    "\n",
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"CartPole-OURS-v0\",\n",
    "    entry_point=\"adaptive_time.environments.cartpole:CartPoleEnv\",\n",
    "    vector_entry_point=\"adaptive_time.environments.cartpole:CartPoleVectorEnv\",\n",
    "    max_episode_steps=500,\n",
    "    reward_threshold=475.0,\n",
    ")\n",
    "\n",
    "def reset_randomness(seed, env):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # env.seed(seed)\n",
    "    env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage of the environment.\n",
    "print(\n",
    "    \"We run the same environment and simple policy twice,\\n\"\n",
    "    \"with different time discretizations. The policy we use\\n\"\n",
    "    \"will always go left, so the time discretization does not\\n\"\n",
    "    \"make a difference to the behaviour, and the total return\\n\"\n",
    "    \"will be the same.\")\n",
    "print()\n",
    "\n",
    "policy = lambda obs: 0\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.02\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj, early_term = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_1 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_1)\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj, early_term = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_2 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_2)\n",
    "\n",
    "#np.testing.assert_almost_equal(total_return_1, total_return_2, decimal=0)\n",
    "\n",
    "print()\n",
    "print(\n",
    "    \"We can expect some difference because we may get an extra\\n\"\n",
    "    \"timesteps in the more fine-grained discretization, but the\\n\"\n",
    "    \"difference should be smallish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** you must adjust the discount factor if changing time-scales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = Fourier_Features()\n",
    "phi.init_fourier_features(4,4)\n",
    "x_thres = 4.8\n",
    "theta_thres = 0.418\n",
    "phi.init_state_normalizers(\n",
    "    np.array([x_thres,2.0,theta_thres,1]),\n",
    "    np.array([-x_thres,-2.0,-theta_thres,-1]))\n",
    "phi.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BudgetType(enum.Enum):\n",
    "    INTERACTIONS = 1\n",
    "    UPDATES = 2\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "        seed, env, sampler, epsilon, budget, budget_type: BudgetType,\n",
    "        termination_prob, gamma, tqdm=None, print_trajectory=False):\n",
    "    \"\"\"Keeps interacting until the budget is (approximately) used up, returns stats.\n",
    "    \n",
    "    Note that the budgets are in terms of processed interactions (or updates). We\n",
    "    will do one last episode, even if the budget is used up, so that we can evaluate\n",
    "    the final weights.\n",
    "    \"\"\"\n",
    "    if tqdm is None:\n",
    "        tqdm_use = lambda x: x\n",
    "\n",
    "    # We record:\n",
    "    total_return = [0]  # The total return at the end of each episode.\n",
    "    total_pivots = [0]\n",
    "    total_interactions = [0]\n",
    "    num_episode = [0]\n",
    "\n",
    "    # Each of the following will record the values for both actions.\n",
    "    # Each element of these will be a (2,)-np.array, so we can just stack them.\n",
    "    returns_per_episode_q = []\n",
    "    predicted_returns_q = []\n",
    "\n",
    "    reset_randomness(seed, env)\n",
    "\n",
    "    observation, _ = env.reset(seed=seed)\n",
    "    d = len(phi.get_fourier_feature(observation))\n",
    "    assert d == phi.num_parameters\n",
    "    features = np.identity(2 * d)   # An estimate of A = xx^T\n",
    "    targets = np.zeros(2 * d)  # An estimate of b = xG\n",
    "    weights = np.zeros(2 * d)   # The weights that approximate A^{-1} b\n",
    "\n",
    "    x_0 = phi.get_fourier_feature([0,0,0,0])  # the initial state\n",
    "    x_sa0 = mc2.phi_sa(x_0, 0)\n",
    "    x_sa1 = mc2.phi_sa(x_0, 1)\n",
    "\n",
    "    def remaining_steps():\n",
    "        if budget_type == BudgetType.INTERACTIONS:\n",
    "            return budget - total_interactions[-1]\n",
    "        elif budget_type == BudgetType.UPDATES:\n",
    "            return budget - total_pivots[-1]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown budget type\")\n",
    "\n",
    "    def policy(state, weights):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(2)\n",
    "        for action in [0, 1]:\n",
    "            x_sa = mc2.phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    while remaining_steps() > 0:\n",
    "\n",
    "        trajectory, early_term = environments.generate_trajectory(\n",
    "                env, policy=lambda s: policy(state=s, weights=weights),\n",
    "                termination_prob=termination_prob)\n",
    "        \n",
    "        # Process and record the return.\n",
    "        return_this_episode = sum(ts[2] for ts in trajectory)\n",
    "        if total_return:\n",
    "            total_return.append(total_return[-1] + return_this_episode)\n",
    "        else:\n",
    "            total_return.append(return_this_episode)\n",
    "\n",
    "        if print_trajectory:\n",
    "            print(\"trajectory-len: \", len(trajectory), \"; trajectory:\")\n",
    "            for idx, (o, a, r, o_) in enumerate(trajectory):\n",
    "                # * ignore reward, as it is always the same here.\n",
    "                # * o_ is the same as the next o.\n",
    "                print(f\"* {idx:4d}: o: {o}\\n\\t --> action: {a}\")\n",
    "\n",
    "        assert early_term is False, \"We should not terminate early in this experiment.\"\n",
    "\n",
    "        # Do updates, record stats from the processed trajectory.\n",
    "        weights, targets, features, cur_avr_returns, num_pivots = mc2.ols_monte_carlo(\n",
    "            trajectory, sampler, tqdm_use, phi, weights, targets, features, x_0, gamma)\n",
    "        \n",
    "        # Update the stats.\n",
    "        total_pivots.append(total_pivots[-1] + num_pivots)\n",
    "        total_interactions.append(total_interactions[-1] + len(trajectory))\n",
    "        num_episode.append(num_episode[-1] + 1)\n",
    "        \n",
    "        # Store the empirical and predicted returns. For any episode, we may\n",
    "        # or may not have empirical returns for both actions. When we don't have an\n",
    "        # estimate, `nan` is returned.\n",
    "        returns_per_episode_q.append(cur_avr_returns)\n",
    "        predicted_returns_q.append(np.array(\n",
    "            [np.inner(x_sa0.flatten(), weights),\n",
    "                np.inner(x_sa1.flatten(), weights)]))\n",
    "    \n",
    "    # The following variant produces plots where we can see\n",
    "    # the effect of the last update.\n",
    "    # # Do one more evaluation run.\n",
    "    # trajectory, early_term = environments.generate_trajectory(\n",
    "    #     env, policy=lambda s: policy(state=s, weights=weights),\n",
    "    #     termination_prob=termination_prob)\n",
    "    # return_this_episode = sum(ts[2] for ts in trajectory)\n",
    "    # print(sampler, \"final return: \", return_this_episode)\n",
    "    # total_return.append(total_return[-1] + return_this_episode)\n",
    "\n",
    "    return {\n",
    "        \"total_return\": total_return,\n",
    "        \"total_pivots\": total_pivots,\n",
    "        \"total_interactions\": total_interactions,\n",
    "        \"num_episode\": num_episode,\n",
    "        \"returns_per_episode_q\": returns_per_episode_q,\n",
    "        \"predicted_returns_q\": predicted_returns_q,\n",
    "    }\n",
    "    \n",
    "    # The following variant produces plots where we\n",
    "    # ensure that the last x-points are within the budget.\n",
    "    # Do one more evaluation run.\n",
    "\n",
    "    # return {\n",
    "    #     \"total_return\": total_return,\n",
    "    #     \"total_pivots\": total_pivots[:-1],\n",
    "    #     \"total_interactions\": total_interactions[:-1],\n",
    "    #     \"num_episode\": num_episode[:-1],\n",
    "    #     \"returns_per_episode_q\": returns_per_episode_q,\n",
    "    #     \"predicted_returns_q\": predicted_returns_q,\n",
    "    # }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination_prob=1.0/10000.0   # 1.0/500000.0\n",
    "epsilon = 0.05\n",
    "# budget = 10_000\n",
    "# budget_type = BudgetType.INTERACTIONS\n",
    "budget = 5_000\n",
    "budget_type = BudgetType.UPDATES\n",
    "\n",
    "num_runs = 45\n",
    "\n",
    "tau = 0.002\n",
    "# tau = 0.02\n",
    "env.stepTime(tau)\n",
    "\n",
    "# sampler = samplers.AdaptiveQuadratureSampler2(tolerance=0.1)\n",
    "# sampler = samplers.AdaptiveQuadratureSampler2(tolerance=0.0)\n",
    "\n",
    "samplers_tried = dict(\n",
    "    q0_10=samplers.AdaptiveQuadratureSampler2(tolerance=10),\n",
    "    q0_5=samplers.AdaptiveQuadratureSampler2(tolerance=5),\n",
    "    q0_1=samplers.AdaptiveQuadratureSampler2(tolerance=1),\n",
    "    u5=samplers.UniformSampler2(5),\n",
    "    u10=samplers.UniformSampler2(10),\n",
    "    u20=samplers.UniformSampler2(20),\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for name, sampler in tqdm(samplers_tried.items()):\n",
    "    print(name, sampler)\n",
    "    #results[name] = []\n",
    "    results[name] = Parallel(n_jobs = num_runs)(\n",
    "        delayed(run_experiment)(\n",
    "            seed+run, env, sampler, epsilon, budget, budget_type,\n",
    "            termination_prob, gamma=0.99999, tqdm=None, print_trajectory=False)\n",
    "            for run in range(num_runs)\n",
    "        )\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[\"q0_5\"][0]\n",
    "print(len(results[\"q0_5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf(tuples_of_x_y_labels_kwargs, title, show):\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    for x, y, label, kwargs in tuples_of_x_y_labels_kwargs:\n",
    "        plt.plot(x, y, label=label, **kwargs)\n",
    "\n",
    "    plt.ylabel('Cumulative\\nEpisode\\nReturn', rotation=0, labelpad=40)\n",
    "    plt.legend()\n",
    "\n",
    "    if title is not None:\n",
    "      plt.title(title)\n",
    "\n",
    "    if show:\n",
    "      plt.show()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # if len(results) > 8:\n",
    "# #    raise ValueError(\"Too many results to plot\")\n",
    "# # colors = plt.cm.get_cmap('Set2')(np.linspace(0,1,len(results)))\n",
    "# colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# tuples_of_x_y_labels_kwargs = []\n",
    "# for i, (name, stats) in enumerate(results.items()):\n",
    "#     for run in stats:\n",
    "#         tuples_of_x_y_labels_kwargs.append((\n",
    "#             run[\"total_interactions\"],\n",
    "#             run[\"total_return\"],\n",
    "#             name,\n",
    "#             {\"color\": colors[i]}\n",
    "#         ))\n",
    "\n",
    "# ax = plot_perf(tuples_of_x_y_labels_kwargs,\n",
    "#                \"Total Return vs Interactions\", False)\n",
    "# # ax.set_yscale('log')\n",
    "# ax.set_xlabel('Number of Interactions')\n",
    "# # ax.set_ylim(24, 30)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ax = plt.gca()\n",
    "# ax.cla()\n",
    "\n",
    "# if len(results) > 8:\n",
    "#    raise ValueError(\"Too many results to plot\")\n",
    "# colors = plt.cm.get_cmap('Set2')(np.linspace(0,1,len(results)))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "tuples_of_x_y_labels_kwargs = []\n",
    "for i, (name, stats) in enumerate(results.items()):\n",
    "    for r_idx, run in enumerate(stats):\n",
    "        tuples_of_x_y_labels_kwargs.append((\n",
    "            run[\"total_pivots\"],\n",
    "            run[\"total_return\"],\n",
    "            name if r_idx==0 else None,\n",
    "            # {\"color\": colors[i], \"marker\": \".\", \"linestyle\": \"None\", \"markersize\": 5, \"alpha\": 0.8}\n",
    "            {\"color\": colors[i], \"marker\": \".\", \"linestyle\": \"-\", \"markersize\": 5, \"alpha\": 0.8}\n",
    "        ))\n",
    "\n",
    "ax = plot_perf(tuples_of_x_y_labels_kwargs,\n",
    "               \"Total Return vs Updates\", False)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel('Number of Updates')\n",
    "# if budget_type == BudgetType.UPDATES:\n",
    "#     ax.set_xlim(-100, budget)\n",
    "#     # Find the y-limits of the plot.\n",
    "#     # for stats in .values():\n",
    "#     for runs in results.values():\n",
    "#         max_total_return = -1\n",
    "#         for run in runs:\n",
    "#             cur_max = max(run[\"total_return\"][:-1])\n",
    "#             if cur_max > max_total_return:\n",
    "#                 max_total_return = cur_max\n",
    "\n",
    "#     #     max_total_return = max(max(runs[\"total_return\"][:-1]) for runs in results.values())\n",
    "#     # max_total_return = max(max(runs[\"total_return\"][:-1]) for runs in results.values())\n",
    "#     # max_total_return = max(max(run[\"total_return\"]) for run in results.values())\n",
    "#     ax.set_ylim(-max_total_return/50, max_total_return)\n",
    "\n",
    "# ax.set_ylim(24, 30)   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, label, kwargs in tuples_of_x_y_labels_kwargs:\n",
    "    if label == \"u20\":\n",
    "        print(x)\n",
    "        print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['q0_10'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_updates = max(\n",
    "    max(run[\"total_pivots\"][-1] for run in runs)\n",
    "    for runs in results.values())\n",
    "print(max_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_updates = np.arange(0, max_updates+1)   # x-axis for the interpolated results\n",
    "\n",
    "interpolated_results = {}  # Each element will be a numpy array\n",
    "for name, stats_for_runs in results.items():\n",
    "    interpolated_results[name] = np.zeros((len(stats_for_runs), len(all_updates)))\n",
    "    for run_idx, run in enumerate(stats_for_runs):\n",
    "        interpolated_results[name][run_idx] = np.interp(\n",
    "            all_updates, run[\"total_pivots\"], run[\"total_return\"])\n",
    "\n",
    "print(interpolated_results.keys())\n",
    "print(interpolated_results['q0_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = {}\n",
    "for name, returns in interpolated_results.items():\n",
    "    mean_results[name] = returns.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "tuples_of_x_y_labels_kwargs = []\n",
    "for i, (name, returns) in enumerate(mean_results.items()):\n",
    "    tuples_of_x_y_labels_kwargs.append((\n",
    "        all_updates, returns, name,\n",
    "        # {\"color\": colors[i], \"marker\": \".\", \"linestyle\": \"None\", \"markersize\": 5, \"alpha\": 0.8}\n",
    "        {\"color\": colors[i], \"marker\": \"\", \"linestyle\": \"-\"}\n",
    "    ))\n",
    "\n",
    "\n",
    "# for i, (name, stats) in enumerate(results.items()):\n",
    "#     for r_idx, run in enumerate(stats):\n",
    "#         tuples_of_x_y_labels_kwargs.append((\n",
    "#             run[\"total_pivots\"],\n",
    "#             run[\"total_return\"],\n",
    "#             None,\n",
    "#             # name if r_idx==0 else None,\n",
    "#             # {\"color\": colors[i], \"marker\": \".\", \"linestyle\": \"None\", \"markersize\": 5, \"alpha\": 0.8}\n",
    "#             # {\"color\": colors[i], \"marker\": \".\", \"linestyle\": \"-\", \"markersize\": 5, \"alpha\": 0.8}\n",
    "#             {\"color\": colors[i], \"marker\": \"\", \"linestyle\": \"-\", \"alpha\": 0.2}\n",
    "#         ))\n",
    "\n",
    "\n",
    "ax = plot_perf(tuples_of_x_y_labels_kwargs,\n",
    "               \"Total Return vs Updates\", False)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_xlabel('Number of Updates')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print()\n",
    "# print(\"Results, a list of num_episodes, and a list of num_pivots for the different seeds:\")\n",
    "# for name, sub_results in results.items():\n",
    "#     num_episodes, num_pivots, num_interactions = zip(*sub_results)\n",
    "#     print(f\"* {name}\")\n",
    "#     if -1 in num_episodes:\n",
    "#         num_eps_stats = f\"?? +- ??\"\n",
    "#     else:\n",
    "#         mean_num_episodes = np.mean(num_episodes)\n",
    "#         std_err_episodes = np.std(num_episodes) / np.sqrt(len(num_episodes))\n",
    "#         num_eps_stats = f\"{np.mean(num_episodes):.2f} +- {std_err_episodes:.2f}\"\n",
    "\n",
    "#     std_err_pivots = np.std(num_pivots) / np.sqrt(len(num_pivots))\n",
    "#     std_err_num_interactions = np.std(num_interactions) / np.sqrt(len(num_interactions))\n",
    "#     print(f\"    * num_episodes: {num_eps_stats}                full list: {num_episodes}\")\n",
    "#     print(f\"    * num_pivots:   {np.mean(num_pivots):.2f} +- {std_err_pivots:.2f}           full list: {num_pivots}\")\n",
    "#     print(f\"    * num_interactions:   {np.mean(num_interactions):.2f} +- {std_err_num_interactions:.2f}\"\n",
    "#           f\"           full list: {num_interactions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
