{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of Trade-offs\n",
    "\n",
    "In this notebook we compare different time discretization methods. First, we collect\n",
    "a trajectory data from the environment at a fine discretization level (this is also\n",
    "the discretization level we run the policy at -- right now, anyway). Then we compare:\n",
    "\n",
    "1. Using uniform discretization at different granularities, e.g. updating with every\n",
    "    1st, 10th, 100th, ...? interactions.\n",
    "2. Using the adaptive method with different tolarances.\n",
    "\n",
    "In order to average out randomness, we'll repeat each setting 3 times for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from adaptive_time.features import Fourier_Features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import adaptive_time.utils\n",
    "from adaptive_time import environments\n",
    "from adaptive_time import mc2\n",
    "from adaptive_time import samplers\n",
    "\n",
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"CartPole-OURS-v0\",\n",
    "    entry_point=\"adaptive_time.environments.cartpole:CartPoleEnv\",\n",
    "    vector_entry_point=\"adaptive_time.environments.cartpole:CartPoleVectorEnv\",\n",
    "    max_episode_steps=500,\n",
    "    reward_threshold=475.0,\n",
    ")\n",
    "\n",
    "def reset_randomness(seed, env):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # env.seed(seed)\n",
    "    env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We run the same environment and simple policy twice,\n",
      "with different time discretizations. The policy we use\n",
      "will always go left, so the time discretization does not\n",
      "make a difference to the behaviour, and the total return\n",
      "will be the same.\n",
      "\n",
      "Total undiscounted return:  10.589912009424973\n",
      "Total undiscounted return:  10.017508472458736\n",
      "\n",
      "We can expect some difference because we may get an extra\n",
      "timesteps in the more fine-grained discretization, but the\n",
      "difference should be smallish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szepi1991/Code/adaptive_time/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.stepTime to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.stepTime` for environment variables or `env.get_wrapper_attr('stepTime')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sample usage of the environment.\n",
    "print(\n",
    "    \"We run the same environment and simple policy twice,\\n\"\n",
    "    \"with different time discretizations. The policy we use\\n\"\n",
    "    \"will always go left, so the time discretization does not\\n\"\n",
    "    \"make a difference to the behaviour, and the total return\\n\"\n",
    "    \"will be the same.\")\n",
    "print()\n",
    "\n",
    "policy = lambda obs: 0\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.02\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_1 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_1)\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_2 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_2)\n",
    "\n",
    "np.testing.assert_almost_equal(total_return_1, total_return_2, decimal=0)\n",
    "\n",
    "print()\n",
    "print(\n",
    "    \"We can expect some difference because we may get an extra\\n\"\n",
    "    \"timesteps in the more fine-grained discretization, but the\\n\"\n",
    "    \"difference should be smallish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** you must adjust the discount factor if changing time-scales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = Fourier_Features()\n",
    "phi.init_fourier_features(4,4)\n",
    "x_thres = 4.8\n",
    "theta_thres = 0.418\n",
    "phi.init_state_normalizers(np.array([x_thres,2.0,theta_thres,1]), np.array([-x_thres,-2.0,-theta_thres,-1]))\n",
    "phi.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 41/548 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szepi1991/Code/adaptive_time/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.stepTime to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.stepTime` for environment variables or `env.get_wrapper_attr('stepTime')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5502de36164a42408f151f40e7ca513c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  empirical returns: [39.07399847  0.        ]  predicted returns: [38.54726255 12.07184041]\n",
      "Using 18/123 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5cec30e82c45c182bb8aea341847ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1  empirical returns: [9.52965598 0.        ]  predicted returns: [24.23798007 12.14794427]\n",
      "Using 19/159 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828fc3205044cedb63f086d6b99b822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2  empirical returns: [10.95753037  0.        ]  predicted returns: [19.85380852 12.75762596]\n",
      "Using 23/179 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372d625585744e0bb6124b2a74c2d891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3  empirical returns: [12.16771336  0.        ]  predicted returns: [17.97095232 12.70873222]\n",
      "Using 23/177 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e27b1d0067e406794eb993133a4c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4  empirical returns: [12.08726013  0.        ]  predicted returns: [16.80486932 12.95675372]\n",
      "Using 21/173 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c275118e29234c088fd54f12b8414a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5  empirical returns: [11.8715752  0.       ]  predicted returns: [15.98069162 13.08495193]\n",
      "Using 23/172 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a414e134a14f29a1515f0037ef0d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6  empirical returns: [11.71337222  0.        ]  predicted returns: [15.37294572 13.32588551]\n",
      "Did 5000 steps! 5000\n",
      "Did 5000 steps! 10000\n",
      "Did 5000 steps! 15000\n",
      "Did 5000 steps! 20000\n",
      "Did 5000 steps! 25000\n",
      "Did 5000 steps! 30000\n",
      "Did 5000 steps! 35000\n",
      "Did 5000 steps! 40000\n",
      "Did 5000 steps! 45000\n",
      "Did 5000 steps! 50000\n",
      "Did 5000 steps! 55000\n",
      "Did 5000 steps! 60000\n",
      "Did 5000 steps! 65000\n",
      "Did 5000 steps! 70000\n",
      "Did 5000 steps! 75000\n",
      "Did 5000 steps! 80000\n",
      "Did 5000 steps! 85000\n",
      "Did 5000 steps! 90000\n",
      "Did 5000 steps! 95000\n",
      "Did 5000 steps! 100000\n",
      "Did 5000 steps! 105000\n",
      "Did 5000 steps! 110000\n",
      "Did 5000 steps! 115000\n",
      "Did 5000 steps! 120000\n",
      "Did 5000 steps! 125000\n",
      "Did 5000 steps! 130000\n",
      "Did 5000 steps! 135000\n",
      "Did 5000 steps! 140000\n",
      "Did 5000 steps! 145000\n",
      "Did 5000 steps! 150000\n",
      "Did 5000 steps! 155000\n",
      "Did 5000 steps! 160000\n",
      "Did 5000 steps! 165000\n",
      "Did 5000 steps! 170000\n",
      "Did 5000 steps! 175000\n",
      "Did 5000 steps! 180000\n",
      "Did 5000 steps! 185000\n",
      "Did 5000 steps! 190000\n",
      "Did 5000 steps! 195000\n",
      "Did 5000 steps! 200000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# adaptive_time.utils.softmax(qs, 1)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adaptive_time\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39margmax(qs)\n\u001b[0;32m---> 45\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m \u001b[43menvironments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_trajectory:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory-len: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trajectory), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; trajectory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/adaptive_time/code/adaptive_time/environments/__init__.py:63\u001b[0m, in \u001b[0;36mgenerate_trajectory\u001b[0;34m(env, seed, policy, termination_prob)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[1;32m     62\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     observation_, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     65\u001b[0m     trajectory\u001b[38;5;241m.\u001b[39mappend([observation, action, reward, observation_])\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mpolicy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     41\u001b[0m     qs[action] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minner(x_sa\u001b[38;5;241m.\u001b[39mflatten(), weights)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# adaptive_time.utils.softmax(qs, 1)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madaptive_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/adaptive_time/code/adaptive_time/utils.py:43\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mArgmax operation on the last axis---randomly break ties\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m- x (np.ndarray): values\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m idxes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(idxes)\n",
      "File \u001b[0;32m~/Code/adaptive_time/.venv/lib/python3.11/site-packages/numpy/core/multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_trajectory = False\n",
    "gamma = 0.999\n",
    "\n",
    "num_episodes = 500\n",
    "epsilon = 0.1\n",
    "\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "sampler = samplers.AdaptiveQuadratureSampler2(tolerance=0.1)\n",
    "sampler = samplers.AdaptiveQuadratureSampler2(tolerance=0.0)\n",
    "\n",
    "\n",
    "# We record:\n",
    "returns_per_episode_q = np.zeros((2, num_episodes))\n",
    "average_returns_q = np.zeros((2, num_episodes))  # the cumulative average of the above\n",
    "predicted_returns_q = np.zeros((2, num_episodes))\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "\n",
    "observation, _ = env.reset(seed=seed)\n",
    "d = len(phi.get_fourier_feature(observation))\n",
    "assert d == phi.num_parameters\n",
    "features = np.identity(2 * d)   # An estimate of A = xx^T\n",
    "targets = np.zeros(2 * d)  # An estimate of b = xG\n",
    "weights = np.zeros(2 * d)   # The weights that approximate A^{-1} b\n",
    "\n",
    "x_0 = phi.get_fourier_feature([0,0,0,0])  # the initial state\n",
    "x_sa0 = mc2.phi_sa(x_0, 0)\n",
    "x_sa1 = mc2.phi_sa(x_0, 1)\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    def policy(state):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(2)\n",
    "        for action in [0, 1]:\n",
    "            x_sa = mc2.phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    trajectory = environments.generate_trajectory(env, policy=policy)\n",
    "\n",
    "    if print_trajectory:\n",
    "        print(\"trajectory-len: \", len(trajectory), \"; trajectory:\")\n",
    "        for idx, (o, a, r, o_) in enumerate(trajectory):\n",
    "            # * ignore reward, as it is always the same here.\n",
    "            # * o_ is the same as the next o.\n",
    "            print(f\"* {idx:4d}: o: {o}\\n\\t --> action: {a}\")\n",
    "\n",
    "    weights, targets, features, cur_avr_returns = mc2.ols_monte_carlo(\n",
    "        trajectory, sampler, tqdm, phi, weights, targets, features, x_0, gamma)\n",
    "    \n",
    "    # Store the empirical and predicted returns. For any episode, we may\n",
    "    # or may not have empirical returns for both actions. When we don't have an\n",
    "    # estimate, `nan` is returned.\n",
    "    returns_per_episode_q[:, episode] = cur_avr_returns\n",
    "    average_returns_q[:, episode] = np.nanmean(returns_per_episode_q[:, :episode+1], axis=1)\n",
    "\n",
    "    predicted_returns_q[0, episode] = np.inner(x_sa0.flatten(), weights)\n",
    "    predicted_returns_q[1, episode] = np.inner(x_sa1.flatten(), weights)\n",
    "    print(\n",
    "        'episode:', episode,\n",
    "        ' empirical returns:' , returns_per_episode_q[:, episode],\n",
    "        ' predicted returns:' , predicted_returns_q[:, episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
