{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of Trade-offs\n",
    "\n",
    "In this notebook we compare different time discretization methods. First, we collect\n",
    "a trajectory data from the environment at a fine discretization level (this is also\n",
    "the discretization level we run the policy at -- right now, anyway). Then we compare:\n",
    "\n",
    "1. Using uniform discretization at different granularities, e.g. updating with every\n",
    "    1st, 10th, 100th, ...? interactions.\n",
    "2. Using the adaptive method with different tolarances.\n",
    "\n",
    "In order to average out randomness, we'll repeat each setting 3 times for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from adaptive_time.features import Fourier_Features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import adaptive_time.utils\n",
    "from adaptive_time import environments\n",
    "from adaptive_time import mc2\n",
    "from adaptive_time import samplers\n",
    "\n",
    "seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"CartPole-OURS-v0\",\n",
    "    entry_point=\"adaptive_time.environments.cartpole:CartPoleEnv\",\n",
    "    vector_entry_point=\"adaptive_time.environments.cartpole:CartPoleVectorEnv\",\n",
    "    max_episode_steps=500,\n",
    "    reward_threshold=475.0,\n",
    ")\n",
    "\n",
    "def reset_randomness(seed, env):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # env.seed(seed)\n",
    "    env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We run the same environment and simple policy twice,\n",
      "with different time discretizations. The policy we use\n",
      "will always go left, so the time discretization does not\n",
      "make a difference to the behaviour, and the total return\n",
      "will be the same.\n",
      "\n",
      "Total undiscounted return:  10.589912009424973\n",
      "Total undiscounted return:  10.017508472458736\n",
      "\n",
      "We can expect some difference because we may get an extra\n",
      "timesteps in the more fine-grained discretization, but the\n",
      "difference should be smallish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szepi1991/Code/adaptive_time/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.stepTime to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.stepTime` for environment variables or `env.get_wrapper_attr('stepTime')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sample usage of the environment.\n",
    "print(\n",
    "    \"We run the same environment and simple policy twice,\\n\"\n",
    "    \"with different time discretizations. The policy we use\\n\"\n",
    "    \"will always go left, so the time discretization does not\\n\"\n",
    "    \"make a difference to the behaviour, and the total return\\n\"\n",
    "    \"will be the same.\")\n",
    "print()\n",
    "\n",
    "policy = lambda obs: 0\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.02\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_1 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_1)\n",
    "\n",
    "env = gym.make('CartPole-OURS-v0')\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "traj = environments.generate_trajectory(env, seed, policy)\n",
    "total_return_2 = sum(ts[2] for ts in traj)\n",
    "print(\"Total undiscounted return: \", total_return_2)\n",
    "\n",
    "np.testing.assert_almost_equal(total_return_1, total_return_2, decimal=0)\n",
    "\n",
    "print()\n",
    "print(\n",
    "    \"We can expect some difference because we may get an extra\\n\"\n",
    "    \"timesteps in the more fine-grained discretization, but the\\n\"\n",
    "    \"difference should be smallish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** you must adjust the discount factor if changing time-scales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = Fourier_Features()\n",
    "phi.init_fourier_features(4,4)\n",
    "x_thres = 4.8\n",
    "theta_thres = 0.418\n",
    "phi.init_state_normalizers(np.array([x_thres,2.0,theta_thres,1]), np.array([-x_thres,-2.0,-theta_thres,-1]))\n",
    "phi.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 41/548 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad795002227b44ccbc7ff8649a61adcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  empirical returns: [2.70082761 0.        ]  predicted returns: [2.68737787 1.56104028]\n",
      "Using 18/123 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9c05e758d4bff924afdc0e2c2dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1  empirical returns: [1.05916968 0.        ]  predicted returns: [1.88507895 1.57379112]\n",
      "Using 21/144 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23745ebb667e409082d1ff17c86f4493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2  empirical returns: [1.25537557 0.        ]  predicted returns: [1.67755039 1.60947657]\n",
      "Using 113/3081 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a671609a7240499495a3a9ad5e2ea531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3  empirical returns: [9.12203382 0.        ]  predicted returns: [4.82117126 7.89959242]\n",
      "Did 5000 steps! 5000\n",
      "Did 5000 steps! 10000\n",
      "Did 5000 steps! 15000\n",
      "Did 5000 steps! 20000\n",
      "Did 5000 steps! 25000\n",
      "Did 5000 steps! 30000\n",
      "Did 5000 steps! 35000\n",
      "Did 5000 steps! 40000\n",
      "Did 5000 steps! 45000\n",
      "Did 5000 steps! 50000\n",
      "Did 5000 steps! 55000\n",
      "Did 5000 steps! 60000\n",
      "Did 5000 steps! 65000\n",
      "Did 5000 steps! 70000\n",
      "Did 5000 steps! 75000\n",
      "Did 5000 steps! 80000\n",
      "Did 5000 steps! 85000\n",
      "Did 5000 steps! 90000\n",
      "Did 5000 steps! 95000\n",
      "Did 5000 steps! 100000\n",
      "Did 5000 steps! 105000\n",
      "Did 5000 steps! 110000\n",
      "Did 5000 steps! 115000\n",
      "Did 5000 steps! 120000\n",
      "Did 5000 steps! 125000\n",
      "Did 5000 steps! 130000\n",
      "Did 5000 steps! 135000\n",
      "Did 5000 steps! 140000\n",
      "Did 5000 steps! 145000\n",
      "Did 5000 steps! 150000\n",
      "Did 5000 steps! 155000\n",
      "Did 5000 steps! 160000\n",
      "Did 5000 steps! 165000\n",
      "Did 5000 steps! 170000\n",
      "Did 5000 steps! 175000\n",
      "Did 5000 steps! 180000\n",
      "Did 5000 steps! 185000\n",
      "Did 5000 steps! 190000\n",
      "Did 5000 steps! 195000\n",
      "Did 5000 steps! 200000\n",
      "Did 5000 steps! 205000\n",
      "Did 5000 steps! 210000\n",
      "Did 5000 steps! 215000\n",
      "Did 5000 steps! 220000\n",
      "Did 5000 steps! 225000\n",
      "Did 5000 steps! 230000\n",
      "Did 5000 steps! 235000\n",
      "Did 5000 steps! 240000\n",
      "Did 5000 steps! 245000\n",
      "Did 5000 steps! 250000\n",
      "Did 5000 steps! 255000\n",
      "Did 5000 steps! 260000\n",
      "Did 5000 steps! 265000\n",
      "Did 5000 steps! 270000\n",
      "Did 5000 steps! 275000\n",
      "Did 5000 steps! 280000\n",
      "Did 5000 steps! 285000\n",
      "Did 5000 steps! 290000\n",
      "Did 5000 steps! 295000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# adaptive_time.utils.softmax(qs, 1)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adaptive_time\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39margmax(qs)\n\u001b[0;32m---> 45\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m \u001b[43menvironments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_trajectory:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory-len: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trajectory), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; trajectory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/adaptive_time/code/adaptive_time/environments/__init__.py:63\u001b[0m, in \u001b[0;36mgenerate_trajectory\u001b[0;34m(env, seed, policy, termination_prob)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[1;32m     62\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     observation_, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     65\u001b[0m     trajectory\u001b[38;5;241m.\u001b[39mappend([observation, action, reward, observation_])\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36mpolicy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Otherwise calculate the best action.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mphi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fourier_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m qs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/Code/adaptive_time/code/adaptive_time/features.py:115\u001b[0m, in \u001b[0;36mFourier_Features.get_fourier_feature\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    113\u001b[0m order_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_list\n\u001b[1;32m    114\u001b[0m state_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m scalars \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij, kj->ik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_new\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#do a row by row dot product with the state. i = length of order list, j = state dimensions, k = 1\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m scalars\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(order_list),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mscalars)\n",
      "File \u001b[0;32m~/Code/adaptive_time/.venv/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[1;32m   1370\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_trajectory = False\n",
    "gamma = 0.999\n",
    "\n",
    "num_episodes = 500\n",
    "epsilon = 0.1\n",
    "\n",
    "tau = 0.002\n",
    "env.stepTime(tau)\n",
    "\n",
    "sampler = samplers.AdaptiveQuadratureSampler2(tolerance=0.1)\n",
    "\n",
    "\n",
    "# We record:\n",
    "returns_per_episode_q = np.zeros((2, num_episodes))\n",
    "average_returns_q = np.zeros((2, num_episodes))  # the cumulative average of the above\n",
    "predicted_returns_q = np.zeros((2, num_episodes))\n",
    "\n",
    "reset_randomness(seed, env)\n",
    "\n",
    "observation, _ = env.reset(seed=seed)\n",
    "d = len(phi.get_fourier_feature(observation))\n",
    "assert d == phi.num_parameters\n",
    "features = np.identity(2 * d)   # An estimate of A = xx^T\n",
    "targets = np.zeros(2 * d)  # An estimate of b = xG\n",
    "weights = np.zeros(2 * d)   # The weights that approximate A^{-1} b\n",
    "\n",
    "x_0 = phi.get_fourier_feature([0,0,0,0])  # the initial state\n",
    "x_sa0 = mc2.phi_sa(x_0, 0)\n",
    "x_sa1 = mc2.phi_sa(x_0, 1)\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    def policy(state):\n",
    "        if random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        # Otherwise calculate the best action.\n",
    "        x = phi.get_fourier_feature(state)\n",
    "        qs = np.zeros(2)\n",
    "        for action in [0, 1]:\n",
    "            x_sa = mc2.phi_sa(x, action)\n",
    "            qs[action] = np.inner(x_sa.flatten(), weights)\n",
    "        # adaptive_time.utils.softmax(qs, 1)\n",
    "        return adaptive_time.utils.argmax(qs)\n",
    "\n",
    "    trajectory = environments.generate_trajectory(env, policy=policy)\n",
    "\n",
    "    if print_trajectory:\n",
    "        print(\"trajectory-len: \", len(trajectory), \"; trajectory:\")\n",
    "        for idx, (o, a, r, o_) in enumerate(trajectory):\n",
    "            # * ignore reward, as it is always the same here.\n",
    "            # * o_ is the same as the next o.\n",
    "            print(f\"* {idx:4d}: o: {o}\\n\\t --> action: {a}\")\n",
    "\n",
    "\n",
    "    # mc2.ols_monte_carlo(\n",
    "    #     trajectory, sampler: samplers.Sampler2, tqdm,\n",
    "    #     phi, weights, targets, features, x0, gamma)\n",
    "\n",
    "    # adaptive_time.utils.discounted_returns(traj, gamma)\n",
    "\n",
    "    weights, targets, features, cur_avr_returns = mc2.ols_monte_carlo(\n",
    "        trajectory, sampler, tqdm, phi, weights, targets, features, x_0, gamma)\n",
    "    \n",
    "    # Store the empirical and predicted returns. For any episode, we may\n",
    "    # or may not have empirical returns for both actions. When we don't have an\n",
    "    # estimate, `nan` is returned.\n",
    "    returns_per_episode_q[:, episode] = cur_avr_returns\n",
    "    average_returns_q[:, episode] = np.nanmean(returns_per_episode_q[:, :episode+1], axis=1)\n",
    "\n",
    "    predicted_returns_q[0, episode] = np.inner(x_sa0.flatten(), weights)\n",
    "    predicted_returns_q[1, episode] = np.inner(x_sa1.flatten(), weights)\n",
    "    print(\n",
    "        'episode:', episode,\n",
    "        ' empirical returns:' , returns_per_episode_q[:, episode],\n",
    "        ' predicted returns:' , predicted_returns_q[:, episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
