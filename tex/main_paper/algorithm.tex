\section{Algorithm}
In this section, we detail both a uniform and adaptive method for discretizing the trajectories when performing updates in Monte-Carlo policy iteration. In \cref{alg:mc}, we detail the variant of Monte-Carlo policy iteration that is trained using either a uniform or adaptive discretization scheme. Given a function that approximates the action-values, \cref{alg:mc} runs the $\varepsilon$-greedy policy induced by the current approximate action-value function and collects $N$ trajectories of, possibly varying, length $T_j$, where $j \in \{1,2,\dotsc,N\}$. If \cref{alg:uniform} is used to discretize the updates, then \cref{alg:mc} performs updates with using every $M$-th state-action-return tuple in the trajectory, i.e. for $t \in \{1,M,2M,\dotsc,T_j\}$ we use tuples $(S_t, A_t, G_t)$ in computing the updated parameter $\theta_i$. 

The choice of $M$ in the uniform d


\begin{proposition}
    For any given tolerance $\tau \geq 0$ and a real-valued list $xs$ of size $N$, \cref{alg:adaptive-quadrature} returns an approximate sum $Q$ that satisfies $|Q - \text{sum}(xs)| \leq \tau$.  
\end{proposition}
\begin{proof}
    The proof will follow by strong induction on the length of the list $N$. We will denote for convenience with $Q_N$ the value returned by the algorithm as an approximation of a sum of length $N$, which is in turn denoted as $\text{sum}_N(xs)$. \\
    \textbf{Base case}: for $N=1$, it trivially holds that: $\text{sum}(xs)=xs[0]=Q_1$. Therefore $|Q - \text{sum}(xs)|=0\leq\tau$. \\
    \textbf{Strong inductive step}: assume the proposition is true for any $n-1\leq N$, for an arbitrary $\overline{\tau}\geq0$. Consider $\tau=2\overline{\tau}$, then either $|Q_n - \text{sum}(xs)|\leq\tau$ or the recursive step is applied, yielding:
    \begin{equation}
        |Q_n - \text{sum}_n(xs)| = |Q_{\lfloor n / 2 \rfloor} + Q_{\lceil n / 2 \rceil} - \text{sum}_{\lfloor n / 2 \rfloor}(xs) - \text{sum}_{\lceil n / 2 \rceil}(xs)|.
    \end{equation}
    Note that for every $n>1$ it holds $\lceil n / 2 \rceil\leq n-1$, therefore:
    \begin{equation}
        |Q_n - \text{sum}_n(xs)| \leq |Q_{\lfloor n / 2 \rfloor} - \text{sum}_{\lfloor n / 2 \rfloor}(xs)| + |Q_{\lceil n / 2 \rceil} - \text{sum}_{\lceil n / 2 \rceil}(xs)|\leq \overline{\tau} + \overline{\tau} = \tau,
    \end{equation}
    which proves the statement for $N=n$.
\end{proof}

\begin{algorithm}[t]
    \caption{\textsc{Adaptive}}\label{alg:adaptive-quadrature}
    \begin{algorithmic}
    \State \textbf{Input:} A list of real numbers $xs$, a list of integers $idx$, dictionary $idxes$ and tolerance $\tau \geq 0$
    \State $N = \text{length}(xs)$.
    \If{$N \geq 2$}
        \State $idxes[idx[0]] = 1$ and $idxes[idx[-1]] = 1$
        \State $Q = N \cdot (xs[0] + xs[-1]) / 2$.
    \Else
        \State $idxes[idx[0]] =1$
        \State \Return $xs[0]$
    \EndIf
    \State $\varepsilon = |Q - \text{sum}(xs)|$
    \If{$\varepsilon \geq \tau$}
        \State $c = \lfloor N / 2 \rfloor$.
        \State $Q, idxes = \textsc{Adaptive}(xs[:c], idx[:c], \tau /2, idxes) + \textsc{Adaptive}(xs[c:], idx[c:], \tau /2, idxes)$
    \EndIf
    \State \Return $Q, idxes$.
    \end{algorithmic}
    \end{algorithm}


    \begin{algorithm}[t]
        \caption{\textsc{Uniform}}\label{alg:uniform}
        \begin{algorithmic}
        \State \textbf{Input:} Integers $N,M$.
        \State $idxes = []$, $idx = 0$ and $i=1$
        \State $idxes$.append$(1)$
        \While{$idx < N$}
        \State $idx = i * M$
        \State $idxes.$append$(i * M)$
        \State $i = i + 1$
        \EndWhile
        \State $idxes$.append$(N)$
        \State \Return $idxes$.
        \end{algorithmic}
        \end{algorithm}



        \begin{algorithm}[t]
            \caption{\textsc{Monte-Carlo Policy Iteration}}\label{alg:mc}
            \begin{algorithmic}
            \State \textbf{Input:} Action-value features $\phi: \mcS \times \mcA \rightarrow \mbR^d$, number of evaluation trajectories $N$, a tolerance $\tau \geq 0$ and uniform spacing integer $M$, a dictionary of update points pivots = $\{\}$ and exploration parameter $\varepsilon \in [0,1]$.
            \State \textbf{Initialize} the action value weights $\theta_0 \in \mbR^d$ arbitrarily
            \For{$i = 1,2,\dotsc$}
                \State $\pi_i(s) = \argmax_{a \in \mcA} \phi(s,a)^\top \theta_{i-1}$ with probability $1-\varepsilon$, else $\pi_s \sim \text{Uniform}(\mcA)$.
                \For{$j = 1,2,\dotsc,N$}
                    \State Collect trajectories $S_{1,i,j},A_{1,i,j},R_{1,i,j},\dotsc,R_{T_j,i,j},S_{T_j+1,i,j}$ using policy $\pi_i$
                    \If{uniform}
                    \State pivots$[i,j] = \textsc{uniform}(T_j, M)$
                    \ElsIf{adaptive}
                    \State temp, pivots$[i,j] = \textsc{adaptive}([R_{1:T_j,i,j}], \{1,2,\dotsc,T_j\},\{\}, \tau])$
                    \Else
                    \State pivots$[i,j]=\{1,2,\dotsc,T_j\}$
                    \EndIf
                \EndFor
                \State Update $\theta_i = \argmin_{\theta \in \mbR^d} \sum_{i,j}\sum_{t\in\text{pivots}[i,j]} \left(\phi(S_{t,i,j},A_{t,i,j}) - G_{t,i,j}\right)^2$
            \EndFor
            \end{algorithmic}
            \end{algorithm}
