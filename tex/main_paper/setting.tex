\section{Preliminaries}
Reinforcement learning is often modeled as a Markov decision process (MDP), which is characterized by the tuple $M = (\mcS, \mcA, P, \gamma, R)$. In this model, an agent and environment interact over a sequence of time steps denoted as $t$. At each time step, the agent receives a state $S_t \in \mcS$ from the environment, where $\mcS$ denotes the set of all possible states. The agent uses the information given by the state to select and action $A_t$ from the set of possible actions $\mcA$. Based on the state of the environment and the agents behavior, i.e. the action, the agent receives a scalar reward $R_t = R(S_t,A_t)$ and transitions to the next state $S_{t+1} \in \mcS$ according to the state-transition probability $P(s'|s,a) = P(S_{t+1} = s' | S_t = s, A_t = a)$ for each $s,s' \in \mcS$ and $a \in \mcA$.

The actual objective of the learner is to maximize the expected discounted return
\begin{align*}
    G_t = R_{t} + \gamma R_{t+1} + \gamma^2 R_{t+1} + \dots = \sum_{k=1}^{T-t} \gamma^{k-1} R_{t+k-1}
\end{align*}
either for a discounted factor $\gamma \in [0,1)$ when the task is continuing, $T = \infty$, or for $\gamma \in [0,1]$ and $T < \infty$, i.e., in an episodic task. The latter return is influenced by the actions taken by the agent, thus its behavior defined through a policy $\pi (a|s)$, which is a probability distribution over actions given a state. Then the agent's goal is to learn the optimal policy $\pi^\star$, which is the policy that maximizes the expected return.

Through the process of policy iteration \cite{sutton1998introduction}, Monte-Carlo algorithms strive to maximize the expected return by approximating the value-function, which is indeed defined as its expectation. The state-value function quantifies the agent's expected return starting in state $s$ and following policy $\pi$, i.e. $v^\pi(s) = \EE_\pi [G_t | S_t =s]$. Given that the task in RL is to face the control problem going beyond prediction, we often want to estimate the action-value function, which is the agent's expected return starting in state $s$, taking action $a$ and then following policy $\pi$, i.e.
\begin{equation}\label{eqn:action-values}
    q_\pi(s,a) = \mathbb{E}_\pi[G_t \, | \, S_t = s, A_t = a].
\end{equation}
In many RL problems of interest, the state-space $\mcS$ is prohibitively large and function approximation is needed in order to enable sample-efficient learning. Note that the algorithmic ideas presented in this paper will be given as linear solution methods, however their applicability is broader as they can be extended to arbitrary function approximation schemes. Given a set of linear features $\phi : \mcS \times \mcA \rightarrow \mbR^d$ and a trajectory $S_1,A_1,R_1,\dotsc,R_T,S_{T+1}$ collected following policy $\pi$ in $M$, the action values can be approximated by solving the least-squares problem
\begin{equation}
    \hat\theta = \argmin_{\theta \in \mbR^d} \sum_{t=1}^T \left(\phi(S_t,A_t)^\top \theta - G_t\right)^2
\end{equation}
and assuming that the feature yield a good linear representation of the action-value function, it holds that $q_\pi(s,a) \approx \phi(s,a)^\top \hat\theta$.
\subsection{Monte-Carlo Policy Iteration} The main objective in \textit{policy iteration} is to maximize the expected return by selecting the policy that maximizes the approximated action-value function, which is learnt from observations collected via interaction with the environment. Given that the number of point in a trajectory negatively affects computational time of the procedure and memory usage, ideally this should be done using as few samples as possible. Monte-Carlo policy iteration works by first performing policy evaluation to learn the action-values using the returns and then acting greedily with respected to the learned action-values. Monte-Carlo policy iteration can be summarized as 
\begin{enumerate}
    \item (Policy evaluation): Collect $N \in \mathbb{N}$ trajectories $S_{1},A_1,R_1,\dotsc,R_T,S_{T+1}$ following policy $\pi$ in MDP $M$. Then solve the least squares problem 
    \begin{align*}
        \hat\theta = \argmin_{\theta \in \mbR^d} \sum_{i=1}^N\sum_{t=1}^T \left(\phi(S_{t,i},A_{t,i})^\top \theta - G_{t,i}\right)^2
    \end{align*}
    where $S_{t,i}$ corresponds to the $t$-th state in the $i$-th trajectory for $i \in \{1,\dotsc,N\}$ and $t \in \{1,\dotsc,T\}$.
    \item (Greedify): Take the new policy to be the policy that is greedy with respect to the approximate action-values, i.e. $\pi(s) = \argmax_{a \in \mcA} \phi(s,a)^\top \hat\theta \approx \argmax q^{\pi'}(s,a)$ where $\pi'$ is the previous policy.
\end{enumerate}
The main question this paper aims to answer is if the number of samples from a trajectory used to approximate the action-value function can be reduced by means of an adaptive algorithm. Usually the system is assumed to have a fixed discretization step, and current algorithms makes use of all the samples they are given. However it has been recently established by \cite{zhang2024managing} that using only a subset of the tuples of the trajectory yields a better result when performing Monte-Carlo value estimation. In their work, they show, both theoretically and empirically, that a simple uniform discretization over the trajectory, i.e. updating using every $M$-th tuple in the trajectory, leads to significantly better value estimation when the number of updates is fixed. In this work, we also propose further discretizing, or subsampling, the trajectory in order to learn with as few updates as possible. By simply considering a coarse discretization we might lose too much information about the behaviour of the agent, while keeping the whole trajectory would lead to a computationally expensive algorithm. We will show that adapting the discretization step allows the algorithm to use less samples while still producing a good approximation of the action-value function, and consequently achieving a good policy. In the next section we will introduce both a uniform scheme for discretizing the Monte-Carlo updates and an adaptive scheme for discretizing the updates.



