@article{park2021time,
  author    = {Seohong Park and
               Jaekyeom Kim and
               Gunhee Kim},
  title     = {Time Discretization-Invariant Safe Action Repetition for Policy Gradient
               Methods},
  journal   = {CoRR},
  volume    = {abs/2111.03941},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.03941},
  eprinttype = {arXiv},
  eprint    = {2111.03941},
  timestamp = {Wed, 10 Nov 2021 16:07:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-03941.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{baird1994reinforcement,
  title = {Reinforcement Learning in Continuous Time: Advantage Updating},
  shorttitle = {Reinforcement Learning in Continuous Time},
  booktitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  author = {Baird, L.C.},
  year = {1994},
  month = jun,
  volume = {4},
  pages = {2448-2453 vol.4},
  doi = {10.1109/ICNN.1994.374604},
}

@article{lakshminarayanan2017dynamic,
  title = {Dynamic {{Action Repetition}} for {{Deep Reinforcement Learning}}},
  author = {Lakshminarayanan, Aravind S and Sharma, Sahil and Ravindran, Balaraman},
  pages = {7},
}

@article{munos2006policy,
  title = {Policy {{Gradient}} in {{Continuous Time}}},
  author = {Munos, R{\'e}mi},
  year = {2006},
  journal = {Journal of Machine Learning Research},
  volume = {7},
  number = {27},
  pages = {771--791},
  issn = {1533-7928},
}

@inproceedings{munos1997reinforcement,
  title = {Reinforcement {{Learning}} for {{Continuous Stochastic Control Problems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Munos, R{\'e}mi and Bourgine, Paul},
  year = {1997},
  volume = {10},
  publisher = {{MIT Press}},
}

@misc{wawrzy2007reinforcement,
  title = {Reinforcement {{Learning}} in {{Fine Time}}},
  author = {Wawrzy{\'n}ski, Pawe{\l}},
  year = {2007},
}

@article{sharma2020learning,
  title = {Learning to {{Repeat}}: {{Fine Grained Action Repetition}} for {{Deep Reinforcement Learning}}},
  shorttitle = {Learning to {{Repeat}}},
  author = {Sharma, Sahil and Srinivas, Aravind and Ravindran, Balaraman},
  year = {2020},
  month = sep,
  journal = {arXiv:1702.06054 [cs]},
  eprint = {1702.06054},
  eprinttype = {arxiv},
  primaryclass = {cs},

}

@article{tallec2019making,
  title = {Making {{Deep Q-learning}} Methods Robust to Time Discretization},
  author = {Tallec, Corentin and Blier, L{\'e}onard and Ollivier, Yann},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.09732 [cs, stat]},
  eprint = {1901.09732},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
}

@article{wawrzy2015control,
  title = {Control {{Policy}} with {{Autocorrelated Noise}} in {{Reinforcement Learning}} for {{Robotics}}},
  author = {Wawrzy{\'n}ski, Pawe{\l}},
  year = {2015},
  month = apr,
  journal = {International Journal of Machine Learning and Computing},
  volume = {5},
  number = {2},
  pages = {91--95},
  issn = {20103700},
  doi = {10.7763/IJMLC.2015.V5.489},
}


@article{shenDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} with {{Robust}} and {{Smooth Policy}}},
  author = {Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo},
  year = {2020},
  month = aug,
  journal = {arXiv:2003.09534 [cs, stat]},
  eprint = {2003.09534},
  eprinttype = {arxiv},
  primaryclass = {cs, stat}
}


% To read
@misc{sinclair2020adaptive,
      title={Adaptive Discretization for Model-Based Reinforcement Learning}, 
      author={Sean R. Sinclair and Tianyu Wang and Gauri Jain and Siddhartha Banerjee and Christina Lee Yu},
      year={2020},
      eprint={2007.00717},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2022managing,
      title={Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off}, 
      author={Zichen Zhang and Johannes Kirschner and Junxi Zhang and Francesco Zanini and Alex Ayoub and Masood Dehghan and Dale Schuurmans},
      year={2022},
      eprint={2212.08949},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dabney2020temporally,
	title = {Temporally-{Extended} \{{\textbackslash}epsilon\}-{Greedy} {Exploration}},
	url = {https://arxiv.org/abs/2006.01782v1},
	doi = {10.48550/arXiv.2006.01782},
	abstract = {Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as \{{\textbackslash}epsilon\}-greedy. In this paper we propose an exploration algorithm that retains the simplicity of \{{\textbackslash}epsilon\}-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of \{{\textbackslash}epsilon\}-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of \{{\textbackslash}epsilon\}-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.},
	language = {en},
	urldate = {2023-02-23},
	author = {Dabney, Will and Ostrovski, Georg and Barreto, Andr√©},
	month = jun,
	year = {2020},
	file = {Full Text PDF:/home/dhawgupta/snap/zotero-snap/common/Zotero/storage/CI24X86Y/Dabney et al. - 2020 - Temporally-Extended epsilon -Greedy Exploration.pdf:application/pdf},
}

@inproceedings{metelli2020control,
	title = {Control {Frequency} {Adaptation} via {Action} {Persistence} in {Batch} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v119/metelli20a.html},
	abstract = {The choice of the control frequency of a system has a relevant impact on the ability of reinforcement learning algorithms to learn a highly performing policy. In this paper, we introduce the notion of action persistence that consists in the repetition of an action for a fixed number of decision steps, having the effect of modifying the control frequency. We start analyzing how action persistence affects the performance of the optimal policy, and then we present a novel algorithm, Persistent Fitted Q-Iteration (PFQI), that extends FQI, with the goal of learning the optimal value function at a given persistence. After having provided a theoretical study of PFQI and a heuristic approach to identify the optimal persistence, we present an experimental campaign on benchmark domains to show the advantages of action persistence and proving the effectiveness of our persistence selection method.},
	language = {en},
	urldate = {2023-06-07},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Metelli, Alberto Maria and Mazzolini, Flavio and Bisi, Lorenzo and Sabbioni, Luca and Restelli, Marcello},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6862--6873},

}

@misc{jacq2022lazy,
	title = {Lazy-{MDPs}: {Towards} {Interpretable} {Reinforcement} {Learning} by {Learning} {When} to {Act}},
	shorttitle = {Lazy-{MDPs}},
	url = {http://arxiv.org/abs/2203.08542},
	doi = {10.48550/arXiv.2203.08542},
	abstract = {Traditionally, Reinforcement Learning (RL) aims at deciding how to act optimally for an artificial agent. We argue that deciding when to act is equally important. As humans, we drift from default, instinctive or memorized behaviors to focused, thought-out behaviors when required by the situation. To enhance RL agents with this aptitude, we propose to augment the standard Markov Decision Process and make a new mode of action available: being lazy, which defers decision-making to a default policy. In addition, we penalize non-lazy actions in order to encourage minimal effort and have agents focus on critical decisions only. We name the resulting formalism lazy-MDPs. We study the theoretical properties of lazy-MDPs, expressing value functions and characterizing optimal solutions. Then we empirically demonstrate that policies learned in lazy-MDPs generally come with a form of interpretability: by construction, they show us the states where the agent takes control over the default policy. We deem those states and corresponding actions important since they explain the difference in performance between the default and the new, lazy policy. With suboptimal policies as default (pretrained or random), we observe that agents are able to get competitive performance in Atari games while only taking control in a limited subset of states.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Jacq, Alexis and Ferret, Johan and Pietquin, Olivier and Geist, Matthieu},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08542 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: AAMAS 2022 (14 pages extended version, added Sec. 7.4 and appendix K)},

}